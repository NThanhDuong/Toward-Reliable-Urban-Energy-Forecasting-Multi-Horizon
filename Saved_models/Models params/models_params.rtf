{\rtf1\ansi\ansicpg1252\cocoartf2865
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fmodern\fcharset0 Courier;}
{\colortbl;\red255\green255\blue255;\red220\green220\blue220;\red30\green31\blue33;\red212\green212\blue212;
\red194\green126\blue101;\red202\green202\blue202;\red167\green197\blue152;}
{\*\expandedcolortbl;;\cssrgb\c89020\c89020\c89020;\cssrgb\c15686\c16471\c17255;\cssrgb\c86275\c86275\c86275;
\cssrgb\c80784\c56863\c47059;\cssrgb\c83137\c83137\c83137;\cssrgb\c70980\c80784\c65882;}
\paperw11900\paperh16840\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\partightenfactor0

\f0\fs28 \cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Best XGB params: \{'model__estimator__subsample': 0.7, 'model__estimator__reg_lambda': 0.5, 'model__estimator__reg_alpha': 0.0, 'model__estimator__n_estimators': 1000, 'model__estimator__min_child_weight': 3, 'model__estimator__max_depth': 6, 'model__estimator__learning_rate': 0.03, 'model__estimator__gamma': 0.1, 'model__estimator__colsample_bytree': 0.7\}\
\
Best LGB params: \{'model__estimator__subsample': 0.9,\
 'model__estimator__reg_lambda': 0.1,\
 'model__estimator__reg_alpha': 0.0,\
 'model__estimator__num_leaves': 63,\
 'model__estimator__n_estimators': 300,\
 'model__estimator__min_child_samples': 20,\
 'model__estimator__max_depth': 8,\
 'model__estimator__learning_rate': 0.05,\
 'model__estimator__colsample_bytree': 0.7\}\
\
Best Transformer params: \cf4 \cb3 \strokec4 \{\cf5 \strokec5 'd_model'\cf4 \strokec4 :\cf6 \strokec6  \cf7 \strokec7 160\cf4 \strokec4 ,\cf6 \strokec6  \cf5 \strokec5 'nhead_160'\cf4 \strokec4 :\cf6 \strokec6  \cf7 \strokec7 4\cf4 \strokec4 ,\cf6 \strokec6  \cf5 \strokec5 'layers'\cf4 \strokec4 :\cf6 \strokec6  \cf7 \strokec7 2\cf4 \strokec4 ,\cf6 \strokec6  \cf5 \strokec5 'd_ff_mult'\cf4 \strokec4 :\cf6 \strokec6  \cf7 \strokec7 4\cf4 \strokec4 ,\cf6 \strokec6  \cf5 \strokec5 'dropout'\cf4 \strokec4 :\cf6 \strokec6  \cf7 \strokec7 0.1450990521919241\cf4 \strokec4 ,\cf6 \strokec6  \cf5 \strokec5 'batch'\cf4 \strokec4 :\cf6 \strokec6  \cf7 \strokec7 96\cf4 \strokec4 ,\cf6 \strokec6  \cf5 \strokec5 'lr'\cf4 \strokec4 :\cf6 \strokec6  \cf7 \strokec7 0.0002934709738664784\cf4 \strokec4 ,\cf6 \strokec6  \cf5 \strokec5 'weight_decay'\cf4 \strokec4 :\cf6 \strokec6  \cf7 \strokec7 0.0009177960560335439\cf4 \strokec4 \}\cf6 \cb1 \strokec6 \
\
\pard\pardeftab720\partightenfactor0
\cf6 \cb3 Best LSTM_attention params: hidden=\cf7 \strokec7 112\cf4 \strokec4 ,\cf6 \strokec6  layers=\cf7 \strokec7 2\cf4 \strokec4 ,\cf6 \strokec6  dropout=\cf7 \strokec7 0.12\cf4 \strokec4 ,\cf6 \cb1 \strokec6 \
\cb3         batch=\cf7 \strokec7 160\cf4 \strokec4 ,\cf6 \strokec6  epochs=\cf7 \strokec7 100\cf4 \strokec4 ,\cf6 \strokec6  lr=\cf7 \strokec7 9e-4\cf4 \strokec4 ,\cf6 \strokec6  weight_decay=\cf7 \strokec7 2e-5\cf4 \strokec4 ,\cf6 \strokec6  grad_clip=\cf7 \strokec7 1.0\
\
\pard\pardeftab720\partightenfactor0
\cf6 \cb1 \strokec6 \
}