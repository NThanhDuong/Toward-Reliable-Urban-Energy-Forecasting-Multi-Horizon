{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"toc_visible":true,"mount_file_id":"1fxTAnVzzpGsNokMFT_OFWgYTjX2Wc78F","authorship_tag":"ABX9TyO5iK8DbYtwLzOEg/5DCVYI"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12884373,"sourceType":"datasetVersion","datasetId":8151547},{"sourceId":589420,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":440771,"modelId":457312},{"sourceId":589800,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":441059,"modelId":457598},{"sourceId":590042,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":441255,"modelId":457794}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setup","metadata":{}},{"cell_type":"code","source":"# Colab / Kaggle cell\n!pip install -q --upgrade \"scikit-learn==1.6.1\" \"xgboost==3.0.5\" \"joblib==1.5.2\"\n\n# force a clean restart so the new wheels are used\nimport os, sys; os.kill(os.getpid(), 9)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:13:22.027374Z","iopub.execute_input":"2025-09-25T09:13:22.027583Z","execution_failed":"2025-09-25T09:13:40.086Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m97.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.4/308.4 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncategory-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.6.1 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.1 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import sklearn, xgboost, joblib\nprint(sklearn.__version__, xgboost.__version__, joblib.__version__)\n# should be 1.6.1, 3.0.5, 1.5.2\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:14:46.514849Z","iopub.execute_input":"2025-09-25T09:14:46.515360Z","iopub.status.idle":"2025-09-25T09:14:47.522474Z","shell.execute_reply.started":"2025-09-25T09:14:46.515339Z","shell.execute_reply":"2025-09-25T09:14:47.521824Z"}},"outputs":[{"name":"stdout","text":"1.6.1 3.0.5 1.5.2\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# --- Strict reproducibility setup ---\nimport os, re, math, warnings\nimport numpy as np\nimport pandas as pd\nimport inspect\nimport random\nimport torch\nSEED = 42\n\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\nos.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"   # or \":16:8\" (needed for deterministic matmul)\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\n\ntorch.backends.cudnn.benchmark = False\ntorch.backends.cudnn.deterministic = True\ntorch.use_deterministic_algorithms(False)            # may fall back to slower kernels\ntorch.backends.cuda.matmul.allow_tf32 = False\ntorch.backends.cudnn.allow_tf32 = False\n\nwarnings.filterwarnings(\"ignore\")","metadata":{"id":"iS-HgxqrMSRs","executionInfo":{"status":"ok","timestamp":1756109611634,"user_tz":-420,"elapsed":215,"user":{"displayName":"Phuc L. Nguyen","userId":"06294373354443477970"}},"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:14:51.533736Z","iopub.execute_input":"2025-09-25T09:14:51.534120Z","iopub.status.idle":"2025-09-25T09:14:57.360564Z","shell.execute_reply.started":"2025-09-25T09:14:51.534100Z","shell.execute_reply":"2025-09-25T09:14:57.359980Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/weather-hanoi-2020-2025-normed/merge_weather_energy_hanoi_20202025_norm.csv\")\ndf.head()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":434},"id":"0CtfbMtJQhv-","executionInfo":{"status":"ok","timestamp":1756109616583,"user_tz":-420,"elapsed":4240,"user":{"displayName":"Phuc L. Nguyen","userId":"06294373354443477970"}},"outputId":"c5a34676-e6f7-4ff1-c76c-cfc83bad0804","trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:14:59.497845Z","iopub.execute_input":"2025-09-25T09:14:59.498252Z","iopub.status.idle":"2025-09-25T09:14:59.704831Z","shell.execute_reply.started":"2025-09-25T09:14:59.498230Z","shell.execute_reply":"2025-09-25T09:14:59.704073Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   Temperature               Weather  Precipitation  Chance of snow  Humidity  \\\n0         25.4  Patchy rain possible            0.6             0.0      89.0   \n1         25.1         Partly cloudy            0.0             0.0      90.0   \n2         24.7  Patchy rain possible            0.0             0.0      91.0   \n3         24.5                Cloudy            0.0             0.0      92.0   \n4         24.1  Patchy rain possible            0.0             0.0      93.0   \n\n       Wind  Wind Gust  Wind Degree Wind Direction  Cloud Cover  Visibility  \\\n0  2.194444   4.388889        295.0            WNW         89.0         9.0   \n1  2.611111   5.111111        297.0            WNW         34.0        10.0   \n2  2.805556   5.500000        309.0             NW         87.0        10.0   \n3  2.611111   4.888889        325.0             NW         71.0        10.0   \n4  2.305556   4.000000        326.0            NNW        100.0        10.0   \n\n             timestamp  is_weekend  season is_holiday  total_consumption_mw  \n0  2020-01-01 00:00:00           0  winter      False               1790.10  \n1  2020-01-01 01:00:00           0  winter      False               1452.26  \n2  2020-01-01 02:00:00           0  winter      False               1483.75  \n3  2020-01-01 03:00:00           0  winter      False               1890.07  \n4  2020-01-01 04:00:00           0  winter      False               1371.23  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Temperature</th>\n      <th>Weather</th>\n      <th>Precipitation</th>\n      <th>Chance of snow</th>\n      <th>Humidity</th>\n      <th>Wind</th>\n      <th>Wind Gust</th>\n      <th>Wind Degree</th>\n      <th>Wind Direction</th>\n      <th>Cloud Cover</th>\n      <th>Visibility</th>\n      <th>timestamp</th>\n      <th>is_weekend</th>\n      <th>season</th>\n      <th>is_holiday</th>\n      <th>total_consumption_mw</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>25.4</td>\n      <td>Patchy rain possible</td>\n      <td>0.6</td>\n      <td>0.0</td>\n      <td>89.0</td>\n      <td>2.194444</td>\n      <td>4.388889</td>\n      <td>295.0</td>\n      <td>WNW</td>\n      <td>89.0</td>\n      <td>9.0</td>\n      <td>2020-01-01 00:00:00</td>\n      <td>0</td>\n      <td>winter</td>\n      <td>False</td>\n      <td>1790.10</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>25.1</td>\n      <td>Partly cloudy</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>90.0</td>\n      <td>2.611111</td>\n      <td>5.111111</td>\n      <td>297.0</td>\n      <td>WNW</td>\n      <td>34.0</td>\n      <td>10.0</td>\n      <td>2020-01-01 01:00:00</td>\n      <td>0</td>\n      <td>winter</td>\n      <td>False</td>\n      <td>1452.26</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>24.7</td>\n      <td>Patchy rain possible</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>91.0</td>\n      <td>2.805556</td>\n      <td>5.500000</td>\n      <td>309.0</td>\n      <td>NW</td>\n      <td>87.0</td>\n      <td>10.0</td>\n      <td>2020-01-01 02:00:00</td>\n      <td>0</td>\n      <td>winter</td>\n      <td>False</td>\n      <td>1483.75</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>24.5</td>\n      <td>Cloudy</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>92.0</td>\n      <td>2.611111</td>\n      <td>4.888889</td>\n      <td>325.0</td>\n      <td>NW</td>\n      <td>71.0</td>\n      <td>10.0</td>\n      <td>2020-01-01 03:00:00</td>\n      <td>0</td>\n      <td>winter</td>\n      <td>False</td>\n      <td>1890.07</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>24.1</td>\n      <td>Patchy rain possible</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>93.0</td>\n      <td>2.305556</td>\n      <td>4.000000</td>\n      <td>326.0</td>\n      <td>NNW</td>\n      <td>100.0</td>\n      <td>10.0</td>\n      <td>2020-01-01 04:00:00</td>\n      <td>0</td>\n      <td>winter</td>\n      <td>False</td>\n      <td>1371.23</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"df.shape","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EtjPaJMfQk_a","executionInfo":{"status":"ok","timestamp":1756109620949,"user_tz":-420,"elapsed":5,"user":{"displayName":"Phuc L. Nguyen","userId":"06294373354443477970"}},"outputId":"54485723-e2c1-46ad-ea6d-7927c15e2dd3","trusted":true,"execution":{"iopub.status.busy":"2025-09-24T12:48:29.586832Z","iopub.execute_input":"2025-09-24T12:48:29.587105Z","iopub.status.idle":"2025-09-24T12:48:29.592069Z","shell.execute_reply.started":"2025-09-24T12:48:29.587085Z","shell.execute_reply":"2025-09-24T12:48:29.591360Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"(47449, 16)"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"CAT_COLS   = ['Weather','Wind Direction','season','is_holiday']\nEXOG_NUM   = ['Temperature','Precipitation','Humidity',\n              'Wind','Wind Gust','Wind Degree','Cloud Cover','Visibility']\nHORIZON    = 24                       # forecast t+1…t+24\nTARGET_LAGS   = range(1,49)           # lags of target (1…48)\nROLL_WINDOWS  = [3,6,12,24,48]        # rolling windows\nEXOG_LAGS     = [0,1,3,6,12,24]       # lags for exogenous vars\nTARGET_COL = 'total_consumption_mw'\nTIME_COL   = 'timestamp'","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iqBCs3VbQnUj","executionInfo":{"status":"ok","timestamp":1756109623734,"user_tz":-420,"elapsed":6,"user":{"displayName":"Phuc L. Nguyen","userId":"06294373354443477970"}},"outputId":"a472b679-11a1-4ac9-d512-4b9cf8c31fcd","trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:15:05.539090Z","iopub.execute_input":"2025-09-25T09:15:05.539370Z","iopub.status.idle":"2025-09-25T09:15:05.543988Z","shell.execute_reply.started":"2025-09-25T09:15:05.539345Z","shell.execute_reply":"2025-09-25T09:15:05.543189Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"df[TIME_COL] = pd.to_datetime(df[TIME_COL])\ndf = df.sort_values(TIME_COL).reset_index(drop=True)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":209},"id":"hws8zuXQQuPT","executionInfo":{"status":"ok","timestamp":1756060919689,"user_tz":-420,"elapsed":13,"user":{"displayName":"Phuc L. Nguyen","userId":"06294373354443477970"}},"outputId":"cdb8f8b4-4ba8-4f4c-c94a-e739f2174703","trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:15:06.792713Z","iopub.execute_input":"2025-09-25T09:15:06.793398Z","iopub.status.idle":"2025-09-25T09:15:06.945474Z","shell.execute_reply.started":"2025-09-25T09:15:06.793377Z","shell.execute_reply":"2025-09-25T09:15:06.944891Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# make sure categoricals are strings\nfor col in CAT_COLS:\n    df[col] = df[col].astype(str)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:15:08.117197Z","iopub.execute_input":"2025-09-25T09:15:08.117744Z","iopub.status.idle":"2025-09-25T09:15:08.125430Z","shell.execute_reply.started":"2025-09-25T09:15:08.117714Z","shell.execute_reply":"2025-09-25T09:15:08.124712Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Feature enginerring & data preprocessing","metadata":{"id":"uEN8KH8RVU44"}},{"cell_type":"code","source":"# add calendar/time features\ndf['hour']      = df[TIME_COL].dt.hour\ndf['dow']       = df[TIME_COL].dt.dayofweek\ndf['dom']       = df[TIME_COL].dt.day\ndf['month']     = df[TIME_COL].dt.month\ndf['doy']       = df[TIME_COL].dt.dayofyear\ndf['hour_sin']  = np.sin(2*np.pi*df['hour']/24)\ndf['hour_cos']  = np.cos(2*np.pi*df['hour']/24)\ndf['dow_sin']   = np.sin(2*np.pi*df['dow']/7)\ndf['dow_cos']   = np.cos(2*np.pi*df['dow']/7)\ndf['doy_sin']   = np.sin(2*np.pi*df['doy']/365.25)\ndf['doy_cos']   = np.cos(2*np.pi*df['doy']/365.25)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:15:10.659090Z","iopub.execute_input":"2025-09-25T09:15:10.659426Z","iopub.status.idle":"2025-09-25T09:15:10.693291Z","shell.execute_reply.started":"2025-09-25T09:15:10.659403Z","shell.execute_reply":"2025-09-25T09:15:10.692724Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# 2. feature engineering\n# create lagged target features\nfor L in TARGET_LAGS:\n    df[f'y_lag_{L}'] = df[TARGET_COL].shift(L)\n\n# rolling statistics on the target, with one‑step shift to avoid leakage\nfor W in ROLL_WINDOWS:\n    df[f'y_rollmean_{W}'] = df[TARGET_COL].shift(1).rolling(W, min_periods=1).mean()\n    df[f'y_rollstd_{W}']  = df[TARGET_COL].shift(1).rolling(W, min_periods=1).std()\n\n# lagged exogenous features\nfor col in EXOG_NUM:\n    for L in EXOG_LAGS:\n        df[f'{col}_lag{L}'] = df[col].shift(L)\n\n# multi‑horizon labels\nfor h in range(1, HORIZON+1):\n    df[f'y_t+{h}'] = df[TARGET_COL].shift(-h)\n\n# drop rows with NaNs (from lagging & shifting)no\ndf_feat = df.dropna().reset_index(drop=True)\ndf_feat.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:15:12.094147Z","iopub.execute_input":"2025-09-25T09:15:12.094424Z","iopub.status.idle":"2025-09-25T09:15:12.424253Z","shell.execute_reply.started":"2025-09-25T09:15:12.094405Z","shell.execute_reply":"2025-09-25T09:15:12.423587Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"(47377, 157)"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"df_feat.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T15:18:21.683497Z","iopub.execute_input":"2025-09-22T15:18:21.684193Z","iopub.status.idle":"2025-09-22T15:18:21.688563Z","shell.execute_reply.started":"2025-09-22T15:18:21.684168Z","shell.execute_reply":"2025-09-22T15:18:21.687909Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"(47377, 157)"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"np.max(df_feat['timestamp'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T13:06:38.834423Z","iopub.execute_input":"2025-09-22T13:06:38.834739Z","iopub.status.idle":"2025-09-22T13:06:38.840802Z","shell.execute_reply.started":"2025-09-22T13:06:38.834718Z","shell.execute_reply":"2025-09-22T13:06:38.840028Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"Timestamp('2025-05-30 00:00:00')"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# 3. train/valid/test split by time\nlabel_cols   = [f'y_t+{h}' for h in range(1, HORIZON+1)]\nfeature_cols = [c for c in df_feat.columns if c not in label_cols + [TIME_COL, TARGET_COL]]\n\nX = df_feat[feature_cols]\nY = df_feat[label_cols].values\n\nN        = len(X)\ntrain_end= int(N - 2*8*7*24)\nvalid_end= int(N - 8*7*24)\n\nX_train, Y_train = X.iloc[:train_end],    Y[:train_end]\nX_valid, Y_valid = X.iloc[train_end:valid_end], Y[train_end:valid_end]\nX_test,  Y_test  = X.iloc[valid_end:],    Y[valid_end:]\n\nprint(X_train.shape, Y_train.shape)\nprint(X_valid.shape, Y_valid.shape)\nprint(X_test.shape, Y_test.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:15:15.913696Z","iopub.execute_input":"2025-09-25T09:15:15.913947Z","iopub.status.idle":"2025-09-25T09:15:15.936890Z","shell.execute_reply.started":"2025-09-25T09:15:15.913929Z","shell.execute_reply":"2025-09-25T09:15:15.936192Z"}},"outputs":[{"name":"stdout","text":"(44689, 131) (44689, 24)\n(1344, 131) (1344, 24)\n(1344, 131) (1344, 24)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"np.max(df_feat['timestamp'].iloc[:valid_end])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-23T15:48:57.002284Z","iopub.execute_input":"2025-09-23T15:48:57.002918Z","iopub.status.idle":"2025-09-23T15:48:57.008254Z","shell.execute_reply.started":"2025-09-23T15:48:57.002894Z","shell.execute_reply":"2025-09-23T15:48:57.007602Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"Timestamp('2025-04-04 00:00:00')"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T17:09:10.889449Z","iopub.execute_input":"2025-09-20T17:09:10.889730Z","iopub.status.idle":"2025-09-20T17:09:10.919703Z","shell.execute_reply.started":"2025-09-20T17:09:10.889710Z","shell.execute_reply":"2025-09-20T17:09:10.919051Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"         y_t+1    y_t+2    y_t+3    y_t+4    y_t+5    y_t+6    y_t+7    y_t+8  \\\n0      1555.56  1505.56  1304.00  1819.42  1383.52  1546.93  1740.87  1746.23   \n1      1505.56  1304.00  1819.42  1383.52  1546.93  1740.87  1746.23  1897.63   \n2      1304.00  1819.42  1383.52  1546.93  1740.87  1746.23  1897.63  2062.55   \n3      1819.42  1383.52  1546.93  1740.87  1746.23  1897.63  2062.55  2296.52   \n4      1383.52  1546.93  1740.87  1746.23  1897.63  2062.55  2296.52  2345.26   \n...        ...      ...      ...      ...      ...      ...      ...      ...   \n44684  3097.82  2915.25  2112.98  1837.54  1595.33  1308.12  1556.42  1325.77   \n44685  2915.25  2112.98  1837.54  1595.33  1308.12  1556.42  1325.77  1568.16   \n44686  2112.98  1837.54  1595.33  1308.12  1556.42  1325.77  1568.16  1706.35   \n44687  1837.54  1595.33  1308.12  1556.42  1325.77  1568.16  1706.35  2008.87   \n44688  1595.33  1308.12  1556.42  1325.77  1568.16  1706.35  2008.87  1304.00   \n\n         y_t+9   y_t+10  ...   y_t+15   y_t+16   y_t+17   y_t+18   y_t+19  \\\n0      1897.63  2062.55  ...  1823.04  1791.94  1878.24  2037.24  2352.93   \n1      2062.55  2296.52  ...  1791.94  1878.24  2037.24  2352.93  3209.41   \n2      2296.52  2345.26  ...  1878.24  2037.24  2352.93  3209.41  3103.21   \n3      2345.26  2187.29  ...  2037.24  2352.93  3209.41  3103.21  2531.59   \n4      2187.29  1976.74  ...  2352.93  3209.41  3103.21  2531.59  2117.06   \n...        ...      ...  ...      ...      ...      ...      ...      ...   \n44684  1568.16  1706.35  ...  1304.00  1317.09  1304.00  1304.00  1304.00   \n44685  1706.35  2008.87  ...  1317.09  1304.00  1304.00  1304.00  1304.00   \n44686  2008.87  1304.00  ...  1304.00  1304.00  1304.00  1304.00  2137.69   \n44687  1304.00  1304.00  ...  1304.00  1304.00  1304.00  2137.69  2368.26   \n44688  1304.00  1304.00  ...  1304.00  1304.00  2137.69  2368.26  2629.19   \n\n        y_t+20   y_t+21   y_t+22   y_t+23   y_t+24  \n0      3209.41  3103.21  2531.59  2117.06  1725.51  \n1      3103.21  2531.59  2117.06  1725.51  1700.35  \n2      2531.59  2117.06  1725.51  1700.35  1681.38  \n3      2117.06  1725.51  1700.35  1681.38  1533.64  \n4      1725.51  1700.35  1681.38  1533.64  1633.85  \n...        ...      ...      ...      ...      ...  \n44684  1304.00  2137.69  2368.26  2629.19  2980.56  \n44685  2137.69  2368.26  2629.19  2980.56  2936.49  \n44686  2368.26  2629.19  2980.56  2936.49  2670.39  \n44687  2629.19  2980.56  2936.49  2670.39  2340.46  \n44688  2980.56  2936.49  2670.39  2340.46  1575.15  \n\n[44689 rows x 24 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>y_t+1</th>\n      <th>y_t+2</th>\n      <th>y_t+3</th>\n      <th>y_t+4</th>\n      <th>y_t+5</th>\n      <th>y_t+6</th>\n      <th>y_t+7</th>\n      <th>y_t+8</th>\n      <th>y_t+9</th>\n      <th>y_t+10</th>\n      <th>...</th>\n      <th>y_t+15</th>\n      <th>y_t+16</th>\n      <th>y_t+17</th>\n      <th>y_t+18</th>\n      <th>y_t+19</th>\n      <th>y_t+20</th>\n      <th>y_t+21</th>\n      <th>y_t+22</th>\n      <th>y_t+23</th>\n      <th>y_t+24</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1555.56</td>\n      <td>1505.56</td>\n      <td>1304.00</td>\n      <td>1819.42</td>\n      <td>1383.52</td>\n      <td>1546.93</td>\n      <td>1740.87</td>\n      <td>1746.23</td>\n      <td>1897.63</td>\n      <td>2062.55</td>\n      <td>...</td>\n      <td>1823.04</td>\n      <td>1791.94</td>\n      <td>1878.24</td>\n      <td>2037.24</td>\n      <td>2352.93</td>\n      <td>3209.41</td>\n      <td>3103.21</td>\n      <td>2531.59</td>\n      <td>2117.06</td>\n      <td>1725.51</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1505.56</td>\n      <td>1304.00</td>\n      <td>1819.42</td>\n      <td>1383.52</td>\n      <td>1546.93</td>\n      <td>1740.87</td>\n      <td>1746.23</td>\n      <td>1897.63</td>\n      <td>2062.55</td>\n      <td>2296.52</td>\n      <td>...</td>\n      <td>1791.94</td>\n      <td>1878.24</td>\n      <td>2037.24</td>\n      <td>2352.93</td>\n      <td>3209.41</td>\n      <td>3103.21</td>\n      <td>2531.59</td>\n      <td>2117.06</td>\n      <td>1725.51</td>\n      <td>1700.35</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1304.00</td>\n      <td>1819.42</td>\n      <td>1383.52</td>\n      <td>1546.93</td>\n      <td>1740.87</td>\n      <td>1746.23</td>\n      <td>1897.63</td>\n      <td>2062.55</td>\n      <td>2296.52</td>\n      <td>2345.26</td>\n      <td>...</td>\n      <td>1878.24</td>\n      <td>2037.24</td>\n      <td>2352.93</td>\n      <td>3209.41</td>\n      <td>3103.21</td>\n      <td>2531.59</td>\n      <td>2117.06</td>\n      <td>1725.51</td>\n      <td>1700.35</td>\n      <td>1681.38</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1819.42</td>\n      <td>1383.52</td>\n      <td>1546.93</td>\n      <td>1740.87</td>\n      <td>1746.23</td>\n      <td>1897.63</td>\n      <td>2062.55</td>\n      <td>2296.52</td>\n      <td>2345.26</td>\n      <td>2187.29</td>\n      <td>...</td>\n      <td>2037.24</td>\n      <td>2352.93</td>\n      <td>3209.41</td>\n      <td>3103.21</td>\n      <td>2531.59</td>\n      <td>2117.06</td>\n      <td>1725.51</td>\n      <td>1700.35</td>\n      <td>1681.38</td>\n      <td>1533.64</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1383.52</td>\n      <td>1546.93</td>\n      <td>1740.87</td>\n      <td>1746.23</td>\n      <td>1897.63</td>\n      <td>2062.55</td>\n      <td>2296.52</td>\n      <td>2345.26</td>\n      <td>2187.29</td>\n      <td>1976.74</td>\n      <td>...</td>\n      <td>2352.93</td>\n      <td>3209.41</td>\n      <td>3103.21</td>\n      <td>2531.59</td>\n      <td>2117.06</td>\n      <td>1725.51</td>\n      <td>1700.35</td>\n      <td>1681.38</td>\n      <td>1533.64</td>\n      <td>1633.85</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>44684</th>\n      <td>3097.82</td>\n      <td>2915.25</td>\n      <td>2112.98</td>\n      <td>1837.54</td>\n      <td>1595.33</td>\n      <td>1308.12</td>\n      <td>1556.42</td>\n      <td>1325.77</td>\n      <td>1568.16</td>\n      <td>1706.35</td>\n      <td>...</td>\n      <td>1304.00</td>\n      <td>1317.09</td>\n      <td>1304.00</td>\n      <td>1304.00</td>\n      <td>1304.00</td>\n      <td>1304.00</td>\n      <td>2137.69</td>\n      <td>2368.26</td>\n      <td>2629.19</td>\n      <td>2980.56</td>\n    </tr>\n    <tr>\n      <th>44685</th>\n      <td>2915.25</td>\n      <td>2112.98</td>\n      <td>1837.54</td>\n      <td>1595.33</td>\n      <td>1308.12</td>\n      <td>1556.42</td>\n      <td>1325.77</td>\n      <td>1568.16</td>\n      <td>1706.35</td>\n      <td>2008.87</td>\n      <td>...</td>\n      <td>1317.09</td>\n      <td>1304.00</td>\n      <td>1304.00</td>\n      <td>1304.00</td>\n      <td>1304.00</td>\n      <td>2137.69</td>\n      <td>2368.26</td>\n      <td>2629.19</td>\n      <td>2980.56</td>\n      <td>2936.49</td>\n    </tr>\n    <tr>\n      <th>44686</th>\n      <td>2112.98</td>\n      <td>1837.54</td>\n      <td>1595.33</td>\n      <td>1308.12</td>\n      <td>1556.42</td>\n      <td>1325.77</td>\n      <td>1568.16</td>\n      <td>1706.35</td>\n      <td>2008.87</td>\n      <td>1304.00</td>\n      <td>...</td>\n      <td>1304.00</td>\n      <td>1304.00</td>\n      <td>1304.00</td>\n      <td>1304.00</td>\n      <td>2137.69</td>\n      <td>2368.26</td>\n      <td>2629.19</td>\n      <td>2980.56</td>\n      <td>2936.49</td>\n      <td>2670.39</td>\n    </tr>\n    <tr>\n      <th>44687</th>\n      <td>1837.54</td>\n      <td>1595.33</td>\n      <td>1308.12</td>\n      <td>1556.42</td>\n      <td>1325.77</td>\n      <td>1568.16</td>\n      <td>1706.35</td>\n      <td>2008.87</td>\n      <td>1304.00</td>\n      <td>1304.00</td>\n      <td>...</td>\n      <td>1304.00</td>\n      <td>1304.00</td>\n      <td>1304.00</td>\n      <td>2137.69</td>\n      <td>2368.26</td>\n      <td>2629.19</td>\n      <td>2980.56</td>\n      <td>2936.49</td>\n      <td>2670.39</td>\n      <td>2340.46</td>\n    </tr>\n    <tr>\n      <th>44688</th>\n      <td>1595.33</td>\n      <td>1308.12</td>\n      <td>1556.42</td>\n      <td>1325.77</td>\n      <td>1568.16</td>\n      <td>1706.35</td>\n      <td>2008.87</td>\n      <td>1304.00</td>\n      <td>1304.00</td>\n      <td>1304.00</td>\n      <td>...</td>\n      <td>1304.00</td>\n      <td>1304.00</td>\n      <td>2137.69</td>\n      <td>2368.26</td>\n      <td>2629.19</td>\n      <td>2980.56</td>\n      <td>2936.49</td>\n      <td>2670.39</td>\n      <td>2340.46</td>\n      <td>1575.15</td>\n    </tr>\n  </tbody>\n</table>\n<p>44689 rows × 24 columns</p>\n</div>"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:15:20.183760Z","iopub.execute_input":"2025-09-25T09:15:20.184017Z","iopub.status.idle":"2025-09-25T09:15:20.187484Z","shell.execute_reply.started":"2025-09-25T09:15:20.183999Z","shell.execute_reply":"2025-09-25T09:15:20.186783Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"HORIZON = 24\nL = max(TARGET_LAGS)  # 48  (history window)\nDTYPE = np.float32","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:15:22.043763Z","iopub.execute_input":"2025-09-25T09:15:22.044018Z","iopub.status.idle":"2025-09-25T09:15:22.047517Z","shell.execute_reply.started":"2025-09-25T09:15:22.044002Z","shell.execute_reply":"2025-09-25T09:15:22.046868Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# 1) Decide which columns are allowed inside the SEQUENCE\n#    (NO future labels; NO GBM-style lag columns; we keep \"current/past\" variables\n#     and time/categorical signals; the model will see the last L rows as a sequence)\nbase_cols = [TARGET_COL] + EXOG_NUM + CAT_COLS + [\n    'hour','dow','dom','month','doy','hour_sin','hour_cos','dow_sin','dow_cos','doy_sin','doy_cos'\n]\nuse_cols = [c for c in base_cols if c in df.columns]\n\ndf_seq = df[use_cols + [TIME_COL]].copy()\n\n# 2) One-hot the categoricals (strings) for NN input\ndf_seq = pd.get_dummies(df_seq, columns=[c for c in CAT_COLS if c in df_seq.columns], drop_first=False, dtype=np.int8)\ndf_seq","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:15:24.278300Z","iopub.execute_input":"2025-09-25T09:15:24.278552Z","iopub.status.idle":"2025-09-25T09:15:24.337354Z","shell.execute_reply.started":"2025-09-25T09:15:24.278537Z","shell.execute_reply":"2025-09-25T09:15:24.336727Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"       total_consumption_mw  Temperature  Precipitation  Humidity      Wind  \\\n0                   1790.10         25.4            0.6      89.0  2.194444   \n1                   1452.26         25.1            0.0      90.0  2.611111   \n2                   1483.75         24.7            0.0      91.0  2.805556   \n3                   1890.07         24.5            0.0      92.0  2.611111   \n4                   1371.23         24.1            0.0      93.0  2.305556   \n...                     ...          ...            ...       ...       ...   \n47444               2449.28         26.8            0.0      84.0  1.888889   \n47445               2554.05         26.5            0.0      86.0  2.388889   \n47446               1895.41         26.1            0.0      88.0  2.388889   \n47447               1558.67         25.6            0.5      91.0  2.305556   \n47448               1362.71         25.3            0.1      87.0  2.500000   \n\n       Wind Gust  Wind Degree  Cloud Cover  Visibility  hour  ...  \\\n0       4.388889        295.0         89.0         9.0     0  ...   \n1       5.111111        297.0         34.0        10.0     1  ...   \n2       5.500000        309.0         87.0        10.0     2  ...   \n3       4.888889        325.0         71.0        10.0     3  ...   \n4       4.000000        326.0        100.0        10.0     4  ...   \n...          ...          ...          ...         ...   ...  ...   \n47444   3.388889        109.0         56.0        10.0    20  ...   \n47445   4.194444        126.0         52.0        10.0    21  ...   \n47446   4.194444        149.0         84.0        10.0    22  ...   \n47447   4.111111        152.0         76.0         9.0    23  ...   \n47448   4.305556        335.0        100.0        10.0     0  ...   \n\n       Wind Direction_SW  Wind Direction_W  Wind Direction_WNW  season_autumn  \\\n0                      0                 0                   1              0   \n1                      0                 0                   1              0   \n2                      0                 0                   0              0   \n3                      0                 0                   0              0   \n4                      0                 0                   0              0   \n...                  ...               ...                 ...            ...   \n47444                  0                 0                   0              0   \n47445                  0                 0                   0              0   \n47446                  0                 0                   0              0   \n47447                  0                 0                   0              0   \n47448                  0                 0                   0              0   \n\n       season_spring  season_summer  season_winter  is_holiday_False  \\\n0                  0              0              1                 1   \n1                  0              0              1                 1   \n2                  0              0              1                 1   \n3                  0              0              1                 1   \n4                  0              0              1                 1   \n...              ...            ...            ...               ...   \n47444              1              0              0                 1   \n47445              1              0              0                 1   \n47446              1              0              0                 1   \n47447              1              0              0                 1   \n47448              1              0              0                 1   \n\n       is_holiday_national  is_holiday_tet  \n0                        0               0  \n1                        0               0  \n2                        0               0  \n3                        0               0  \n4                        0               0  \n...                    ...             ...  \n47444                    0               0  \n47445                    0               0  \n47446                    0               0  \n47447                    0               0  \n47448                    0               0  \n\n[47449 rows x 49 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>total_consumption_mw</th>\n      <th>Temperature</th>\n      <th>Precipitation</th>\n      <th>Humidity</th>\n      <th>Wind</th>\n      <th>Wind Gust</th>\n      <th>Wind Degree</th>\n      <th>Cloud Cover</th>\n      <th>Visibility</th>\n      <th>hour</th>\n      <th>...</th>\n      <th>Wind Direction_SW</th>\n      <th>Wind Direction_W</th>\n      <th>Wind Direction_WNW</th>\n      <th>season_autumn</th>\n      <th>season_spring</th>\n      <th>season_summer</th>\n      <th>season_winter</th>\n      <th>is_holiday_False</th>\n      <th>is_holiday_national</th>\n      <th>is_holiday_tet</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1790.10</td>\n      <td>25.4</td>\n      <td>0.6</td>\n      <td>89.0</td>\n      <td>2.194444</td>\n      <td>4.388889</td>\n      <td>295.0</td>\n      <td>89.0</td>\n      <td>9.0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1452.26</td>\n      <td>25.1</td>\n      <td>0.0</td>\n      <td>90.0</td>\n      <td>2.611111</td>\n      <td>5.111111</td>\n      <td>297.0</td>\n      <td>34.0</td>\n      <td>10.0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1483.75</td>\n      <td>24.7</td>\n      <td>0.0</td>\n      <td>91.0</td>\n      <td>2.805556</td>\n      <td>5.500000</td>\n      <td>309.0</td>\n      <td>87.0</td>\n      <td>10.0</td>\n      <td>2</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1890.07</td>\n      <td>24.5</td>\n      <td>0.0</td>\n      <td>92.0</td>\n      <td>2.611111</td>\n      <td>4.888889</td>\n      <td>325.0</td>\n      <td>71.0</td>\n      <td>10.0</td>\n      <td>3</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1371.23</td>\n      <td>24.1</td>\n      <td>0.0</td>\n      <td>93.0</td>\n      <td>2.305556</td>\n      <td>4.000000</td>\n      <td>326.0</td>\n      <td>100.0</td>\n      <td>10.0</td>\n      <td>4</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>47444</th>\n      <td>2449.28</td>\n      <td>26.8</td>\n      <td>0.0</td>\n      <td>84.0</td>\n      <td>1.888889</td>\n      <td>3.388889</td>\n      <td>109.0</td>\n      <td>56.0</td>\n      <td>10.0</td>\n      <td>20</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>47445</th>\n      <td>2554.05</td>\n      <td>26.5</td>\n      <td>0.0</td>\n      <td>86.0</td>\n      <td>2.388889</td>\n      <td>4.194444</td>\n      <td>126.0</td>\n      <td>52.0</td>\n      <td>10.0</td>\n      <td>21</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>47446</th>\n      <td>1895.41</td>\n      <td>26.1</td>\n      <td>0.0</td>\n      <td>88.0</td>\n      <td>2.388889</td>\n      <td>4.194444</td>\n      <td>149.0</td>\n      <td>84.0</td>\n      <td>10.0</td>\n      <td>22</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>47447</th>\n      <td>1558.67</td>\n      <td>25.6</td>\n      <td>0.5</td>\n      <td>91.0</td>\n      <td>2.305556</td>\n      <td>4.111111</td>\n      <td>152.0</td>\n      <td>76.0</td>\n      <td>9.0</td>\n      <td>23</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>47448</th>\n      <td>1362.71</td>\n      <td>25.3</td>\n      <td>0.1</td>\n      <td>87.0</td>\n      <td>2.500000</td>\n      <td>4.305556</td>\n      <td>335.0</td>\n      <td>100.0</td>\n      <td>10.0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>47449 rows × 49 columns</p>\n</div>"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"# 3) Make sure \"is_weekend\" exists (optional but useful)\nif 'is_weekend' not in df_seq.columns:\n    df_seq['is_weekend'] = (df_seq['dow'] >= 5).astype(np.int8)\n\n# 4) Drop the first L-1 rows and last H rows to allow full windows and labels later\n#    We'll create windows on the fly; indices will map to the \"window end\" time t.\ndf_seq = df_seq.reset_index(drop=True)\ndf_seq","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:15:26.121339Z","iopub.execute_input":"2025-09-25T09:15:26.122005Z","iopub.status.idle":"2025-09-25T09:15:26.147566Z","shell.execute_reply.started":"2025-09-25T09:15:26.121982Z","shell.execute_reply":"2025-09-25T09:15:26.146968Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"       total_consumption_mw  Temperature  Precipitation  Humidity      Wind  \\\n0                   1790.10         25.4            0.6      89.0  2.194444   \n1                   1452.26         25.1            0.0      90.0  2.611111   \n2                   1483.75         24.7            0.0      91.0  2.805556   \n3                   1890.07         24.5            0.0      92.0  2.611111   \n4                   1371.23         24.1            0.0      93.0  2.305556   \n...                     ...          ...            ...       ...       ...   \n47444               2449.28         26.8            0.0      84.0  1.888889   \n47445               2554.05         26.5            0.0      86.0  2.388889   \n47446               1895.41         26.1            0.0      88.0  2.388889   \n47447               1558.67         25.6            0.5      91.0  2.305556   \n47448               1362.71         25.3            0.1      87.0  2.500000   \n\n       Wind Gust  Wind Degree  Cloud Cover  Visibility  hour  ...  \\\n0       4.388889        295.0         89.0         9.0     0  ...   \n1       5.111111        297.0         34.0        10.0     1  ...   \n2       5.500000        309.0         87.0        10.0     2  ...   \n3       4.888889        325.0         71.0        10.0     3  ...   \n4       4.000000        326.0        100.0        10.0     4  ...   \n...          ...          ...          ...         ...   ...  ...   \n47444   3.388889        109.0         56.0        10.0    20  ...   \n47445   4.194444        126.0         52.0        10.0    21  ...   \n47446   4.194444        149.0         84.0        10.0    22  ...   \n47447   4.111111        152.0         76.0         9.0    23  ...   \n47448   4.305556        335.0        100.0        10.0     0  ...   \n\n       Wind Direction_W  Wind Direction_WNW  season_autumn  season_spring  \\\n0                     0                   1              0              0   \n1                     0                   1              0              0   \n2                     0                   0              0              0   \n3                     0                   0              0              0   \n4                     0                   0              0              0   \n...                 ...                 ...            ...            ...   \n47444                 0                   0              0              1   \n47445                 0                   0              0              1   \n47446                 0                   0              0              1   \n47447                 0                   0              0              1   \n47448                 0                   0              0              1   \n\n       season_summer  season_winter  is_holiday_False  is_holiday_national  \\\n0                  0              1                 1                    0   \n1                  0              1                 1                    0   \n2                  0              1                 1                    0   \n3                  0              1                 1                    0   \n4                  0              1                 1                    0   \n...              ...            ...               ...                  ...   \n47444              0              0                 1                    0   \n47445              0              0                 1                    0   \n47446              0              0                 1                    0   \n47447              0              0                 1                    0   \n47448              0              0                 1                    0   \n\n       is_holiday_tet  is_weekend  \n0                   0           0  \n1                   0           0  \n2                   0           0  \n3                   0           0  \n4                   0           0  \n...               ...         ...  \n47444               0           0  \n47445               0           0  \n47446               0           0  \n47447               0           0  \n47448               0           1  \n\n[47449 rows x 50 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>total_consumption_mw</th>\n      <th>Temperature</th>\n      <th>Precipitation</th>\n      <th>Humidity</th>\n      <th>Wind</th>\n      <th>Wind Gust</th>\n      <th>Wind Degree</th>\n      <th>Cloud Cover</th>\n      <th>Visibility</th>\n      <th>hour</th>\n      <th>...</th>\n      <th>Wind Direction_W</th>\n      <th>Wind Direction_WNW</th>\n      <th>season_autumn</th>\n      <th>season_spring</th>\n      <th>season_summer</th>\n      <th>season_winter</th>\n      <th>is_holiday_False</th>\n      <th>is_holiday_national</th>\n      <th>is_holiday_tet</th>\n      <th>is_weekend</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1790.10</td>\n      <td>25.4</td>\n      <td>0.6</td>\n      <td>89.0</td>\n      <td>2.194444</td>\n      <td>4.388889</td>\n      <td>295.0</td>\n      <td>89.0</td>\n      <td>9.0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1452.26</td>\n      <td>25.1</td>\n      <td>0.0</td>\n      <td>90.0</td>\n      <td>2.611111</td>\n      <td>5.111111</td>\n      <td>297.0</td>\n      <td>34.0</td>\n      <td>10.0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1483.75</td>\n      <td>24.7</td>\n      <td>0.0</td>\n      <td>91.0</td>\n      <td>2.805556</td>\n      <td>5.500000</td>\n      <td>309.0</td>\n      <td>87.0</td>\n      <td>10.0</td>\n      <td>2</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1890.07</td>\n      <td>24.5</td>\n      <td>0.0</td>\n      <td>92.0</td>\n      <td>2.611111</td>\n      <td>4.888889</td>\n      <td>325.0</td>\n      <td>71.0</td>\n      <td>10.0</td>\n      <td>3</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1371.23</td>\n      <td>24.1</td>\n      <td>0.0</td>\n      <td>93.0</td>\n      <td>2.305556</td>\n      <td>4.000000</td>\n      <td>326.0</td>\n      <td>100.0</td>\n      <td>10.0</td>\n      <td>4</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>47444</th>\n      <td>2449.28</td>\n      <td>26.8</td>\n      <td>0.0</td>\n      <td>84.0</td>\n      <td>1.888889</td>\n      <td>3.388889</td>\n      <td>109.0</td>\n      <td>56.0</td>\n      <td>10.0</td>\n      <td>20</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>47445</th>\n      <td>2554.05</td>\n      <td>26.5</td>\n      <td>0.0</td>\n      <td>86.0</td>\n      <td>2.388889</td>\n      <td>4.194444</td>\n      <td>126.0</td>\n      <td>52.0</td>\n      <td>10.0</td>\n      <td>21</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>47446</th>\n      <td>1895.41</td>\n      <td>26.1</td>\n      <td>0.0</td>\n      <td>88.0</td>\n      <td>2.388889</td>\n      <td>4.194444</td>\n      <td>149.0</td>\n      <td>84.0</td>\n      <td>10.0</td>\n      <td>22</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>47447</th>\n      <td>1558.67</td>\n      <td>25.6</td>\n      <td>0.5</td>\n      <td>91.0</td>\n      <td>2.305556</td>\n      <td>4.111111</td>\n      <td>152.0</td>\n      <td>76.0</td>\n      <td>9.0</td>\n      <td>23</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>47448</th>\n      <td>1362.71</td>\n      <td>25.3</td>\n      <td>0.1</td>\n      <td>87.0</td>\n      <td>2.500000</td>\n      <td>4.305556</td>\n      <td>335.0</td>\n      <td>100.0</td>\n      <td>10.0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>47449 rows × 50 columns</p>\n</div>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# 5) Build the feature matrix F and target vector y from the same df (no leakage)\nfeature_cols = [c for c in df_seq.columns if c not in [TIME_COL, TARGET_COL]]\nF_all = df_seq[feature_cols].astype(DTYPE).values                 # (T, D)\ny_all = df_seq[[TARGET_COL]].astype(DTYPE).values                 # (T, 1)\nt_all = df_seq[TIME_COL].to_numpy()     ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:15:27.452468Z","iopub.execute_input":"2025-09-25T09:15:27.453142Z","iopub.status.idle":"2025-09-25T09:15:27.464843Z","shell.execute_reply.started":"2025-09-25T09:15:27.453118Z","shell.execute_reply":"2025-09-25T09:15:27.464042Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# 6) Sliding windows: [t-L+1..t] -> [t+1..t+H]\ndef make_windows(F, y, times, L, H):\n    T, D = F.shape\n    N = T - L - H + 1\n    X = np.empty((N, L, D), dtype=DTYPE)\n    Y = np.empty((N, H), dtype=DTYPE)\n    end_ts = np.empty(N, dtype='datetime64[ns]')  # timestamp at t\n    for i in range(N):\n        X[i] = F[i:i+L]\n        Y[i] = y[i+L:i+L+H, 0]\n        end_ts[i] = times[i+L-1]\n    return X, Y, end_ts\n\nX_raw, Y_raw, end_times = make_windows(F_all, y_all, t_all, L, HORIZON)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:15:29.298545Z","iopub.execute_input":"2025-09-25T09:15:29.299087Z","iopub.status.idle":"2025-09-25T09:15:29.512903Z","shell.execute_reply.started":"2025-09-25T09:15:29.299064Z","shell.execute_reply":"2025-09-25T09:15:29.512311Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"X_raw.shape, end_times","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T11:33:53.667034Z","iopub.execute_input":"2025-09-25T11:33:53.667335Z","iopub.status.idle":"2025-09-25T11:33:53.672918Z","shell.execute_reply.started":"2025-09-25T11:33:53.667315Z","shell.execute_reply":"2025-09-25T11:33:53.672160Z"}},"outputs":[{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"((47378, 48, 48),\n array(['2020-01-02T23:00:00.000000000', '2020-01-03T00:00:00.000000000',\n        '2020-01-03T01:00:00.000000000', ...,\n        '2025-05-29T22:00:00.000000000', '2025-05-29T23:00:00.000000000',\n        '2025-05-30T00:00:00.000000000'], dtype='datetime64[ns]'))"},"metadata":{}}],"execution_count":62},{"cell_type":"code","source":"# 7) \ncut_train_ts = df_feat.iloc[train_end-1][TIME_COL]\ncut_valid_ts = df_feat.iloc[valid_end-1][TIME_COL]\n\ntrain_mask = (end_times <= np.datetime64(cut_train_ts))\nvalid_mask = (end_times >  np.datetime64(cut_train_ts)) & (end_times <= np.datetime64(cut_valid_ts))\ntest_mask  = (end_times >  np.datetime64(cut_valid_ts))\n\nX_train, Y_train = X_raw[train_mask], Y_raw[train_mask]\nX_val,   Y_val   = X_raw[valid_mask], Y_raw[valid_mask]\nX_test,  Y_test  = X_raw[test_mask],  Y_raw[test_mask]\n\nprint(\"Sequence shapes:\", X_train.shape, Y_train.shape, \"|\", X_val.shape, Y_val.shape, \"|\", X_test.shape, Y_test.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:15:32.493230Z","iopub.execute_input":"2025-09-25T09:15:32.493482Z","iopub.status.idle":"2025-09-25T09:15:32.629330Z","shell.execute_reply.started":"2025-09-25T09:15:32.493466Z","shell.execute_reply":"2025-09-25T09:15:32.628709Z"}},"outputs":[{"name":"stdout","text":"Sequence shapes: (44690, 48, 48) (44690, 24) | (1344, 48, 48) (1344, 24) | (1344, 48, 48) (1344, 24)\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"test_times = end_times[test_mask]\nprint(\"First Transformer test timestamp:\", test_times[0])\nprint(\"Last Transformer test timestamp:\", test_times[-1])\nprint(\"First GBM test timestamp:\", df_feat.iloc[valid_end][TIME_COL])\nprint(\"Last GBM test timestamp:\", df_feat.iloc[-1][TIME_COL])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:15:34.518033Z","iopub.execute_input":"2025-09-25T09:15:34.518603Z","iopub.status.idle":"2025-09-25T09:15:34.524069Z","shell.execute_reply.started":"2025-09-25T09:15:34.518580Z","shell.execute_reply":"2025-09-25T09:15:34.523265Z"}},"outputs":[{"name":"stdout","text":"First Transformer test timestamp: 2025-04-04T01:00:00.000000000\nLast Transformer test timestamp: 2025-05-30T00:00:00.000000000\nFirst GBM test timestamp: 2025-04-04 01:00:00\nLast GBM test timestamp: 2025-05-30 00:00:00\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# 8) Scale features and targets using TRAIN ONLY (good practice for NNs)\nx_scaler = StandardScaler().fit(X_train.reshape(-1, X_train.shape[-1]))\nX_train = x_scaler.transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape).astype(DTYPE)\nX_val   = x_scaler.transform(X_val.reshape(-1, X_val.shape[-1])).reshape(X_val.shape).astype(DTYPE)\nX_test  = x_scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape).astype(DTYPE)\n\ny_scaler = StandardScaler().fit(Y_train.reshape(-1,1))\nY_train_s = y_scaler.transform(Y_train.reshape(-1,1)).reshape(Y_train.shape).astype(DTYPE)\nY_val_s   = y_scaler.transform(Y_val.reshape(-1,1)).reshape(Y_val.shape).astype(DTYPE)\nY_test_s  = y_scaler.transform(Y_test.reshape(-1,1)).reshape(Y_test.shape).astype(DTYPE)\n\nprint(\"Scaled shapes:\", X_train.shape, Y_train_s.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:15:35.747596Z","iopub.execute_input":"2025-09-25T09:15:35.747885Z","iopub.status.idle":"2025-09-25T09:15:37.852092Z","shell.execute_reply.started":"2025-09-25T09:15:35.747864Z","shell.execute_reply":"2025-09-25T09:15:37.851390Z"}},"outputs":[{"name":"stdout","text":"Scaled shapes: (44690, 48, 48) (44690, 24)\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"print(\"Scaled shapes:\", X_val.shape, X_test.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T20:08:07.644265Z","iopub.execute_input":"2025-09-24T20:08:07.644526Z","iopub.status.idle":"2025-09-24T20:08:07.648912Z","shell.execute_reply.started":"2025-09-24T20:08:07.644509Z","shell.execute_reply":"2025-09-24T20:08:07.648193Z"}},"outputs":[{"name":"stdout","text":"Scaled shapes: (1344, 48, 48) (1344, 48, 48)\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# 9) (Optional but recommended) Build known-ahead future covariates Z for each horizon h=1..H\n#    Use ONLY features known at t+h: calendar signals + holiday/season (not observed weather).\ndef build_future_covariates(times, H):\n    # times here should be the \"window end\" timestamps: we’ll advance by +h hours\n    # Features: hour/dow/doy sin/cos, is_weekend, and one-hots for is_holiday and season if present\n    # We'll reuse df to look up is_holiday/season at t+h\n    df_key = df[[TIME_COL, 'is_holiday', 'season']].copy()\n    df_key[TIME_COL] = pd.to_datetime(df_key[TIME_COL])\n    df_key = df_key.set_index(TIME_COL).sort_index()\n\n    Z_list = []\n    for ts in times:\n        row = []\n        for h in range(1, H+1):\n            t_h = pd.Timestamp(ts) + pd.Timedelta(hours=h)\n            hour = t_h.hour; dow = t_h.dayofweek; doy = t_h.dayofyear\n            v = [\n                np.sin(2*np.pi*hour/24), np.cos(2*np.pi*hour/24),\n                np.sin(2*np.pi*dow/7),  np.cos(2*np.pi*dow/7),\n                np.sin(2*np.pi*doy/365.25), np.cos(2*np.pi*doy/365.25),\n                1 if dow>=5 else 0,  # is_weekend\n            ]\n            # holiday + season one-hots from the calendar row if present\n            if t_h in df_key.index:\n                hol = str(df_key.loc[t_h, 'is_holiday'])\n                sea = str(df_key.loc[t_h, 'season'])\n            else:\n                hol, sea = \"False\", None\n            # map to fixed one-hot order\n            hol_vec = [int(hol==\"False\"), int(hol==\"tet\"), int(hol==\"national\")]\n            sea_vec = [int(sea==s) for s in [\"winter\",\"spring\",\"summer\",\"autumn\"]] if sea is not None else [0,0,0,0]\n            v += hol_vec + sea_vec\n            row.append(v)\n        Z_list.append(row)\n    Z = np.array(Z_list, dtype=DTYPE)  # (N, H, Ff)\n    return Z\n\nZ_train = build_future_covariates(end_times[train_mask], HORIZON)\nZ_val   = build_future_covariates(end_times[valid_mask], HORIZON)\nZ_test  = build_future_covariates(end_times[test_mask],  HORIZON)\n\nprint(\"Future covariates Z:\", Z_train.shape, Z_val.shape, Z_test.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:15:40.765256Z","iopub.execute_input":"2025-09-25T09:15:40.765529Z","iopub.status.idle":"2025-09-25T09:16:34.036756Z","shell.execute_reply.started":"2025-09-25T09:15:40.765506Z","shell.execute_reply":"2025-09-25T09:16:34.035970Z"}},"outputs":[{"name":"stdout","text":"Future covariates Z: (44690, 24, 14) (1344, 24, 14) (1344, 24, 14)\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# 10) Bundle for later inverse-transform\nclass Bundle: pass\nbundle = Bundle()\nbundle.y_scaler = y_scaler\nbundle.x_scaler = x_scaler\nbundle.L = L\nbundle.H = HORIZON\nbundle.feature_cols = feature_cols","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:16:54.802474Z","iopub.execute_input":"2025-09-25T09:16:54.803314Z","iopub.status.idle":"2025-09-25T09:16:54.807111Z","shell.execute_reply.started":"2025-09-25T09:16:54.803287Z","shell.execute_reply":"2025-09-25T09:16:54.806467Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"# Train LSTM + Attention","metadata":{"id":"ZyWArOKIZiHb"}},{"cell_type":"markdown","source":"## Baseline LSTM + attention","metadata":{"id":"9jr9aY8Ibzye"}},{"cell_type":"code","source":"# train_lstm_attention.py\n# Baseline LSTM + Attention for 24-step (multi-horizon) regression.\n# Expects: X_train, Y_train_s, X_val, Y_val_s, X_test, Y_test_s, bundle (with y_scaler)\n\nimport math, os, random, numpy as np, torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import autocast, GradScaler\n\n# ---------- Repro ----------\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\ntorch.backends.cudnn.benchmark = True  # speed for fixed shapes\n\n# ---------- If your arrays are not in memory -----------\n# import numpy as np, joblib\n# X_train = np.load(\"X_train.npy\");  Y_train_s = np.load(\"Y_train_s.npy\")\n# X_val   = np.load(\"X_val.npy\");    Y_val_s   = np.load(\"Y_val_s.npy\")\n# X_test  = np.load(\"X_test.npy\");   Y_test_s  = np.load(\"Y_test_s.npy\")\n# bundle  = joblib.load(\"preproc_bundle.joblib\")  # with y_scaler\n\n# ---------- Tiny dataset wrapper ----------\nclass SeqDataset(Dataset):\n    def __init__(self, X, Y):\n        self.X = torch.from_numpy(X)        # (N, L, D)\n        self.Y = torch.from_numpy(Y)        # (N, H)\n    def __len__(self): return self.X.shape[0]\n    def __getitem__(self, i): return self.X[i], self.Y[i]\n\n# ---------- LSTM + simple dot-product attention ----------\nclass LSTMAttn(nn.Module):\n    def __init__(self, input_dim, hidden=128, layers=1, dropout=0.2, horizon=24):\n        super().__init__()\n        self.lstm = nn.LSTM(\n            input_size=input_dim,\n            hidden_size=hidden,\n            num_layers=layers,\n            batch_first=True,\n            dropout=dropout if layers > 1 else 0.0,\n        )\n        self.dropout = nn.Dropout(dropout)\n        self.head = nn.Sequential(\n            nn.Linear(2*hidden, 128),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(128, horizon),  # direct multi-output\n        )\n\n    def forward(self, x):                     # x: (B, L, D)\n        seq, (h_n, _) = self.lstm(x)          # seq: (B, L, H), h_n[-1]: (B, H)\n        q = h_n[-1]                            # query = last hidden state\n        # temporal (across L) scaled dot-product attention\n        scores = torch.sum(seq * q.unsqueeze(1), dim=-1) / math.sqrt(seq.size(-1))  # (B, L)\n        w = torch.softmax(scores, dim=1)       # (B, L)\n        ctx = torch.sum(seq * w.unsqueeze(-1), dim=1)                                # (B, H)\n        out = self.head(self.dropout(torch.cat([ctx, q], dim=-1)))                   # (B, HORIZON)\n        return out, w\n\n# ---------- Training / eval helpers ----------\ndef make_loaders(Xtr, Ytr, Xva, Yva, batch=128, workers=0):\n    return (\n        DataLoader(SeqDataset(Xtr, Ytr), batch_size=batch, shuffle=True,  num_workers=workers, pin_memory=True),\n        DataLoader(SeqDataset(Xva, Yva), batch_size=batch, shuffle=False, num_workers=workers, pin_memory=True),\n    )\n\n@torch.no_grad()\ndef eval_epoch(model, loader, device):\n    model.eval()\n    mse, n = 0.0, 0\n    for x, y in loader:\n        x, y = x.to(device), y.to(device)\n        yhat, _ = model(x)\n        mse += torch.mean((yhat - y)**2).item() * x.size(0)\n        n += x.size(0)\n    return mse / n\n\ndef train_model(\n    X_train, Y_train_s, X_val, Y_val_s, *,\n    hidden=128, layers=1, dropout=0.2, batch=128,\n    epochs=40, lr=1e-3, weight_decay=1e-5, grad_clip=1.0,\n):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    L, D = X_train.shape[1], X_train.shape[2]\n    H = Y_train_s.shape[1]\n\n    train_loader, val_loader = make_loaders(X_train, Y_train_s, X_val, Y_val_s, batch=batch)\n\n    model = LSTMAttn(input_dim=D, hidden=hidden, layers=layers, dropout=dropout, horizon=H).to(device)\n    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n    scaler = GradScaler(enabled=(device.type == \"cuda\"))\n    best_state, best_val = None, float(\"inf\")\n    patience, patience_left = 5, 5  # early stopping\n\n    for epoch in range(1, epochs+1):\n        model.train()\n        running, nitems = 0.0, 0\n        for x, y in train_loader:\n            x, y = x.to(device), y.to(device)\n            opt.zero_grad(set_to_none=True)\n            with autocast(enabled=(device.type == \"cuda\")):\n                yhat, _ = model(x)\n                loss = nn.functional.mse_loss(yhat, y)\n            scaler.scale(loss).backward()\n            if grad_clip is not None:\n                scaler.unscale_(opt)\n                nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n            scaler.step(opt); scaler.update()\n            running += loss.item() * x.size(0); nitems += x.size(0)\n\n        train_mse = running / nitems\n        val_mse = eval_epoch(model, val_loader, device)\n        print(f\"Epoch {epoch:02d} | train MSE {train_mse:.4f} | val MSE {val_mse:.4f}\")\n\n        # early stopping on val MSE\n        if val_mse + 1e-6 < best_val:\n            best_val = val_mse\n            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n            patience_left = patience\n        else:\n            patience_left -= 1\n            if patience_left == 0:\n                print(\"Early stopping.\")\n                break\n\n    if best_state is not None:\n        model.load_state_dict(best_state)\n    return model\n\n# ---------- Metrics on original MW ----------\n@torch.no_grad()\ndef evaluate_on_test(model, X_test, Y_test_s, y_scaler, batch=256):\n    device = next(model.parameters()).device\n    loader = DataLoader(SeqDataset(X_test, Y_test_s), batch_size=batch, shuffle=False)\n    model.eval()\n    preds_s, trues_s = [], []\n    for x, y in loader:\n        x = x.to(device)\n        yhat, _ = model(x)\n        preds_s.append(yhat.cpu().numpy())\n        trues_s.append(y.numpy())\n    preds_s = np.concatenate(preds_s, axis=0)\n    trues_s = np.concatenate(trues_s, axis=0)\n\n    # invert scaling to MW\n    preds = y_scaler.inverse_transform(preds_s.reshape(-1,1)).reshape(preds_s.shape)\n    trues = y_scaler.inverse_transform(trues_s.reshape(-1,1)).reshape(trues_s.shape)\n\n    mae = np.mean(np.abs(preds - trues))\n    rmse = np.sqrt(np.mean((preds - trues)**2))\n    # horizon-wise MAE (optional insight)\n    mae_h = np.mean(np.abs(preds - trues), axis=0)   # shape (24,)\n    return {\"MAE\": mae, \"RMSE\": rmse, \"MAE_by_h\": mae_h, \"preds\": preds, \"trues\": trues}\n\n# ---------- Train ----------\nif __name__ == \"__main__\":\n    # If you ran the preprocessing script in the same session, the variables already exist.\n    # Otherwise, see the np.load section near the top.\n\n    # Fast baseline hyperparams (tweak later):\n    model = train_model(\n        X_train, Y_train_s, X_val, Y_val_s,\n        hidden=128, layers=1, dropout=0.2,\n        batch=128, epochs=40, lr=1e-3, weight_decay=1e-5, grad_clip=1.0\n    )\n\n    # Evaluate on test set (in MW)\n    metrics = evaluate_on_test(model, X_test, Y_test_s, bundle.y_scaler)\n    print(f\"\\nTest MAE (MW):  {metrics['MAE']:.2f}\")\n    print(f\"Test RMSE (MW): {metrics['RMSE']:.2f}\")\n    print(\"Horizon-wise MAE (MW):\", np.round(metrics[\"MAE_by_h\"], 2))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X_bK4x-YgcC","executionInfo":{"status":"ok","timestamp":1756110467379,"user_tz":-420,"elapsed":19049,"user":{"displayName":"Phuc L. Nguyen","userId":"06294373354443477970"}},"outputId":"e1a54338-ad55-4dd9-df24-ac503d623e99","trusted":true,"execution":{"iopub.status.busy":"2025-09-22T19:22:07.238306Z","iopub.execute_input":"2025-09-22T19:22:07.238608Z","iopub.status.idle":"2025-09-22T19:23:04.683417Z","shell.execute_reply.started":"2025-09-22T19:22:07.238561Z","shell.execute_reply":"2025-09-22T19:23:04.682826Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/3939490526.py:90: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler(enabled=(device.type == \"cuda\"))\n/tmp/ipykernel_36/3939490526.py:100: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast(enabled=(device.type == \"cuda\")):\n","output_type":"stream"},{"name":"stdout","text":"Epoch 01 | train MSE 0.2423 | val MSE 0.1407\nEpoch 02 | train MSE 0.1526 | val MSE 0.1114\nEpoch 03 | train MSE 0.1444 | val MSE 0.1039\nEpoch 04 | train MSE 0.1399 | val MSE 0.0909\nEpoch 05 | train MSE 0.1371 | val MSE 0.0940\nEpoch 06 | train MSE 0.1351 | val MSE 0.0781\nEpoch 07 | train MSE 0.1328 | val MSE 0.0789\nEpoch 08 | train MSE 0.1324 | val MSE 0.0752\nEpoch 09 | train MSE 0.1303 | val MSE 0.0753\nEpoch 10 | train MSE 0.1305 | val MSE 0.0800\nEpoch 11 | train MSE 0.1290 | val MSE 0.0726\nEpoch 12 | train MSE 0.1276 | val MSE 0.0916\nEpoch 13 | train MSE 0.1274 | val MSE 0.0701\nEpoch 14 | train MSE 0.1238 | val MSE 0.0661\nEpoch 15 | train MSE 0.1231 | val MSE 0.0693\nEpoch 16 | train MSE 0.1218 | val MSE 0.0690\nEpoch 17 | train MSE 0.1208 | val MSE 0.0643\nEpoch 18 | train MSE 0.1182 | val MSE 0.0749\nEpoch 19 | train MSE 0.1186 | val MSE 0.0698\nEpoch 20 | train MSE 0.1173 | val MSE 0.0747\nEpoch 21 | train MSE 0.1154 | val MSE 0.0712\nEpoch 22 | train MSE 0.1148 | val MSE 0.0538\nEpoch 23 | train MSE 0.1127 | val MSE 0.0576\nEpoch 24 | train MSE 0.1132 | val MSE 0.0658\nEpoch 25 | train MSE 0.1119 | val MSE 0.0730\nEpoch 26 | train MSE 0.1118 | val MSE 0.0602\nEpoch 27 | train MSE 0.1107 | val MSE 0.0662\nEarly stopping.\n\nTest MAE (MW):  457.16\nTest RMSE (MW): 640.17\nHorizon-wise MAE (MW): [476.86 489.46 491.72 469.46 453.86 434.61 410.21 420.35 434.1  439.99\n 452.37 475.25 469.11 485.65 479.27 482.78 478.45 461.21 453.66 449.88\n 457.35 432.2  434.35 439.57]\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"# Save model (optional)\ntorch.save(model.state_dict(), \"lstm_attention_baseline.pt\")","metadata":{"id":"BEJustSDZ9Zn","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### with future know ahead covs","metadata":{}},{"cell_type":"code","source":"# train_lstm_attention.py  (minimal Z-aware version)\n\nimport math, os, random, numpy as np, torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import autocast, GradScaler\n\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\ntorch.backends.cudnn.benchmark = True\n\n# ---------- Dataset: optionally carry known-ahead Z ----------\nclass SeqDataset(Dataset):\n    def __init__(self, X, Y, Z_future=None):\n        self.X = torch.from_numpy(X)        # (N, L, D)\n        self.Y = torch.from_numpy(Y)        # (N, H)\n        self.Z = None if Z_future is None else torch.from_numpy(Z_future.astype(np.float32))  # (N, H, Fz)\n        self.has_future = self.Z is not None\n    def __len__(self): return self.X.shape[0]\n    def __getitem__(self, i):\n        if self.has_future: return self.X[i], self.Y[i], self.Z[i]\n        else:                return self.X[i], self.Y[i]\n\ndef make_loaders(Xtr, Ytr, Xva, Yva, Ztr=None, Zva=None, batch=128, workers=0):\n    return (\n        DataLoader(SeqDataset(Xtr, Ytr, Ztr), batch_size=batch, shuffle=True,  num_workers=workers, pin_memory=True, drop_last=True),\n        DataLoader(SeqDataset(Xva, Yva, Zva), batch_size=batch, shuffle=False, num_workers=workers, pin_memory=True),\n    )\n\n# ---------- LSTM + attention, with optional Z fusion ----------\nclass LSTMAttn(nn.Module):\n    def __init__(self, input_dim, hidden=128, layers=1, dropout=0.2, horizon=24, future_feat_dim=0):\n        super().__init__()\n        self.horizon = horizon\n        self.lstm = nn.LSTM(\n            input_size=input_dim,\n            hidden_size=hidden,\n            num_layers=layers,\n            batch_first=True,\n            dropout=dropout if layers > 1 else 0.0,\n        )\n        self.dropout = nn.Dropout(dropout)\n        self.future_proj = nn.Linear(future_feat_dim, hidden) if (future_feat_dim and future_feat_dim > 0) else None\n        head_in = 2*hidden + (hidden if self.future_proj is not None else 0)\n        # per-horizon shared head applied across H (B,H,head_in) -> (B,H,1)\n        self.head = nn.Sequential(\n            nn.LayerNorm(head_in),\n            nn.Linear(head_in, 128),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(128, 1),\n        )\n\n    def forward(self, x, z_future=None):           # x: (B,L,D), z_future: (B,H,Fz) or None\n        seq, (h_n, _) = self.lstm(x)               # seq: (B,L,Hd), h_n[-1]: (B,Hd)\n        q = h_n[-1]                                # (B,Hd)   — query = last hidden\n        # temporal attention over L\n        scores = torch.sum(seq * q.unsqueeze(1), dim=-1) / math.sqrt(seq.size(-1))  # (B,L)\n        w = torch.softmax(scores, dim=1)           # (B,L)\n        ctx = torch.sum(seq * w.unsqueeze(-1), dim=1)                               # (B,Hd)\n\n        B, Hd, H = x.size(0), seq.size(-1), self.horizon\n        base = torch.cat([ctx, q], dim=-1).unsqueeze(1).expand(B, H, -1)            # (B,H,2Hd)\n\n        if self.future_proj is not None and z_future is not None:\n            z_proj = self.future_proj(z_future)                                      # (B,H,Hd)\n            fused = torch.cat([base, z_proj], dim=-1)                                # (B,H,2Hd+Hd)\n        else:\n            fused = base                                                             # (B,H,2Hd)\n\n        yhat = self.head(self.dropout(fused)).squeeze(-1)                            # (B,H)\n        return yhat, w\n\n# ---------- Eval (MSE on scaled) ----------\n@torch.no_grad()\ndef eval_epoch(model, loader, device):\n    model.eval()\n    mse, n = 0.0, 0\n    for batch in loader:\n        if len(batch) == 2: x, y = batch; z = None\n        else:               x, y, z = batch\n        x, y = x.to(device), y.to(device)\n        z = None if z is None else z.to(device)\n        yhat, _ = model(x, z_future=z)\n        mse += torch.mean((yhat - y)**2).item() * x.size(0)\n        n += x.size(0)\n    return mse / n\n\n# ---------- Train ----------\ndef train_model(\n    X_train, Y_train_s, X_val, Y_val_s, *,\n    Z_train=None, Z_val=None,\n    hidden=128, layers=1, dropout=0.2, batch=128,\n    epochs=40, lr=1e-3, weight_decay=1e-5, grad_clip=1.0,\n):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    D = X_train.shape[2]; H = Y_train_s.shape[1]\n    Fz = 0 if Z_train is None else Z_train.shape[2]\n\n    train_loader, val_loader = make_loaders(X_train, Y_train_s, X_val, Y_val_s, Z_train, Z_val, batch=batch)\n\n    model = LSTMAttn(input_dim=D, hidden=hidden, layers=layers, dropout=dropout,\n                     horizon=H, future_feat_dim=Fz).to(device)\n    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n    scaler = GradScaler(enabled=(device.type == \"cuda\"))\n    best_state, best_val = None, float(\"inf\")\n    patience, patience_left = 6, 6\n\n    for epoch in range(1, epochs+1):\n        model.train()\n        running, nitems = 0.0, 0\n        for batch in train_loader:\n            if len(batch) == 2: x, y = batch; z = None\n            else:               x, y, z = batch\n            x, y = x.to(device), y.to(device)\n            z = None if z is None else z.to(device)\n\n            opt.zero_grad(set_to_none=True)\n            with autocast(enabled=(device.type == \"cuda\")):\n                yhat, _ = model(x, z_future=z)\n                loss = nn.functional.mse_loss(yhat, y)\n            scaler.scale(loss).backward()\n            if grad_clip is not None:\n                scaler.unscale_(opt); nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n            scaler.step(opt); scaler.update()\n            running += loss.item() * x.size(0); nitems += x.size(0)\n\n        val_mse = eval_epoch(model, val_loader, device)\n        print(f\"Epoch {epoch:02d} | train MSE {running/nitems:.4f} | val MSE {val_mse:.4f}\")\n\n        if val_mse + 1e-6 < best_val:\n            best_val = val_mse\n            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n            patience_left = patience\n        else:\n            patience_left -= 1\n            if patience_left == 0:\n                print(\"Early stopping.\"); break\n\n    if best_state is not None: model.load_state_dict(best_state)\n    return model\n\n# ---------- Metrics on original MW ----------\n@torch.no_grad()\ndef evaluate_on_test(model, X_test, Y_test_s, y_scaler, Z_test=None, batch=256):\n    device = next(model.parameters()).device\n    loader = DataLoader(SeqDataset(X_test, Y_test_s, Z_test), batch_size=batch, shuffle=False)\n    model.eval()\n    preds_s, trues_s = [], []\n    for batch in loader:\n        if len(batch) == 2: x, y = batch; z = None\n        else:               x, y, z = batch\n        x = x.to(device); z = None if z is None else z.to(device)\n        yhat, _ = model(x, z_future=z)\n        preds_s.append(yhat.cpu().numpy()); trues_s.append(y.numpy())\n    preds_s = np.concatenate(preds_s, axis=0)\n    trues_s = np.concatenate(trues_s, axis=0)\n    preds = y_scaler.inverse_transform(preds_s.reshape(-1,1)).reshape(preds_s.shape)\n    trues = y_scaler.inverse_transform(trues_s.reshape(-1,1)).reshape(trues_s.shape)\n    mae = np.mean(np.abs(preds - trues))\n    rmse = np.sqrt(np.mean((preds - trues)**2))\n    mae_h = np.mean(np.abs(preds - trues), axis=0)\n    return {\"MAE\": mae, \"RMSE\": rmse, \"MAE_by_h\": mae_h, \"preds\": preds, \"trues\": trues}\n\n# ---------- Train ----------\nif __name__ == \"__main__\":\n    model = train_model(\n        X_train, Y_train_s, X_val, Y_val_s,\n        Z_train=Z_train, Z_val=Z_val,\n        hidden=128, layers=1, dropout=0.2,\n        batch=128, epochs=60, lr=8e-4, weight_decay=2e-5, grad_clip=1.0\n    )\n    metrics = evaluate_on_test(model, X_test, Y_test_s, bundle.y_scaler, Z_test=Z_test)\n    print(f\"\\nTest MAE (MW):  {metrics['MAE']:.2f}\")\n    print(f\"Test RMSE (MW): {metrics['RMSE']:.2f}\")\n    print(\"Horizon-wise MAE (MW):\", np.round(metrics[\"MAE_by_h\"], 2))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T17:47:18.892368Z","iopub.execute_input":"2025-09-22T17:47:18.892690Z","iopub.status.idle":"2025-09-22T17:47:42.928898Z","shell.execute_reply.started":"2025-09-22T17:47:18.892669Z","shell.execute_reply":"2025-09-22T17:47:42.928116Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/560264767.py:105: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler(enabled=(device.type == \"cuda\"))\n/tmp/ipykernel_36/560264767.py:119: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast(enabled=(device.type == \"cuda\")):\n","output_type":"stream"},{"name":"stdout","text":"Epoch 01 | train MSE 0.1689 | val MSE 0.0514\nEpoch 02 | train MSE 0.1274 | val MSE 0.0452\nEpoch 03 | train MSE 0.1219 | val MSE 0.0330\nEpoch 04 | train MSE 0.1181 | val MSE 0.0439\nEpoch 05 | train MSE 0.1159 | val MSE 0.0399\nEpoch 06 | train MSE 0.1133 | val MSE 0.0350\nEpoch 07 | train MSE 0.1121 | val MSE 0.0358\nEpoch 08 | train MSE 0.1126 | val MSE 0.0364\nEpoch 09 | train MSE 0.1113 | val MSE 0.0403\nEarly stopping.\n\nTest MAE (MW):  227.78\nTest RMSE (MW): 315.11\nHorizon-wise MAE (MW): [230.06 226.65 219.69 208.23 203.62 206.95 216.19 215.42 210.67 211.08\n 222.24 233.35 234.99 237.77 238.69 242.11 242.98 242.34 239.09 239.89\n 239.44 237.92 235.79 231.56]\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"# train_lstm_attention.py  (minimal Z-aware version)\n\nimport math, os, random, numpy as np, torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import autocast, GradScaler\n\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\ntorch.backends.cudnn.benchmark = True\n\n# ---------- Dataset: optionally carry known-ahead Z ----------\nclass SeqDataset(Dataset):\n    def __init__(self, X, Y, Z_future=None):\n        self.X = torch.from_numpy(X)        # (N, L, D)\n        self.Y = torch.from_numpy(Y)        # (N, H)\n        self.Z = None if Z_future is None else torch.from_numpy(Z_future.astype(np.float32))  # (N, H, Fz)\n        self.has_future = self.Z is not None\n    def __len__(self): return self.X.shape[0]\n    def __getitem__(self, i):\n        if self.has_future: return self.X[i], self.Y[i], self.Z[i]\n        else:                return self.X[i], self.Y[i]\n\ndef make_loaders(Xtr, Ytr, Xva, Yva, Ztr=None, Zva=None, batch=128, workers=0):\n    return (\n        DataLoader(SeqDataset(Xtr, Ytr, Ztr), batch_size=batch, shuffle=True,  num_workers=workers, pin_memory=True, drop_last=True),\n        DataLoader(SeqDataset(Xva, Yva, Zva), batch_size=batch, shuffle=False, num_workers=workers, pin_memory=True),\n    )\n\n# ---------- LSTM + attention, with optional Z fusion ----------\nclass LSTMAttn(nn.Module):\n    def __init__(self, input_dim, hidden=128, layers=1, dropout=0.2, horizon=24, future_feat_dim=0):\n        super().__init__()\n        self.horizon = horizon\n        self.lstm = nn.LSTM(\n            input_size=input_dim,\n            hidden_size=hidden,\n            num_layers=layers,\n            batch_first=True,\n            dropout=dropout if layers > 1 else 0.0,\n        )\n        self.dropout = nn.Dropout(dropout)\n        self.future_proj = nn.Linear(future_feat_dim, hidden) if (future_feat_dim and future_feat_dim > 0) else None\n        head_in = 2*hidden + (hidden if self.future_proj is not None else 0)\n        # per-horizon shared head applied across H (B,H,head_in) -> (B,H,1)\n        self.head = nn.Sequential(\n            nn.LayerNorm(head_in),\n            nn.Linear(head_in, 128),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(128, 1),\n        )\n\n    def forward(self, x, z_future=None):           # x: (B,L,D), z_future: (B,H,Fz) or None\n        seq, (h_n, _) = self.lstm(x)               # seq: (B,L,Hd), h_n[-1]: (B,Hd)\n        q = h_n[-1]                                # (B,Hd)   — query = last hidden\n        # temporal attention over L\n        scores = torch.sum(seq * q.unsqueeze(1), dim=-1) / math.sqrt(seq.size(-1))  # (B,L)\n        w = torch.softmax(scores, dim=1)           # (B,L)\n        ctx = torch.sum(seq * w.unsqueeze(-1), dim=1)                               # (B,Hd)\n\n        B, Hd, H = x.size(0), seq.size(-1), self.horizon\n        base = torch.cat([ctx, q], dim=-1).unsqueeze(1).expand(B, H, -1)            # (B,H,2Hd)\n\n        if self.future_proj is not None and z_future is not None:\n            z_proj = self.future_proj(z_future)                                      # (B,H,Hd)\n            fused = torch.cat([base, z_proj], dim=-1)                                # (B,H,2Hd+Hd)\n        else:\n            fused = base                                                             # (B,H,2Hd)\n\n        yhat = self.head(self.dropout(fused)).squeeze(-1)                            # (B,H)\n        return yhat, w\n\n# ---------- Eval (MSE on scaled) ----------\n@torch.no_grad()\ndef eval_epoch(model, loader, device):\n    model.eval()\n    mse, n = 0.0, 0\n    for batch in loader:\n        if len(batch) == 2: x, y = batch; z = None\n        else:               x, y, z = batch\n        x, y = x.to(device), y.to(device)\n        z = None if z is None else z.to(device)\n        yhat, _ = model(x, z_future=z)\n        mse += torch.mean((yhat - y)**2).item() * x.size(0)\n        n += x.size(0)\n    return mse / n\n\n# ---------- Train ----------\ndef train_model(\n    X_train, Y_train_s, X_val, Y_val_s, *,\n    Z_train=None, Z_val=None,\n    hidden=128, layers=1, dropout=0.2, batch=128,\n    epochs=40, lr=1e-3, weight_decay=1e-5, grad_clip=1.0,\n):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    D = X_train.shape[2]; H = Y_train_s.shape[1]\n    Fz = 0 if Z_train is None else Z_train.shape[2]\n\n    train_loader, val_loader = make_loaders(X_train, Y_train_s, X_val, Y_val_s, Z_train, Z_val, batch=batch)\n\n    model = LSTMAttn(input_dim=D, hidden=hidden, layers=layers, dropout=dropout,\n                     horizon=H, future_feat_dim=Fz).to(device)\n    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n    scaler = GradScaler(enabled=(device.type == \"cuda\"))\n    best_state, best_val = None, float(\"inf\")\n    patience, patience_left = 6, 6\n\n    for epoch in range(1, epochs+1):\n        model.train()\n        running, nitems = 0.0, 0\n        for batch in train_loader:\n            if len(batch) == 2: x, y = batch; z = None\n            else:               x, y, z = batch\n            x, y = x.to(device), y.to(device)\n            z = None if z is None else z.to(device)\n\n            opt.zero_grad(set_to_none=True)\n            with autocast(enabled=(device.type == \"cuda\")):\n                yhat, _ = model(x, z_future=z)\n                loss = nn.functional.mse_loss(yhat, y)\n            scaler.scale(loss).backward()\n            if grad_clip is not None:\n                scaler.unscale_(opt); nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n            scaler.step(opt); scaler.update()\n            running += loss.item() * x.size(0); nitems += x.size(0)\n\n        val_mse = eval_epoch(model, val_loader, device)\n        print(f\"Epoch {epoch:02d} | train MSE {running/nitems:.4f} | val MSE {val_mse:.4f}\")\n\n        if val_mse + 1e-6 < best_val:\n            best_val = val_mse\n            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n            patience_left = patience\n        else:\n            patience_left -= 1\n            if patience_left == 0:\n                print(\"Early stopping.\"); break\n\n    if best_state is not None: model.load_state_dict(best_state)\n    return model\n\n# ---------- Metrics on original MW ----------\n@torch.no_grad()\ndef evaluate_on_test(model, X_test, Y_test_s, y_scaler, Z_test=None, batch=256):\n    device = next(model.parameters()).device\n    loader = DataLoader(SeqDataset(X_test, Y_test_s, Z_test), batch_size=batch, shuffle=False)\n    model.eval()\n    preds_s, trues_s = [], []\n    for batch in loader:\n        if len(batch) == 2: x, y = batch; z = None\n        else:               x, y, z = batch\n        x = x.to(device); z = None if z is None else z.to(device)\n        yhat, _ = model(x, z_future=z)\n        preds_s.append(yhat.cpu().numpy()); trues_s.append(y.numpy())\n    preds_s = np.concatenate(preds_s, axis=0)\n    trues_s = np.concatenate(trues_s, axis=0)\n    preds = y_scaler.inverse_transform(preds_s.reshape(-1,1)).reshape(preds_s.shape)\n    trues = y_scaler.inverse_transform(trues_s.reshape(-1,1)).reshape(trues_s.shape)\n    mae = np.mean(np.abs(preds - trues))\n    rmse = np.sqrt(np.mean((preds - trues)**2))\n    mae_h = np.mean(np.abs(preds - trues), axis=0)\n    return {\"MAE\": mae, \"RMSE\": rmse, \"MAE_by_h\": mae_h, \"preds\": preds, \"trues\": trues}\n\n# ---------- Train ----------\nif __name__ == \"__main__\":\n    model = train_model(\n        X_train, Y_train_s, X_val, Y_val_s,\n        Z_train=Z_train, Z_val=Z_val,\n        hidden=128, layers=1, dropout=0.2,\n        batch=128, epochs=40, lr=1e-3, weight_decay=1e-5, grad_clip=1.0\n    )\n    metrics = evaluate_on_test(model, X_test, Y_test_s, bundle.y_scaler, Z_test=Z_test)\n    print(f\"\\nTest MAE (MW):  {metrics['MAE']:.2f}\")\n    print(f\"Test RMSE (MW): {metrics['RMSE']:.2f}\")\n    print(\"Horizon-wise MAE (MW):\", np.round(metrics[\"MAE_by_h\"], 2))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T17:50:44.142176Z","iopub.execute_input":"2025-09-22T17:50:44.142775Z","iopub.status.idle":"2025-09-22T17:51:18.270760Z","shell.execute_reply.started":"2025-09-22T17:50:44.142751Z","shell.execute_reply":"2025-09-22T17:51:18.270059Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/2034740318.py:105: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler(enabled=(device.type == \"cuda\"))\n/tmp/ipykernel_36/2034740318.py:119: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast(enabled=(device.type == \"cuda\")):\n","output_type":"stream"},{"name":"stdout","text":"Epoch 01 | train MSE 0.1636 | val MSE 0.0478\nEpoch 02 | train MSE 0.1256 | val MSE 0.0495\nEpoch 03 | train MSE 0.1197 | val MSE 0.0347\nEpoch 04 | train MSE 0.1171 | val MSE 0.0431\nEpoch 05 | train MSE 0.1142 | val MSE 0.0436\nEpoch 06 | train MSE 0.1123 | val MSE 0.0387\nEpoch 07 | train MSE 0.1126 | val MSE 0.0296\nEpoch 08 | train MSE 0.1077 | val MSE 0.0400\nEpoch 09 | train MSE 0.1048 | val MSE 0.0395\nEpoch 10 | train MSE 0.1038 | val MSE 0.0366\nEpoch 11 | train MSE 0.1007 | val MSE 0.0380\nEpoch 12 | train MSE 0.0993 | val MSE 0.0316\nEpoch 13 | train MSE 0.0981 | val MSE 0.0348\nEarly stopping.\n\nTest MAE (MW):  207.98\nTest RMSE (MW): 286.86\nHorizon-wise MAE (MW): [207.66 206.95 202.71 197.52 192.69 191.64 195.67 198.64 199.53 201.79\n 205.65 207.73 210.86 212.61 213.83 215.62 216.32 217.51 217.81 219.92\n 218.8  217.73 213.98 208.3 ]\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"# train_lstm_attention.py  (minimal Z-aware version)\n\nimport math, os, random, numpy as np, torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import autocast, GradScaler\n\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\ntorch.backends.cudnn.benchmark = True\n\n# ---------- Dataset: optionally carry known-ahead Z ----------\nclass SeqDataset(Dataset):\n    def __init__(self, X, Y, Z_future=None):\n        self.X = torch.from_numpy(X)        # (N, L, D)\n        self.Y = torch.from_numpy(Y)        # (N, H)\n        self.Z = None if Z_future is None else torch.from_numpy(Z_future.astype(np.float32))  # (N, H, Fz)\n        self.has_future = self.Z is not None\n    def __len__(self): return self.X.shape[0]\n    def __getitem__(self, i):\n        if self.has_future: return self.X[i], self.Y[i], self.Z[i]\n        else:                return self.X[i], self.Y[i]\n\ndef make_loaders(Xtr, Ytr, Xva, Yva, Ztr=None, Zva=None, batch=128, workers=0):\n    return (\n        DataLoader(SeqDataset(Xtr, Ytr, Ztr), batch_size=batch, shuffle=True,  num_workers=workers, pin_memory=True, drop_last=True),\n        DataLoader(SeqDataset(Xva, Yva, Zva), batch_size=batch, shuffle=False, num_workers=workers, pin_memory=True),\n    )\n\n# ---------- LSTM + attention, with optional Z fusion ----------\nclass LSTMAttn(nn.Module):\n    def __init__(self, input_dim, hidden=128, layers=1, dropout=0.2, horizon=24, future_feat_dim=0):\n        super().__init__()\n        self.horizon = horizon\n        self.lstm = nn.LSTM(\n            input_size=input_dim,\n            hidden_size=hidden,\n            num_layers=layers,\n            batch_first=True,\n            dropout=dropout if layers > 1 else 0.0,\n        )\n        self.dropout = nn.Dropout(dropout)\n        self.future_proj = nn.Linear(future_feat_dim, hidden) if (future_feat_dim and future_feat_dim > 0) else None\n        head_in = 2*hidden + (hidden if self.future_proj is not None else 0)\n        # per-horizon shared head applied across H (B,H,head_in) -> (B,H,1)\n        self.head = nn.Sequential(\n            nn.LayerNorm(head_in),\n            nn.Linear(head_in, 128),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(128, 1),\n        )\n\n    def forward(self, x, z_future=None):           # x: (B,L,D), z_future: (B,H,Fz) or None\n        seq, (h_n, _) = self.lstm(x)               # seq: (B,L,Hd), h_n[-1]: (B,Hd)\n        q = h_n[-1]                                # (B,Hd)   — query = last hidden\n        # temporal attention over L\n        scores = torch.sum(seq * q.unsqueeze(1), dim=-1) / math.sqrt(seq.size(-1))  # (B,L)\n        w = torch.softmax(scores, dim=1)           # (B,L)\n        ctx = torch.sum(seq * w.unsqueeze(-1), dim=1)                               # (B,Hd)\n\n        B, Hd, H = x.size(0), seq.size(-1), self.horizon\n        base = torch.cat([ctx, q], dim=-1).unsqueeze(1).expand(B, H, -1)            # (B,H,2Hd)\n\n        if self.future_proj is not None and z_future is not None:\n            z_proj = self.future_proj(z_future)                                      # (B,H,Hd)\n            fused = torch.cat([base, z_proj], dim=-1)                                # (B,H,2Hd+Hd)\n        else:\n            fused = base                                                             # (B,H,2Hd)\n\n        yhat = self.head(self.dropout(fused)).squeeze(-1)                            # (B,H)\n        return yhat, w\n\n# ---------- Eval (MSE on scaled) ----------\n@torch.no_grad()\ndef eval_epoch(model, loader, device):\n    model.eval()\n    mse, n = 0.0, 0\n    for batch in loader:\n        if len(batch) == 2: x, y = batch; z = None\n        else:               x, y, z = batch\n        x, y = x.to(device), y.to(device)\n        z = None if z is None else z.to(device)\n        yhat, _ = model(x, z_future=z)\n        mse += torch.mean((yhat - y)**2).item() * x.size(0)\n        n += x.size(0)\n    return mse / n\n\n# ---------- Train ----------\ndef train_model(\n    X_train, Y_train_s, X_val, Y_val_s, *,\n    Z_train=None, Z_val=None,\n    hidden=128, layers=1, dropout=0.2, batch=128,\n    epochs=40, lr=1e-3, weight_decay=1e-5, grad_clip=1.0,\n):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    D = X_train.shape[2]; H = Y_train_s.shape[1]\n    Fz = 0 if Z_train is None else Z_train.shape[2]\n\n    train_loader, val_loader = make_loaders(X_train, Y_train_s, X_val, Y_val_s, Z_train, Z_val, batch=batch)\n\n    model = LSTMAttn(input_dim=D, hidden=hidden, layers=layers, dropout=dropout,\n                     horizon=H, future_feat_dim=Fz).to(device)\n    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\", factor=0.5, patience=2)\n    scaler = GradScaler(enabled=(device.type == \"cuda\"))\n    best_state, best_val = None, float(\"inf\")\n    patience, patience_left = 6, 6\n\n    loss_fn = nn.SmoothL1Loss(reduction=\"none\")\n    w = torch.linspace(1.15, 0.95, Y_train_s.shape[1], device=device).view(1, -1)\n    for epoch in range(1, epochs+1):\n        model.train()\n        running, nitems = 0.0, 0\n        for batch in train_loader:\n            if len(batch) == 2: x, y = batch; z = None\n            else:               x, y, z = batch\n            x, y = x.to(device), y.to(device)\n            z = None if z is None else z.to(device)\n\n            opt.zero_grad(set_to_none=True)\n            with autocast(enabled=(device.type == \"cuda\")):\n                yhat, _ = model(x, z_future=z)\n                L = loss_fn(yhat, y)   # shape (B,H)\n                L = (L * w).mean()\n            scaler.scale(L).backward()\n            if grad_clip is not None:\n                scaler.unscale_(opt); nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n            scaler.step(opt); scaler.update()\n            running += L.item() * x.size(0); nitems += x.size(0)\n\n        val_mse = eval_epoch(model, val_loader, device)\n        sched.step(val_mse)\n        print(f\"Epoch {epoch:02d} | train MSE {running/nitems:.4f} | val MSE {val_mse:.4f}\")\n        \n        if val_mse + 1e-6 < best_val:\n            best_val = val_mse\n            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n            patience_left = patience\n        else:\n            patience_left -= 1\n            if patience_left == 0:\n                print(\"Early stopping.\"); break\n\n    if best_state is not None: model.load_state_dict(best_state)\n    return model\n\n# ---------- Metrics on original MW ----------\n@torch.no_grad()\ndef evaluate_on_test(model, X_test, Y_test_s, y_scaler, Z_test=None, batch=256):\n    device = next(model.parameters()).device\n    loader = DataLoader(SeqDataset(X_test, Y_test_s, Z_test), batch_size=batch, shuffle=False)\n    model.eval()\n    preds_s, trues_s = [], []\n    for batch in loader:\n        if len(batch) == 2: x, y = batch; z = None\n        else:               x, y, z = batch\n        x = x.to(device); z = None if z is None else z.to(device)\n        yhat, _ = model(x, z_future=z)\n        preds_s.append(yhat.cpu().numpy()); trues_s.append(y.numpy())\n    preds_s = np.concatenate(preds_s, axis=0)\n    trues_s = np.concatenate(trues_s, axis=0)\n    preds = y_scaler.inverse_transform(preds_s.reshape(-1,1)).reshape(preds_s.shape)\n    trues = y_scaler.inverse_transform(trues_s.reshape(-1,1)).reshape(trues_s.shape)\n    mae = np.mean(np.abs(preds - trues))\n    rmse = np.sqrt(np.mean((preds - trues)**2))\n    mae_h = np.mean(np.abs(preds - trues), axis=0)\n    return {\"MAE\": mae, \"RMSE\": rmse, \"MAE_by_h\": mae_h, \"preds\": preds, \"trues\": trues}\n\n# ---------- Train ----------\nif __name__ == \"__main__\":\n    model = train_model(\n        X_train, Y_train_s, X_val, Y_val_s,\n        Z_train=Z_train, Z_val=Z_val,\n        hidden=160, layers=2, dropout=0.15,\n        batch=128, epochs=90, lr=7.5e-4, weight_decay=3e-5, grad_clip=1.0\n    )\n    metrics = evaluate_on_test(model, X_test, Y_test_s, bundle.y_scaler, Z_test=Z_test)\n    print(f\"\\nTest MAE (MW):  {metrics['MAE']:.2f}\")\n    print(f\"Test RMSE (MW): {metrics['RMSE']:.2f}\")\n    print(\"Horizon-wise MAE (MW):\", np.round(metrics[\"MAE_by_h\"], 2))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T19:05:58.019171Z","iopub.execute_input":"2025-09-22T19:05:58.019775Z","iopub.status.idle":"2025-09-22T19:06:41.630860Z","shell.execute_reply.started":"2025-09-22T19:05:58.019750Z","shell.execute_reply":"2025-09-22T19:06:41.630251Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/1396846758.py:106: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler(enabled=(device.type == \"cuda\"))\n/tmp/ipykernel_36/1396846758.py:122: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast(enabled=(device.type == \"cuda\")):\n","output_type":"stream"},{"name":"stdout","text":"Epoch 01 | train MSE 0.0746 | val MSE 0.0528\nEpoch 02 | train MSE 0.0567 | val MSE 0.0374\nEpoch 03 | train MSE 0.0543 | val MSE 0.0334\nEpoch 04 | train MSE 0.0527 | val MSE 0.0287\nEpoch 05 | train MSE 0.0516 | val MSE 0.0310\nEpoch 06 | train MSE 0.0509 | val MSE 0.0277\nEpoch 07 | train MSE 0.0502 | val MSE 0.0512\nEpoch 08 | train MSE 0.0497 | val MSE 0.0442\nEpoch 09 | train MSE 0.0495 | val MSE 0.0316\nEpoch 10 | train MSE 0.0481 | val MSE 0.0342\nEpoch 11 | train MSE 0.0482 | val MSE 0.0377\nEpoch 12 | train MSE 0.0475 | val MSE 0.0352\nEarly stopping.\n\nTest MAE (MW):  209.00\nTest RMSE (MW): 300.62\nHorizon-wise MAE (MW): [198.5  193.88 192.61 193.08 195.92 198.16 202.07 203.41 205.04 208.45\n 213.71 217.04 220.16 221.28 222.77 222.73 223.58 220.69 217.9  213.71\n 210.52 209.2  206.92 204.65]\n","output_type":"stream"}],"execution_count":35},{"cell_type":"markdown","source":"Use this below","metadata":{}},{"cell_type":"code","source":"# train_lstm_attention.py  (minimal Z-aware version)\n\nimport math, os, random, numpy as np, torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import autocast, GradScaler\n\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\ntorch.backends.cudnn.benchmark = True\n\n# ---------- Dataset: optionally carry known-ahead Z ----------\nclass SeqDataset(Dataset):\n    def __init__(self, X, Y, Z_future=None):\n        self.X = torch.from_numpy(X)        # (N, L, D)\n        self.Y = torch.from_numpy(Y)        # (N, H)\n        self.Z = None if Z_future is None else torch.from_numpy(Z_future.astype(np.float32))  # (N, H, Fz)\n        self.has_future = self.Z is not None\n    def __len__(self): return self.X.shape[0]\n    def __getitem__(self, i):\n        if self.has_future: return self.X[i], self.Y[i], self.Z[i]\n        else:                return self.X[i], self.Y[i]\n\ndef make_loaders(Xtr, Ytr, Xva, Yva, Ztr=None, Zva=None, batch=128, workers=0):\n    return (\n        DataLoader(SeqDataset(Xtr, Ytr, Ztr), batch_size=batch, shuffle=True,  num_workers=workers, pin_memory=True, drop_last=True),\n        DataLoader(SeqDataset(Xva, Yva, Zva), batch_size=batch, shuffle=False, num_workers=workers, pin_memory=True),\n    )\n\n# ---------- LSTM + attention, with optional Z fusion ----------\nclass LSTMAttn(nn.Module):\n    def __init__(self, input_dim, hidden=128, layers=1, dropout=0.2, horizon=24, future_feat_dim=0):\n        super().__init__()\n        self.horizon = horizon\n        self.lstm = nn.LSTM(\n            input_size=input_dim,\n            hidden_size=hidden,\n            num_layers=layers,\n            batch_first=True,\n            dropout=dropout if layers > 1 else 0.0,\n        )\n        self.dropout = nn.Dropout(dropout)\n        self.future_proj = nn.Linear(future_feat_dim, hidden) if (future_feat_dim and future_feat_dim > 0) else None\n        head_in = 2*hidden + (hidden if self.future_proj is not None else 0)\n        # per-horizon shared head applied across H (B,H,head_in) -> (B,H,1)\n        self.head = nn.Sequential(\n            nn.LayerNorm(head_in),\n            nn.Linear(head_in, 128),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(128, 1),\n        )\n\n    def forward(self, x, z_future=None):           # x: (B,L,D), z_future: (B,H,Fz) or None\n        seq, (h_n, _) = self.lstm(x)               # seq: (B,L,Hd), h_n[-1]: (B,Hd)\n        q = h_n[-1]                                # (B,Hd)   — query = last hidden\n        # temporal attention over L\n        scores = torch.sum(seq * q.unsqueeze(1), dim=-1) / math.sqrt(seq.size(-1))  # (B,L)\n        w = torch.softmax(scores, dim=1)           # (B,L)\n        ctx = torch.sum(seq * w.unsqueeze(-1), dim=1)                               # (B,Hd)\n\n        B, Hd, H = x.size(0), seq.size(-1), self.horizon\n        base = torch.cat([ctx, q], dim=-1).unsqueeze(1).expand(B, H, -1)            # (B,H,2Hd)\n\n        if self.future_proj is not None and z_future is not None:\n            z_proj = self.future_proj(z_future)                                      # (B,H,Hd)\n            fused = torch.cat([base, z_proj], dim=-1)                                # (B,H,2Hd+Hd)\n        else:\n            fused = base                                                             # (B,H,2Hd)\n\n        yhat = self.head(self.dropout(fused)).squeeze(-1)                            # (B,H)\n        return yhat, w\n\n# ---------- Eval (MSE on scaled) ----------\n@torch.no_grad()\ndef eval_epoch(model, loader, device):\n    model.eval()\n    mse, n = 0.0, 0\n    for batch in loader:\n        if len(batch) == 2: x, y = batch; z = None\n        else:               x, y, z = batch\n        x, y = x.to(device), y.to(device)\n        z = None if z is None else z.to(device)\n        yhat, _ = model(x, z_future=z)\n        mse += torch.mean((yhat - y)**2).item() * x.size(0)\n        n += x.size(0)\n    return mse / n\n\n# ---------- Train ----------\ndef train_model(\n    X_train, Y_train_s, X_val, Y_val_s, *,\n    Z_train=None, Z_val=None,\n    hidden=128, layers=1, dropout=0.2, batch=128,\n    epochs=40, lr=1e-3, weight_decay=1e-5, grad_clip=1.0,\n):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    D = X_train.shape[2]; H = Y_train_s.shape[1]\n    Fz = 0 if Z_train is None else Z_train.shape[2]\n\n    train_loader, val_loader = make_loaders(X_train, Y_train_s, X_val, Y_val_s, Z_train, Z_val, batch=batch)\n\n    model = LSTMAttn(input_dim=D, hidden=hidden, layers=layers, dropout=dropout,\n                     horizon=H, future_feat_dim=Fz).to(device)\n    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n    scaler = GradScaler(enabled=(device.type == \"cuda\"))\n    best_state, best_val = None, float(\"inf\")\n    patience, patience_left = 6, 6\n\n    for epoch in range(1, epochs+1):\n        model.train()\n        running, nitems = 0.0, 0\n        for batch in train_loader:\n            if len(batch) == 2: x, y = batch; z = None\n            else:               x, y, z = batch\n            x, y = x.to(device), y.to(device)\n            z = None if z is None else z.to(device)\n\n            opt.zero_grad(set_to_none=True)\n            with autocast(enabled=(device.type == \"cuda\")):\n                yhat, _ = model(x, z_future=z)\n                loss = nn.functional.mse_loss(yhat, y)\n            scaler.scale(loss).backward()\n            if grad_clip is not None:\n                scaler.unscale_(opt); nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n            scaler.step(opt); scaler.update()\n            running += loss.item() * x.size(0); nitems += x.size(0)\n\n        val_mse = eval_epoch(model, val_loader, device)\n        print(f\"Epoch {epoch:02d} | train MSE {running/nitems:.4f} | val MSE {val_mse:.4f}\")\n        \n        if val_mse + 1e-6 < best_val:\n            best_val = val_mse\n            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n            patience_left = patience\n        else:\n            patience_left -= 1\n            if patience_left == 0:\n                print(\"Early stopping.\"); break\n\n    if best_state is not None: model.load_state_dict(best_state)\n    return model\n\n# ---------- Metrics on original MW ----------\n@torch.no_grad()\ndef evaluate_on_test(model, X_test, Y_test_s, y_scaler, Z_test=None, batch=256):\n    device = next(model.parameters()).device\n    loader = DataLoader(SeqDataset(X_test, Y_test_s, Z_test), batch_size=batch, shuffle=False)\n    model.eval()\n    preds_s, trues_s = [], []\n    for batch in loader:\n        if len(batch) == 2: x, y = batch; z = None\n        else:               x, y, z = batch\n        x = x.to(device); z = None if z is None else z.to(device)\n        yhat, _ = model(x, z_future=z)\n        preds_s.append(yhat.cpu().numpy()); trues_s.append(y.numpy())\n    preds_s = np.concatenate(preds_s, axis=0)\n    trues_s = np.concatenate(trues_s, axis=0)\n    preds = y_scaler.inverse_transform(preds_s.reshape(-1,1)).reshape(preds_s.shape)\n    trues = y_scaler.inverse_transform(trues_s.reshape(-1,1)).reshape(trues_s.shape)\n    def safe_mape(y_true, y_pred, eps=1e-6):\n        y_true = np.asarray(y_true)\n        return np.mean(np.abs((y_true - y_pred) / np.clip(np.abs(y_true), eps, None))) * 100.0\n    \n    mae = np.mean(np.abs(preds - trues))\n    rmse = np.sqrt(np.mean((preds - trues)**2))\n    mae_h = np.mean(np.abs(preds - trues), axis=0)\n    mape = safe_mape(trues, preds)\n    return {\"MAE\": mae, \"RMSE\": rmse, \"MAE_by_h\": mae_h, \"MAPE\": mape, \"preds\": preds, \"trues\": trues}\n\n# ---------- Train ----------\nif __name__ == \"__main__\":\n    model_lstmattention = train_model(\n        X_train, Y_train_s, X_val, Y_val_s,\n        Z_train=Z_train, Z_val=Z_val,\n        hidden=112, layers=2, dropout=0.12,\n        batch=160, epochs=100, lr=9e-4, weight_decay=2e-5, grad_clip=1.0\n    )\n    metrics_lstmattention = evaluate_on_test(model_lstmattention, X_test, Y_test_s, bundle.y_scaler, Z_test=Z_test)\n    print(f\"\\nTest MAE (MW):  {metrics_lstmattention['MAE']:.2f}\")\n    print(f\"Test RMSE (MW): {metrics_lstmattention['RMSE']:.2f}\")\n    print(f\"Test MAPE (MW): {metrics_lstmattention['MAPE']:.2f}\")\n    print(\"Horizon-wise MAE (MW):\", np.round(metrics_lstmattention[\"MAE_by_h\"], 2))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:18:22.047349Z","iopub.execute_input":"2025-09-25T09:18:22.047643Z","iopub.status.idle":"2025-09-25T09:19:13.943365Z","shell.execute_reply.started":"2025-09-25T09:18:22.047620Z","shell.execute_reply":"2025-09-25T09:19:13.942691Z"}},"outputs":[{"name":"stdout","text":"Epoch 01 | train MSE 0.1622 | val MSE 0.0328\nEpoch 02 | train MSE 0.1215 | val MSE 0.0302\nEpoch 03 | train MSE 0.1163 | val MSE 0.0369\nEpoch 04 | train MSE 0.1128 | val MSE 0.0322\nEpoch 05 | train MSE 0.1116 | val MSE 0.0336\nEpoch 06 | train MSE 0.1106 | val MSE 0.0282\nEpoch 07 | train MSE 0.1077 | val MSE 0.0325\nEpoch 08 | train MSE 0.1088 | val MSE 0.0351\nEpoch 09 | train MSE 0.1071 | val MSE 0.0282\nEpoch 10 | train MSE 0.1083 | val MSE 0.0334\nEpoch 11 | train MSE 0.1049 | val MSE 0.0289\nEpoch 12 | train MSE 0.1031 | val MSE 0.0291\nEpoch 13 | train MSE 0.0993 | val MSE 0.0270\nEpoch 14 | train MSE 0.0927 | val MSE 0.0287\nEpoch 15 | train MSE 0.0893 | val MSE 0.0289\nEpoch 16 | train MSE 0.0916 | val MSE 0.0286\nEpoch 17 | train MSE 0.0861 | val MSE 0.0301\nEpoch 18 | train MSE 0.0828 | val MSE 0.0291\nEpoch 19 | train MSE 0.0831 | val MSE 0.0277\nEarly stopping.\n\nTest MAE (MW):  183.55\nTest RMSE (MW): 256.13\nTest MAPE (MW): 7.60\nHorizon-wise MAE (MW): [182.14 181.66 181.04 180.34 181.51 182.5  183.43 184.05 184.49 185.49\n 186.56 187.31 187.18 186.12 185.39 185.43 185.65 184.05 182.69 181.02\n 180.85 181.4  182.23 182.64]\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"torch.save(model_lstmattention.state_dict(), \"lstmattention_forecaster.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:24:05.022907Z","iopub.execute_input":"2025-09-25T09:24:05.023661Z","iopub.status.idle":"2025-09-25T09:24:05.030854Z","shell.execute_reply.started":"2025-09-25T09:24:05.023637Z","shell.execute_reply":"2025-09-25T09:24:05.030281Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"## Light bigger LSTM + attention","metadata":{"id":"wdwVud0JbxDg"}},{"cell_type":"code","source":"# train_lstm_attention_mae.py\nimport math, numpy as np, torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import autocast, GradScaler\n\n# expects: X_train, Y_train_s, X_val, Y_val_s, X_test, Y_test_s, bundle\n\n# ---------- Data ----------\nclass SeqDataset(Dataset):\n    def __init__(self, X, Y): self.X = torch.from_numpy(X); self.Y = torch.from_numpy(Y)\n    def __len__(self): return self.X.shape[0]\n    def __getitem__(self, i): return self.X[i], self.Y[i]\n\ndef loaders(Xtr, Ytr, Xva, Yva, batch=128, workers=0):\n    return (\n        DataLoader(SeqDataset(Xtr,Ytr), batch_size=batch, shuffle=True,  num_workers=workers, pin_memory=True),\n        DataLoader(SeqDataset(Xva,Yva), batch_size=batch, shuffle=False, num_workers=workers, pin_memory=True),\n    )\n\n# ---------- Model (2-layer bi-LSTM + multi-head temporal attention) ----------\nclass MHAttnPool(nn.Module):\n    def __init__(self, d_model, n_heads=4, p=0.15):\n        super().__init__()\n        assert d_model % n_heads == 0\n        self.nh, self.dk = n_heads, d_model // n_heads\n        self.q = nn.Linear(d_model, d_model, bias=False)\n        self.k = nn.Linear(d_model, d_model, bias=False)\n        self.v = nn.Linear(d_model, d_model, bias=False)\n        self.drop = nn.Dropout(p)\n    def forward(self, seq, q):  # seq: (B,L,D), q: (B,D)\n        B,L,D = seq.shape; H,d = self.nh, self.dk\n        Q = self.q(q).view(B,H,d)\n        K = self.k(seq).view(B,L,H,d)\n        V = self.v(seq).view(B,L,H,d)\n        score = torch.einsum(\"bhd,bLhd->bhL\", Q, K) / math.sqrt(d)\n        w = torch.softmax(score, dim=-1)\n        w = self.drop(w)\n        ctx = torch.einsum(\"bhL,bLhd->bhd\", w, V).reshape(B,D)\n        return ctx, w  # (B,D), (B,H,L)\n\nclass BigLSTMAttn(nn.Module):\n    def __init__(self, input_dim, horizon=24, hidden=256, layers=2, heads=4,\n                 dropout=0.30, bidirectional=True):\n        super().__init__()\n        self.bidirectional = bidirectional\n        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden, num_layers=layers,\n                            bidirectional=bidirectional, batch_first=True,\n                            dropout=dropout if layers>1 else 0.0)\n        D = hidden * (2 if bidirectional else 1)\n        self.attn = MHAttnPool(D, n_heads=heads, p=dropout*0.5)\n        self.norm = nn.LayerNorm(D)\n        self.mlp  = nn.Sequential(\n            nn.Linear(D*2, 256), nn.ReLU(), nn.Dropout(dropout),\n            nn.Linear(256, horizon)\n        )\n    def _last(self, h_n):  # (layers*dirs,B,H)\n        layers_dirs, B, H = h_n.shape\n        dirs = 2 if self.bidirectional else 1\n        last = h_n.view(-1, dirs, B, H)[-1]              # (dirs,B,H)\n        return torch.cat([last[d] for d in range(dirs)], dim=1) if dirs==2 else last[0]\n    def forward(self, x):\n        seq, (h_n, _) = self.lstm(x)\n        q = self._last(h_n)\n        ctx, attn = self.attn(seq, q)\n        fused = torch.cat([self.norm(ctx), q], dim=-1)\n        yhat = self.mlp(fused)\n        return yhat, attn\n\n# ---------- Metrics ----------\n@torch.no_grad()\ndef val_metrics_MW(model, loader, y_scaler):\n    model.eval()\n    mse_s, n = 0.0, 0\n    preds_s, trues_s = [], []\n    for x, y in loader:\n        x, y = x.to(next(model.parameters()).device), y.to(next(model.parameters()).device)\n        yhat_s, _ = model(x)\n        mse_s += torch.mean((yhat_s - y)**2).item() * x.size(0); n += x.size(0)\n        preds_s.append(yhat_s.detach().cpu().numpy()); trues_s.append(y.detach().cpu().numpy())\n    preds_s, trues_s = np.concatenate(preds_s), np.concatenate(trues_s)\n    preds = bundle.y_scaler.inverse_transform(preds_s.reshape(-1,1)).reshape(preds_s.shape)\n    trues = bundle.y_scaler.inverse_transform(trues_s.reshape(-1,1)).reshape(trues_s.shape)\n    mae = np.mean(np.abs(preds - trues))\n    return mae, mse_s/n  # (MW), (scaled MSE)\n\n# ---------- Train with early-stop on val MAE (MW) ----------\ndef train(\n    X_train, Y_train_s, X_val, Y_val_s,\n    hidden=256, layers=2, heads=4, dropout=0.30, bidirectional=True,\n    batch=128, epochs=80, lr=1e-3, weight_decay=2e-4, grad_clip=1.0,\n    horizon_weighted=True\n):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    D, H = X_train.shape[2], Y_train_s.shape[1]\n    tr_loader = DataLoader(SeqDataset(X_train, Y_train_s), batch_size=batch, shuffle=True,  pin_memory=True)\n    va_loader = DataLoader(SeqDataset(X_val,   Y_val_s),   batch_size=batch, shuffle=False, pin_memory=True)\n\n    model = BigLSTMAttn(D, horizon=H, hidden=hidden, layers=layers,\n                        heads=heads, dropout=dropout, bidirectional=bidirectional).to(device)\n    opt   = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n    # ↓ remove verbose (older torch doesn't support it)\n    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\", factor=0.5, patience=2)\n    scaler = GradScaler(enabled=(device.type == \"cuda\"))\n\n    loss_fn = nn.SmoothL1Loss(beta=1.0, reduction=\"none\")\n    w = None\n    if horizon_weighted:\n        # weight early horizons a bit more\n        w = torch.linspace(1.25, 0.85, H, device=device).view(1, H)\n\n    best_mae, best_state = float(\"inf\"), None\n    patience, left = 8, 8\n\n    for epoch in range(1, epochs + 1):\n        model.train()\n        run, nitems = 0.0, 0\n        for xb, yb in tr_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            opt.zero_grad(set_to_none=True)\n            with autocast(enabled=(device.type == \"cuda\")):\n                yhat_s, _ = model(xb)\n                L = loss_fn(yhat_s, yb)         # (B,H)\n                if w is not None: L = L * w\n                L = L.mean()\n            scaler.scale(L).backward()\n            if grad_clip:\n                scaler.unscale_(opt)\n                nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n            scaler.step(opt); scaler.update()\n            run += L.item() * xb.size(0); nitems += xb.size(0)\n\n        # ---- validation: early-stop on MAE (MW)\n        val_mae_mw, val_mse_s = val_metrics_MW(model, va_loader, bundle.y_scaler)\n\n        # ---- LR scheduler (log when it reduces LR)\n        prev_lr = opt.param_groups[0][\"lr\"]\n        sched.step(val_mae_mw)\n        new_lr  = opt.param_groups[0][\"lr\"]\n        if new_lr < prev_lr:\n            print(f\"[scheduler] LR reduced: {prev_lr:.2e} → {new_lr:.2e}\")\n\n        print(f\"Epoch {epoch:02d} | train loss {run/nitems:.4f} \"\n              f\"| val MAE(MW) {val_mae_mw:.2f} | val MSE(scaled) {val_mse_s:.4f}\")\n\n        if val_mae_mw + 1e-6 < best_mae:\n            best_mae = val_mae_mw\n            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n            left = patience\n        else:\n            left -= 1\n            if left == 0:\n                print(\"Early stopping.\"); break\n\n    if best_state is not None:\n        model.load_state_dict(best_state)\n    return model\n\n# ---------- Evaluate on test (MW)\n@torch.no_grad()\ndef test_report(model, X_test, Y_test_s, y_scaler, batch=256):\n    dl = DataLoader(SeqDataset(X_test, Y_test_s), batch_size=batch, shuffle=False)\n    device = next(model.parameters()).device\n    preds_s, trues_s = [], []\n    for xb, yb in dl:\n        xb = xb.to(device)\n        yhat_s, _ = model(xb)\n        preds_s.append(yhat_s.cpu().numpy()); trues_s.append(yb.numpy())\n    preds_s, trues_s = np.concatenate(preds_s), np.concatenate(trues_s)\n    preds = y_scaler.inverse_transform(preds_s.reshape(-1,1)).reshape(preds_s.shape)\n    trues = y_scaler.inverse_transform(trues_s.reshape(-1,1)).reshape(trues_s.shape)\n    mae = np.mean(np.abs(preds - trues))\n    rmse = np.sqrt(np.mean((preds - trues)**2))\n    mae_h = np.mean(np.abs(preds - trues), axis=0)\n    print(f\"\\nTest  MAE (MW):  {mae:.2f}\")\n    print(f\"Test RMSE (MW):  {rmse:.2f}\")\n    print(\"Horizon-wise MAE (MW):\", np.round(mae_h, 2))\n    return {\"MAE\": mae, \"RMSE\": rmse, \"MAE_by_h\": mae_h, \"preds\": preds, \"trues\": trues}\n\n# ---------- Run ----------\nif __name__ == \"__main__\":\n    model = train(\n        X_train, Y_train_s, X_val, Y_val_s,\n        hidden=256, layers=2, heads=4, dropout=0.30, bidirectional=True,\n        batch=128, epochs=80, lr=1e-3, weight_decay=2e-4, grad_clip=1.0,\n        horizon_weighted=True\n    )\n    _ = test_report(model, X_test, Y_test_s, bundle.y_scaler)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gPocsyeubnfi","executionInfo":{"status":"ok","timestamp":1756063197553,"user_tz":-420,"elapsed":190764,"user":{"displayName":"Phuc L. Nguyen","userId":"06294373354443477970"}},"outputId":"ee4c24fc-30d1-4535-894e-f6d17631bbad","trusted":true,"execution":{"iopub.status.busy":"2025-09-20T14:59:24.993622Z","iopub.execute_input":"2025-09-20T14:59:24.993894Z","iopub.status.idle":"2025-09-20T15:01:20.792416Z","shell.execute_reply.started":"2025-09-20T14:59:24.993875Z","shell.execute_reply":"2025-09-20T15:01:20.791579Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/2345825755.py:103: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler(enabled=(device.type == \"cuda\"))\n/tmp/ipykernel_36/2345825755.py:120: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast(enabled=(device.type == \"cuda\")):\n","output_type":"stream"},{"name":"stdout","text":"Epoch 01 | train loss 0.1028 | val MAE(MW) 214.86 | val MSE(scaled) 0.0623\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/2345825755.py:120: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast(enabled=(device.type == \"cuda\")):\n","output_type":"stream"},{"name":"stdout","text":"Epoch 02 | train loss 0.0679 | val MAE(MW) 194.68 | val MSE(scaled) 0.0572\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/2345825755.py:120: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast(enabled=(device.type == \"cuda\")):\n","output_type":"stream"},{"name":"stdout","text":"Epoch 03 | train loss 0.0648 | val MAE(MW) 182.88 | val MSE(scaled) 0.0434\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/2345825755.py:120: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast(enabled=(device.type == \"cuda\")):\n","output_type":"stream"},{"name":"stdout","text":"Epoch 04 | train loss 0.0632 | val MAE(MW) 191.77 | val MSE(scaled) 0.0532\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/2345825755.py:120: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast(enabled=(device.type == \"cuda\")):\n","output_type":"stream"},{"name":"stdout","text":"Epoch 05 | train loss 0.0618 | val MAE(MW) 197.32 | val MSE(scaled) 0.0611\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/2345825755.py:120: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast(enabled=(device.type == \"cuda\")):\n","output_type":"stream"},{"name":"stdout","text":"[scheduler] LR reduced: 1.00e-03 → 5.00e-04\nEpoch 06 | train loss 0.0613 | val MAE(MW) 195.51 | val MSE(scaled) 0.0586\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/2345825755.py:120: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast(enabled=(device.type == \"cuda\")):\n","output_type":"stream"},{"name":"stdout","text":"Epoch 07 | train loss 0.0581 | val MAE(MW) 198.03 | val MSE(scaled) 0.0535\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/2345825755.py:120: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast(enabled=(device.type == \"cuda\")):\n","output_type":"stream"},{"name":"stdout","text":"Epoch 08 | train loss 0.0576 | val MAE(MW) 207.59 | val MSE(scaled) 0.0646\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/2345825755.py:120: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast(enabled=(device.type == \"cuda\")):\n","output_type":"stream"},{"name":"stdout","text":"[scheduler] LR reduced: 5.00e-04 → 2.50e-04\nEpoch 09 | train loss 0.0571 | val MAE(MW) 197.29 | val MSE(scaled) 0.0578\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/2345825755.py:120: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast(enabled=(device.type == \"cuda\")):\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10 | train loss 0.0557 | val MAE(MW) 182.44 | val MSE(scaled) 0.0485\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/2345825755.py:120: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast(enabled=(device.type == \"cuda\")):\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11 | train loss 0.0547 | val MAE(MW) 200.66 | val MSE(scaled) 0.0613\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/2345825755.py:120: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast(enabled=(device.type == \"cuda\")):\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12 | train loss 0.0548 | val MAE(MW) 195.16 | val MSE(scaled) 0.0575\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/2345825755.py:120: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast(enabled=(device.type == \"cuda\")):\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13 | train loss 0.0528 | val MAE(MW) 179.04 | val MSE(scaled) 0.0491\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/2345825755.py:120: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast(enabled=(device.type == \"cuda\")):\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14 | train loss 0.0522 | val MAE(MW) 193.16 | val MSE(scaled) 0.0585\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/2345825755.py:120: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast(enabled=(device.type == \"cuda\")):\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15 | train loss 0.0497 | val MAE(MW) 193.02 | val MSE(scaled) 0.0570\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/2345825755.py:120: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast(enabled=(device.type == \"cuda\")):\n","output_type":"stream"},{"name":"stdout","text":"[scheduler] LR reduced: 2.50e-04 → 1.25e-04\nEpoch 16 | train loss 0.0487 | val MAE(MW) 184.98 | val MSE(scaled) 0.0521\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/2345825755.py:120: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast(enabled=(device.type == \"cuda\")):\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17 | train loss 0.0460 | val MAE(MW) 189.19 | val MSE(scaled) 0.0559\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/2345825755.py:120: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast(enabled=(device.type == \"cuda\")):\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18 | train loss 0.0453 | val MAE(MW) 186.58 | val MSE(scaled) 0.0528\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/2345825755.py:120: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast(enabled=(device.type == \"cuda\")):\n","output_type":"stream"},{"name":"stdout","text":"[scheduler] LR reduced: 1.25e-04 → 6.25e-05\nEpoch 19 | train loss 0.0445 | val MAE(MW) 188.23 | val MSE(scaled) 0.0555\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/2345825755.py:120: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast(enabled=(device.type == \"cuda\")):\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20 | train loss 0.0437 | val MAE(MW) 186.39 | val MSE(scaled) 0.0545\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/2345825755.py:120: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast(enabled=(device.type == \"cuda\")):\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21 | train loss 0.0432 | val MAE(MW) 186.22 | val MSE(scaled) 0.0529\nEarly stopping.\n\nTest  MAE (MW):  268.98\nTest RMSE (MW):  380.96\nHorizon-wise MAE (MW): [216.22 222.28 238.67 241.88 246.47 243.83 254.75 283.58 289.61 296.28\n 308.41 309.31 311.46 313.38 320.23 312.05 298.55 283.03 275.32 261.39\n 249.72 237.57 221.82 219.67]\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"  torch.save(model.state_dict(), \"lstm_attn_L48_earlystop_mae.pt\")","metadata":{"id":"3WZoYLQmgBf-","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Transformer (baseline)","metadata":{"id":"u-fi1HBES9jP"}},{"cell_type":"code","source":"torch.save(model.state_dict(), \"transformer_forecaster.pt\")","metadata":{"id":"DDG5E_cZTiB9","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train_transformer_baseline_fixed.py  (minimal deterministic-friendly patch)\n\nimport os, random, math, numpy as np\n\n# ---- Toggle: set True for strict determinism (requires kernel restart) ----\nSTRICT_DETERMINISM = False\n\n# If strict, cuBLAS needs this BEFORE importing torch (and a fresh kernel)\nif STRICT_DETERMINISM:\n    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"  # or \":16:8\"\n\nimport torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# --- seeding (simple & safe) ---\nSEED = 42\ndef seed_everything(seed=42):\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    random.seed(seed); np.random.seed(seed)\n    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n    # keep deterministic cuDNN kernels, but don't force torch.use_deterministic_algorithms\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n    # TF32 off helps reproducibility\n    torch.backends.cuda.matmul.allow_tf32 = False\n    torch.backends.cudnn.allow_tf32 = False\n\nseed_everything(SEED)\n\n# --- AMP (new API with fallback); set USE_AMP=False for tighter reproducibility ---\nUSE_AMP = True\ntry:\n    from torch.amp import autocast, GradScaler\n    def amp_ctx():\n        if not USE_AMP:\n            class _NoOp:\n                def __enter__(self): return None\n                def __exit__(self, *a): return False\n            return _NoOp()\n        use_bf16 = (torch.cuda.is_available()\n                    and torch.cuda.get_device_capability()[0] >= 8)\n        return autocast(device_type=\"cuda\", dtype=torch.bfloat16 if use_bf16 else torch.float16)\nexcept Exception:\n    from torch.cuda.amp import autocast, GradScaler\n    def amp_ctx():\n        return autocast(enabled=(USE_AMP and torch.cuda.is_available()))\n\n# ========== Data ==========\nclass SeqDataset(Dataset):\n    \"\"\"Returns (X, Y) if Z is None; otherwise (X, Y, Z). Avoids collating None.\"\"\"\n    def __init__(self, X, Y, Z_future=None):\n        self.X = torch.from_numpy(X)              # (N, L, D)\n        self.Y = torch.from_numpy(Y)              # (N, H)\n        self.Z = None if Z_future is None else torch.from_numpy(Z_future.astype(np.float32))\n        self.has_future = self.Z is not None\n    def __len__(self): return self.X.shape[0]\n    def __getitem__(self, i):\n        if self.has_future:\n            return self.X[i], self.Y[i], self.Z[i]\n        else:\n            return self.X[i], self.Y[i]\n\ndef _seed_worker(worker_id: int):\n    wseed = torch.initial_seed() % 2**32\n    np.random.seed(wseed); random.seed(wseed)\n\ndef loaders(Xtr,Ytr,Xva,Yva, Ztr=None, Zva=None, batch=128, workers=0, seed: int = SEED):\n    g = torch.Generator().manual_seed(seed)  # deterministic shuffle\n    return (\n        DataLoader(SeqDataset(Xtr,Ytr,Ztr), batch_size=batch, shuffle=True,  drop_last=True,\n                   num_workers=workers, pin_memory=True, worker_init_fn=_seed_worker, generator=g),\n        DataLoader(SeqDataset(Xva,Yva,Zva), batch_size=batch, shuffle=False, drop_last=False,\n                   num_workers=workers, pin_memory=True, worker_init_fn=_seed_worker, generator=g),\n    )\n\n# ========== Model ==========\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=1000):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div); pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))  # (1, max_len, d_model)\n    def forward(self, x): return x + self.pe[:, :x.size(1)]\n\nclass TransformerForecaster(nn.Module):\n    def __init__(self, input_dim, horizon=24, d_model=256, nhead=8, num_layers=3,\n                 d_ff=512, dropout=0.1, future_feat_dim=0):\n        super().__init__()\n        self.horizon = horizon\n        self.input_proj = nn.Linear(input_dim, d_model)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=d_model, nhead=nhead, dim_feedforward=d_ff,\n            dropout=dropout, batch_first=True, norm_first=True\n        )\n        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n        self.pos = PositionalEncoding(d_model)\n        self.cls = nn.Parameter(torch.randn(1, 1, d_model) * 0.02)\n        self.horz_emb = nn.Parameter(torch.randn(horizon, d_model) * 0.02)\n        self.future_proj = nn.Linear(future_feat_dim, d_model) if future_feat_dim and future_feat_dim>0 else None\n        head_in = d_model + d_model + (d_model if self.future_proj else 0)\n        self.head = nn.Sequential(nn.LayerNorm(head_in), nn.Linear(head_in, 256),\n                                  nn.ReLU(), nn.Dropout(dropout), nn.Linear(256, 1))\n\n    def forward(self, x, z_future=None):\n        B, L, _ = x.shape\n        x = self.pos(self.input_proj(x))             # (B,L,d)\n        cls = self.cls.expand(B, 1, -1)\n        enc = self.encoder(torch.cat([cls, x], dim=1))\n        context = enc[:, 0, :]                       # (B,d)\n        H = self.horizon\n        he = self.horz_emb.unsqueeze(0).expand(B, H, -1)  # (B,H,d)\n        ctx = context.unsqueeze(1).expand(B, H, -1)       # (B,H,d)\n        if self.future_proj is not None and z_future is not None:\n            z_proj = self.future_proj(z_future)           # (B,H,d)\n            fuse = torch.cat([ctx, he, z_proj], dim=-1)\n        else:\n            fuse = torch.cat([ctx, he], dim=-1)\n        return self.head(fuse).squeeze(-1)                # (B,H)\n\n# ========== Metrics & training ==========\n@torch.no_grad()\ndef val_metrics_MW(model, loader, y_scaler):\n    model.eval()\n    preds_s, trues_s, n, mse_s = [], [], 0, 0.0\n    for batch in loader:\n        if len(batch)==2: xb, yb = batch; zb = None\n        else: xb, yb, zb = batch\n        xb = xb.to(next(model.parameters()).device)\n        yb = yb.to(next(model.parameters()).device)\n        zb = None if zb is None else zb.to(xb.device)\n        yhat_s = model(xb, zb)\n        mse_s += torch.mean((yhat_s - yb)**2).item() * xb.size(0); n += xb.size(0)\n        preds_s.append(yhat_s.detach().cpu().numpy()); trues_s.append(yb.detach().cpu().numpy())\n    preds_s, trues_s = np.concatenate(preds_s), np.concatenate(trues_s)\n    preds = bundle.y_scaler.inverse_transform(preds_s.reshape(-1,1)).reshape(preds_s.shape)\n    trues = bundle.y_scaler.inverse_transform(trues_s.reshape(-1,1)).reshape(trues_s.shape)\n    mae = np.mean(np.abs(preds - trues))\n    return mae, (mse_s / n)\n\ndef train_transformer(\n    X_train, Y_train_s, X_val, Y_val_s, Z_train=None, Z_val=None,\n    d_model=256, nhead=8, layers=3, d_ff=512, dropout=0.1,\n    batch=128, epochs=80, lr=2e-4, weight_decay=2e-4, horizon_weighted=True, seed: int = SEED\n):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    Din, H = X_train.shape[2], Y_train_s.shape[1]\n    Ff = 0 if Z_train is None else Z_train.shape[2]\n    tr_loader, va_loader = loaders(X_train, Y_train_s, X_val, Y_val_s, Z_train, Z_val,\n                                   batch=batch, workers=0, seed=seed)\n\n    model = TransformerForecaster(Din, horizon=H, d_model=d_model, nhead=nhead,\n                                  num_layers=layers, d_ff=d_ff, dropout=dropout,\n                                  future_feat_dim=Ff).to(device)\n    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\", factor=0.5, patience=2)\n    scaler = GradScaler(enabled=(USE_AMP and torch.cuda.is_available()))\n\n    loss_fn = nn.SmoothL1Loss(reduction=\"none\")\n    w = torch.linspace(1.20, 0.90, H, device=device).view(1, H) if horizon_weighted else None\n\n    best_mae, best = float(\"inf\"), None\n    patience, left = 8, 8\n    MIN_DELTA = 0.0\n\n    for epoch in range(1, epochs+1):\n        model.train()\n        run, nitems = 0.0, 0\n        for batch in tr_loader:\n            if len(batch)==2: xb, yb = batch; zb = None\n            else: xb, yb, zb = batch\n            xb = xb.to(device); yb = yb.to(device); zb = None if zb is None else zb.to(device)\n            opt.zero_grad(set_to_none=True)\n            with amp_ctx():\n                yhat_s = model(xb, zb)\n                L = loss_fn(yhat_s, yb)\n                if w is not None: L = L * w\n                L = L.mean()\n            scaler.scale(L).backward()\n            scaler.unscale_(opt); nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            scaler.step(opt); scaler.update()\n            run += L.item() * xb.size(0); nitems += xb.size(0)\n\n        val_mae_mw, val_mse_s = val_metrics_MW(model, va_loader, bundle.y_scaler)\n        sched.step(val_mae_mw)\n        print(f\"Epoch {epoch:02d} | train loss {run/nitems:.4f} | val MAE(MW) {val_mae_mw:.2f} | val MSE(scaled) {val_mse_s:.4f}\")\n\n        if best_mae - val_mae_mw > MIN_DELTA:\n            best_mae, best, left = val_mae_mw, {k: v.detach().cpu().clone() for k,v in model.state_dict().items()}, patience\n        else:\n            left -= 1\n            if left == 0:\n                print(\"Early stopping.\"); break\n\n    if best is not None: model.load_state_dict(best)\n    return model\n\n@torch.no_grad()\ndef test_report_tf(model, X_test, Y_test_s, y_scaler, Z_test=None, batch=256):\n    dl = DataLoader(SeqDataset(X_test, Y_test_s, Z_test), batch_size=batch, shuffle=False)\n    device = next(model.parameters()).device\n    preds_s, trues_s = [], []\n    for batch in dl:\n        if len(batch)==2: xb, yb = batch; zb = None\n        else: xb, yb, zb = batch\n        xb = xb.to(device); zb = None if zb is None else zb.to(device)\n        yhat_s = model(xb, zb)\n        preds_s.append(yhat_s.cpu().numpy()); trues_s.append(yb.numpy())\n    preds_s, trues_s = np.concatenate(preds_s), np.concatenate(trues_s)\n    preds = y_scaler.inverse_transform(preds_s.reshape(-1,1)).reshape(preds_s.shape)\n    trues = y_scaler.inverse_transform(trues_s.reshape(-1,1)).reshape(trues_s.shape)\n    mae = np.mean(np.abs(preds - trues))\n    rmse = np.sqrt(np.mean((preds - trues)**2))\n    mae_h = np.mean(np.abs(preds - trues), axis=0)\n    print(f\"\\nTest  MAE (MW):  {mae:.2f}\")\n    print(f\"Test RMSE (MW): {rmse:.2f}\")\n    print(\"Horizon-wise MAE (MW):\", np.round(mae_h, 2))\n    return {\"MAE\": mae, \"RMSE\": rmse, \"MAE_by_h\": mae_h}\n\n# ---------- Run ----------\nif __name__ == \"__main__\":\n    model = train_transformer(\n        X_train, Y_train_s, X_val, Y_val_s,\n        Z_train=None, Z_val=None,\n        d_model=256, nhead=8, layers=3, d_ff=512, dropout=0.1,\n        batch=128, epochs=60, lr=2e-4, weight_decay=2e-4, horizon_weighted=True, seed=SEED\n    )\n    _ = test_report_tf(model, X_test, Y_test_s, bundle.y_scaler, Z_test=None)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T13:11:23.086244Z","iopub.execute_input":"2025-09-22T13:11:23.086533Z","iopub.status.idle":"2025-09-22T13:14:09.810053Z","shell.execute_reply.started":"2025-09-22T13:11:23.086514Z","shell.execute_reply":"2025-09-22T13:14:09.809340Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 01 | train loss 0.2592 | val MAE(MW) 362.17 | val MSE(scaled) 0.1532\nEpoch 02 | train loss 0.0881 | val MAE(MW) 258.74 | val MSE(scaled) 0.0898\nEpoch 03 | train loss 0.0683 | val MAE(MW) 218.81 | val MSE(scaled) 0.0710\nEpoch 04 | train loss 0.0625 | val MAE(MW) 207.39 | val MSE(scaled) 0.0610\nEpoch 05 | train loss 0.0599 | val MAE(MW) 204.47 | val MSE(scaled) 0.0608\nEpoch 06 | train loss 0.0580 | val MAE(MW) 197.91 | val MSE(scaled) 0.0607\nEpoch 07 | train loss 0.0571 | val MAE(MW) 187.93 | val MSE(scaled) 0.0527\nEpoch 08 | train loss 0.0560 | val MAE(MW) 187.41 | val MSE(scaled) 0.0498\nEpoch 09 | train loss 0.0553 | val MAE(MW) 179.84 | val MSE(scaled) 0.0470\nEpoch 10 | train loss 0.0545 | val MAE(MW) 196.45 | val MSE(scaled) 0.0542\nEpoch 11 | train loss 0.0541 | val MAE(MW) 179.93 | val MSE(scaled) 0.0474\nEpoch 12 | train loss 0.0537 | val MAE(MW) 193.80 | val MSE(scaled) 0.0546\nEpoch 13 | train loss 0.0520 | val MAE(MW) 191.28 | val MSE(scaled) 0.0524\nEpoch 14 | train loss 0.0517 | val MAE(MW) 182.92 | val MSE(scaled) 0.0479\nEpoch 15 | train loss 0.0516 | val MAE(MW) 189.97 | val MSE(scaled) 0.0518\nEpoch 16 | train loss 0.0507 | val MAE(MW) 178.39 | val MSE(scaled) 0.0470\nEpoch 17 | train loss 0.0506 | val MAE(MW) 186.77 | val MSE(scaled) 0.0510\nEpoch 18 | train loss 0.0504 | val MAE(MW) 183.02 | val MSE(scaled) 0.0490\nEpoch 19 | train loss 0.0503 | val MAE(MW) 185.70 | val MSE(scaled) 0.0508\nEpoch 20 | train loss 0.0498 | val MAE(MW) 184.12 | val MSE(scaled) 0.0507\nEpoch 21 | train loss 0.0498 | val MAE(MW) 184.48 | val MSE(scaled) 0.0510\nEpoch 22 | train loss 0.0495 | val MAE(MW) 185.74 | val MSE(scaled) 0.0508\nEpoch 23 | train loss 0.0493 | val MAE(MW) 180.06 | val MSE(scaled) 0.0481\nEpoch 24 | train loss 0.0493 | val MAE(MW) 179.45 | val MSE(scaled) 0.0478\nEarly stopping.\n\nTest  MAE (MW):  289.54\nTest RMSE (MW): 394.28\nHorizon-wise MAE (MW): [342.68 308.77 272.03 253.9  249.73 251.09 247.85 224.06 235.41 299.7\n 362.48 365.5  325.97 302.16 295.69 283.93 268.6  272.03 264.59 271.1\n 278.14 295.28 331.29 347.09]\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# train_transformer_baseline_fixed.py  (minimal deterministic-friendly patch)\n\nimport os, random, math, numpy as np\n\n# ---- Toggle: set True for strict determinism (requires kernel restart) ----\nSTRICT_DETERMINISM = False\n\n# If strict, cuBLAS needs this BEFORE importing torch (and a fresh kernel)\nif STRICT_DETERMINISM:\n    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"  # or \":16:8\"\n\nimport torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# --- seeding (simple & safe) ---\nSEED = 42\ndef seed_everything(seed=42):\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    random.seed(seed); np.random.seed(seed)\n    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n    # keep deterministic cuDNN kernels, but don't force torch.use_deterministic_algorithms\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n    # TF32 off helps reproducibility\n    torch.backends.cuda.matmul.allow_tf32 = False\n    torch.backends.cudnn.allow_tf32 = False\n\nseed_everything(SEED)\n\n# --- AMP (new API with fallback); set USE_AMP=False for tighter reproducibility ---\nUSE_AMP = True\ntry:\n    from torch.amp import autocast, GradScaler\n    def amp_ctx():\n        if not USE_AMP:\n            class _NoOp:\n                def __enter__(self): return None\n                def __exit__(self, *a): return False\n            return _NoOp()\n        use_bf16 = (torch.cuda.is_available()\n                    and torch.cuda.get_device_capability()[0] >= 8)\n        return autocast(device_type=\"cuda\", dtype=torch.bfloat16 if use_bf16 else torch.float16)\nexcept Exception:\n    from torch.cuda.amp import autocast, GradScaler\n    def amp_ctx():\n        return autocast(enabled=(USE_AMP and torch.cuda.is_available()))\n\n# ========== Data ==========\nclass SeqDataset(Dataset):\n    \"\"\"Returns (X, Y) if Z is None; otherwise (X, Y, Z). Avoids collating None.\"\"\"\n    def __init__(self, X, Y, Z_future=None):\n        self.X = torch.from_numpy(X)              # (N, L, D)\n        self.Y = torch.from_numpy(Y)              # (N, H)\n        self.Z = None if Z_future is None else torch.from_numpy(Z_future.astype(np.float32))\n        self.has_future = self.Z is not None\n    def __len__(self): return self.X.shape[0]\n    def __getitem__(self, i):\n        if self.has_future:\n            return self.X[i], self.Y[i], self.Z[i]\n        else:\n            return self.X[i], self.Y[i]\n\ndef _seed_worker(worker_id: int):\n    wseed = torch.initial_seed() % 2**32\n    np.random.seed(wseed); random.seed(wseed)\n\ndef loaders(Xtr,Ytr,Xva,Yva, Ztr=None, Zva=None, batch=128, workers=0, seed: int = SEED):\n    g = torch.Generator().manual_seed(seed)  # deterministic shuffle\n    return (\n        DataLoader(SeqDataset(Xtr,Ytr,Ztr), batch_size=batch, shuffle=True,  drop_last=True,\n                   num_workers=workers, pin_memory=True, worker_init_fn=_seed_worker, generator=g),\n        DataLoader(SeqDataset(Xva,Yva,Zva), batch_size=batch, shuffle=False, drop_last=False,\n                   num_workers=workers, pin_memory=True, worker_init_fn=_seed_worker, generator=g),\n    )\n\n# ========== Model ==========\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=1000):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div); pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))  # (1, max_len, d_model)\n    def forward(self, x): return x + self.pe[:, :x.size(1)]\n\nclass TransformerForecaster(nn.Module):\n    def __init__(self, input_dim, horizon=24, d_model=256, nhead=8, num_layers=3,\n                 d_ff=512, dropout=0.1, future_feat_dim=0):\n        super().__init__()\n        self.horizon = horizon\n        self.input_proj = nn.Linear(input_dim, d_model)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=d_model, nhead=nhead, dim_feedforward=d_ff,\n            dropout=dropout, batch_first=True, norm_first=True\n        )\n        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n        self.pos = PositionalEncoding(d_model)\n        self.cls = nn.Parameter(torch.randn(1, 1, d_model) * 0.02)\n        self.horz_emb = nn.Parameter(torch.randn(horizon, d_model) * 0.02)\n        self.future_proj = nn.Linear(future_feat_dim, d_model) if future_feat_dim and future_feat_dim>0 else None\n        head_in = d_model + d_model + (d_model if self.future_proj else 0)\n        self.head = nn.Sequential(nn.LayerNorm(head_in), nn.Linear(head_in, 256),\n                                  nn.ReLU(), nn.Dropout(dropout), nn.Linear(256, 1))\n\n    def forward(self, x, z_future=None):\n        B, L, _ = x.shape\n        x = self.pos(self.input_proj(x))             # (B,L,d)\n        cls = self.cls.expand(B, 1, -1)\n        enc = self.encoder(torch.cat([cls, x], dim=1))\n        context = enc[:, 0, :]                       # (B,d)\n        H = self.horizon\n        he = self.horz_emb.unsqueeze(0).expand(B, H, -1)  # (B,H,d)\n        ctx = context.unsqueeze(1).expand(B, H, -1)       # (B,H,d)\n        if self.future_proj is not None and z_future is not None:\n            z_proj = self.future_proj(z_future)           # (B,H,d)\n            fuse = torch.cat([ctx, he, z_proj], dim=-1)\n        else:\n            fuse = torch.cat([ctx, he], dim=-1)\n        return self.head(fuse).squeeze(-1)                # (B,H)\n\n# ========== Metrics & training ==========\n@torch.no_grad()\ndef val_metrics_MW(model, loader, y_scaler):\n    model.eval()\n    preds_s, trues_s, n, mse_s = [], [], 0, 0.0\n    for batch in loader:\n        if len(batch)==2: xb, yb = batch; zb = None\n        else: xb, yb, zb = batch\n        xb = xb.to(next(model.parameters()).device)\n        yb = yb.to(next(model.parameters()).device)\n        zb = None if zb is None else zb.to(xb.device)\n        yhat_s = model(xb, zb)\n        mse_s += torch.mean((yhat_s - yb)**2).item() * xb.size(0); n += xb.size(0)\n        preds_s.append(yhat_s.detach().cpu().numpy()); trues_s.append(yb.detach().cpu().numpy())\n    preds_s, trues_s = np.concatenate(preds_s), np.concatenate(trues_s)\n    preds = bundle.y_scaler.inverse_transform(preds_s.reshape(-1,1)).reshape(preds_s.shape)\n    trues = bundle.y_scaler.inverse_transform(trues_s.reshape(-1,1)).reshape(trues_s.shape)\n    mae = np.mean(np.abs(preds - trues))\n    return mae, (mse_s / n)\n\ndef train_transformer(\n    X_train, Y_train_s, X_val, Y_val_s, Z_train=None, Z_val=None,\n    d_model=256, nhead=8, layers=3, d_ff=512, dropout=0.1,\n    batch=128, epochs=80, lr=2e-4, weight_decay=2e-4, horizon_weighted=True, seed: int = SEED\n):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    Din, H = X_train.shape[2], Y_train_s.shape[1]\n    Ff = 0 if Z_train is None else Z_train.shape[2]\n    tr_loader, va_loader = loaders(X_train, Y_train_s, X_val, Y_val_s, Z_train, Z_val,\n                                   batch=batch, workers=0, seed=seed)\n\n    model = TransformerForecaster(Din, horizon=H, d_model=d_model, nhead=nhead,\n                                  num_layers=layers, d_ff=d_ff, dropout=dropout,\n                                  future_feat_dim=Ff).to(device)\n    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\", factor=0.5, patience=2)\n    scaler = GradScaler(enabled=(USE_AMP and torch.cuda.is_available()))\n\n    loss_fn = nn.SmoothL1Loss(reduction=\"none\")\n    w = torch.linspace(1.20, 0.90, H, device=device).view(1, H) if horizon_weighted else None\n\n    best_mae, best = float(\"inf\"), None\n    patience, left = 8, 8\n    MIN_DELTA = 0.0\n\n    for epoch in range(1, epochs+1):\n        model.train()\n        run, nitems = 0.0, 0\n        for batch in tr_loader:\n            if len(batch)==2: xb, yb = batch; zb = None\n            else: xb, yb, zb = batch\n            xb = xb.to(device); yb = yb.to(device); zb = None if zb is None else zb.to(device)\n            opt.zero_grad(set_to_none=True)\n            with amp_ctx():\n                yhat_s = model(xb, zb)\n                L = loss_fn(yhat_s, yb)\n                if w is not None: L = L * w\n                L = L.mean()\n            scaler.scale(L).backward()\n            scaler.unscale_(opt); nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            scaler.step(opt); scaler.update()\n            run += L.item() * xb.size(0); nitems += xb.size(0)\n\n        val_mae_mw, val_mse_s = val_metrics_MW(model, va_loader, bundle.y_scaler)\n        sched.step(val_mae_mw)\n        print(f\"Epoch {epoch:02d} | train loss {run/nitems:.4f} | val MAE(MW) {val_mae_mw:.2f} | val MSE(scaled) {val_mse_s:.4f}\")\n\n        if best_mae - val_mae_mw > MIN_DELTA:\n            best_mae, best, left = val_mae_mw, {k: v.detach().cpu().clone() for k,v in model.state_dict().items()}, patience\n        else:\n            left -= 1\n            if left == 0:\n                print(\"Early stopping.\"); break\n\n    if best is not None: model.load_state_dict(best)\n    return model\n\n@torch.no_grad()\ndef test_report_tf(model, X_test, Y_test_s, y_scaler, Z_test=None, batch=256):\n    dl = DataLoader(SeqDataset(X_test, Y_test_s, Z_test), batch_size=batch, shuffle=False)\n    device = next(model.parameters()).device\n    preds_s, trues_s = [], []\n    for batch in dl:\n        if len(batch)==2: xb, yb = batch; zb = None\n        else: xb, yb, zb = batch\n        xb = xb.to(device); zb = None if zb is None else zb.to(device)\n        yhat_s = model(xb, zb)\n        preds_s.append(yhat_s.cpu().numpy()); trues_s.append(yb.numpy())\n    preds_s, trues_s = np.concatenate(preds_s), np.concatenate(trues_s)\n    preds = y_scaler.inverse_transform(preds_s.reshape(-1,1)).reshape(preds_s.shape)\n    trues = y_scaler.inverse_transform(trues_s.reshape(-1,1)).reshape(trues_s.shape)\n    mae = np.mean(np.abs(preds - trues))\n    rmse = np.sqrt(np.mean((preds - trues)**2))\n    mae_h = np.mean(np.abs(preds - trues), axis=0)\n    print(f\"\\nTest  MAE (MW):  {mae:.2f}\")\n    print(f\"Test RMSE (MW): {rmse:.2f}\")\n    print(\"Horizon-wise MAE (MW):\", np.round(mae_h, 2))\n    return {\"MAE\": mae, \"RMSE\": rmse, \"MAE_by_h\": mae_h}\n\n# ---------- Run ----------\nif __name__ == \"__main__\":\n    model = train_transformer(\n        X_train, Y_train_s, X_val, Y_val_s,\n        Z_train=Z_train, Z_val=Z_val,\n        d_model=320, nhead=8, layers=4, d_ff=1024, dropout=0.18,\n        batch=128, epochs=80, lr=2e-4, weight_decay=6e-4, horizon_weighted=True, seed=42\n    )\n    _ = test_report_tf(model, X_test, Y_test_s, bundle.y_scaler, Z_test=Z_test)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T13:21:39.025297Z","iopub.execute_input":"2025-09-22T13:21:39.025991Z","iopub.status.idle":"2025-09-22T13:26:14.971729Z","shell.execute_reply.started":"2025-09-22T13:21:39.025960Z","shell.execute_reply":"2025-09-22T13:26:14.971054Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 01 | train loss 0.0892 | val MAE(MW) 156.84 | val MSE(scaled) 0.0341\nEpoch 02 | train loss 0.0548 | val MAE(MW) 158.52 | val MSE(scaled) 0.0347\nEpoch 03 | train loss 0.0528 | val MAE(MW) 146.32 | val MSE(scaled) 0.0299\nEpoch 04 | train loss 0.0516 | val MAE(MW) 149.31 | val MSE(scaled) 0.0316\nEpoch 05 | train loss 0.0507 | val MAE(MW) 149.00 | val MSE(scaled) 0.0324\nEpoch 06 | train loss 0.0500 | val MAE(MW) 138.64 | val MSE(scaled) 0.0284\nEpoch 07 | train loss 0.0498 | val MAE(MW) 137.99 | val MSE(scaled) 0.0287\nEpoch 08 | train loss 0.0491 | val MAE(MW) 144.71 | val MSE(scaled) 0.0302\nEpoch 09 | train loss 0.0487 | val MAE(MW) 138.13 | val MSE(scaled) 0.0289\nEpoch 10 | train loss 0.0479 | val MAE(MW) 136.31 | val MSE(scaled) 0.0276\nEpoch 11 | train loss 0.0480 | val MAE(MW) 138.54 | val MSE(scaled) 0.0290\nEpoch 12 | train loss 0.0472 | val MAE(MW) 134.81 | val MSE(scaled) 0.0271\nEpoch 13 | train loss 0.0470 | val MAE(MW) 134.70 | val MSE(scaled) 0.0269\nEpoch 14 | train loss 0.0465 | val MAE(MW) 130.70 | val MSE(scaled) 0.0260\nEpoch 15 | train loss 0.0461 | val MAE(MW) 140.09 | val MSE(scaled) 0.0287\nEpoch 16 | train loss 0.0437 | val MAE(MW) 130.69 | val MSE(scaled) 0.0261\nEpoch 17 | train loss 0.0455 | val MAE(MW) 133.52 | val MSE(scaled) 0.0270\nEpoch 18 | train loss 0.0405 | val MAE(MW) 133.29 | val MSE(scaled) 0.0270\nEpoch 19 | train loss 0.0395 | val MAE(MW) 135.48 | val MSE(scaled) 0.0277\nEpoch 20 | train loss 0.0389 | val MAE(MW) 133.88 | val MSE(scaled) 0.0275\nEpoch 21 | train loss 0.0355 | val MAE(MW) 133.18 | val MSE(scaled) 0.0271\nEpoch 22 | train loss 0.0330 | val MAE(MW) 134.68 | val MSE(scaled) 0.0276\nEpoch 23 | train loss 0.0320 | val MAE(MW) 134.36 | val MSE(scaled) 0.0277\nEpoch 24 | train loss 0.0298 | val MAE(MW) 133.11 | val MSE(scaled) 0.0273\nEarly stopping.\n\nTest  MAE (MW):  202.74\nTest RMSE (MW): 278.61\nHorizon-wise MAE (MW): [202.37 202.53 201.43 202.08 201.94 201.83 201.87 201.48 200.72 200.71\n 199.93 199.91 200.36 201.18 201.67 203.14 203.49 203.86 204.4  205.79\n 206.24 206.64 206.09 206.13]\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"torch.save(model.state_dict(), \"transformer_forecaster_baseline.pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T17:37:09.418710Z","iopub.execute_input":"2025-09-20T17:37:09.419655Z","iopub.status.idle":"2025-09-20T17:37:09.457843Z","shell.execute_reply.started":"2025-09-20T17:37:09.419628Z","shell.execute_reply":"2025-09-20T17:37:09.457246Z"}},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"# Transformer (bigger)","metadata":{"id":"SWPr36k2XxBW"}},{"cell_type":"code","source":"torch.save(model.state_dict(), \"transformer_horizon_queries.pt\")","metadata":{"id":"BDYQ-JwFX3S4","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train_transformer_baseline_fixed.py  (minimal deterministic-friendly patch)\n\nimport os, random, math, numpy as np\n\n# ---- Toggle: set True for strict determinism (requires kernel restart) ----\nSTRICT_DETERMINISM = False\n\n# If strict, cuBLAS needs this BEFORE importing torch (and a fresh kernel)\nif STRICT_DETERMINISM:\n    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"  # or \":16:8\"\n\nimport torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# --- seeding (simple & safe) ---\nSEED = 42\ndef seed_everything(seed=42):\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    random.seed(seed); np.random.seed(seed)\n    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n    # keep deterministic cuDNN kernels, but don't force torch.use_deterministic_algorithms\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n    # TF32 off helps reproducibility\n    torch.backends.cuda.matmul.allow_tf32 = False\n    torch.backends.cudnn.allow_tf32 = False\n\nseed_everything(SEED)\n\n# --- AMP (new API with fallback); set USE_AMP=False for tighter reproducibility ---\nUSE_AMP = True\ntry:\n    from torch.amp import autocast, GradScaler\n    def amp_ctx():\n        if not USE_AMP:\n            class _NoOp:\n                def __enter__(self): return None\n                def __exit__(self, *a): return False\n            return _NoOp()\n        use_bf16 = (torch.cuda.is_available()\n                    and torch.cuda.get_device_capability()[0] >= 8)\n        return autocast(device_type=\"cuda\", dtype=torch.bfloat16 if use_bf16 else torch.float16)\nexcept Exception:\n    from torch.cuda.amp import autocast, GradScaler\n    def amp_ctx():\n        return autocast(enabled=(USE_AMP and torch.cuda.is_available()))\n\n# ========== Data ==========\nclass SeqDataset(Dataset):\n    \"\"\"Returns (X, Y) if Z is None; otherwise (X, Y, Z). Avoids collating None.\"\"\"\n    def __init__(self, X, Y, Z_future=None):\n        self.X = torch.from_numpy(X)              # (N, L, D)\n        self.Y = torch.from_numpy(Y)              # (N, H)\n        self.Z = None if Z_future is None else torch.from_numpy(Z_future.astype(np.float32))\n        self.has_future = self.Z is not None\n    def __len__(self): return self.X.shape[0]\n    def __getitem__(self, i):\n        if self.has_future:\n            return self.X[i], self.Y[i], self.Z[i]\n        else:\n            return self.X[i], self.Y[i]\n\ndef _seed_worker(worker_id: int):\n    wseed = torch.initial_seed() % 2**32\n    np.random.seed(wseed); random.seed(wseed)\n\ndef loaders(Xtr,Ytr,Xva,Yva, Ztr=None, Zva=None, batch=128, workers=0, seed: int = SEED):\n    g = torch.Generator().manual_seed(seed)  # deterministic shuffle\n    return (\n        DataLoader(SeqDataset(Xtr,Ytr,Ztr), batch_size=batch, shuffle=True,  drop_last=True,\n                   num_workers=workers, pin_memory=True, worker_init_fn=_seed_worker, generator=g),\n        DataLoader(SeqDataset(Xva,Yva,Zva), batch_size=batch, shuffle=False, drop_last=False,\n                   num_workers=workers, pin_memory=True, worker_init_fn=_seed_worker, generator=g),\n    )\n\n# ========== Model ==========\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=1000):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div); pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))  # (1, max_len, d_model)\n    def forward(self, x): return x + self.pe[:, :x.size(1)]\n\nclass TransformerForecaster(nn.Module):\n    def __init__(self, input_dim, horizon=24, d_model=256, nhead=8, num_layers=3,\n                 d_ff=512, dropout=0.1, future_feat_dim=0):\n        super().__init__()\n        self.horizon = horizon\n        self.input_proj = nn.Linear(input_dim, d_model)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=d_model, nhead=nhead, dim_feedforward=d_ff,\n            dropout=dropout, batch_first=True, norm_first=True\n        )\n        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n        self.pos = PositionalEncoding(d_model)\n        self.cls = nn.Parameter(torch.randn(1, 1, d_model) * 0.02)\n        self.horz_emb = nn.Parameter(torch.randn(horizon, d_model) * 0.02)\n        self.future_proj = nn.Linear(future_feat_dim, d_model) if future_feat_dim and future_feat_dim>0 else None\n        head_in = d_model + d_model + (d_model if self.future_proj else 0)\n        self.head = nn.Sequential(nn.LayerNorm(head_in), nn.Linear(head_in, 256),\n                                  nn.ReLU(), nn.Dropout(dropout), nn.Linear(256, 1))\n\n    def forward(self, x, z_future=None):\n        B, L, _ = x.shape\n        x = self.pos(self.input_proj(x))             # (B,L,d)\n        cls = self.cls.expand(B, 1, -1)\n        enc = self.encoder(torch.cat([cls, x], dim=1))\n        context = enc[:, 0, :]                       # (B,d)\n        H = self.horizon\n        he = self.horz_emb.unsqueeze(0).expand(B, H, -1)  # (B,H,d)\n        ctx = context.unsqueeze(1).expand(B, H, -1)       # (B,H,d)\n        if self.future_proj is not None and z_future is not None:\n            z_proj = self.future_proj(z_future)           # (B,H,d)\n            fuse = torch.cat([ctx, he, z_proj], dim=-1)\n        else:\n            fuse = torch.cat([ctx, he], dim=-1)\n        return self.head(fuse).squeeze(-1)                # (B,H)\n\n# ========== Metrics & training ==========\n@torch.no_grad()\ndef val_metrics_MW(model, loader, y_scaler):\n    model.eval()\n    preds_s, trues_s, n, mse_s = [], [], 0, 0.0\n    for batch in loader:\n        if len(batch)==2: xb, yb = batch; zb = None\n        else: xb, yb, zb = batch\n        xb = xb.to(next(model.parameters()).device)\n        yb = yb.to(next(model.parameters()).device)\n        zb = None if zb is None else zb.to(xb.device)\n        yhat_s = model(xb, zb)\n        mse_s += torch.mean((yhat_s - yb)**2).item() * xb.size(0); n += xb.size(0)\n        preds_s.append(yhat_s.detach().cpu().numpy()); trues_s.append(yb.detach().cpu().numpy())\n    preds_s, trues_s = np.concatenate(preds_s), np.concatenate(trues_s)\n    preds = bundle.y_scaler.inverse_transform(preds_s.reshape(-1,1)).reshape(preds_s.shape)\n    trues = bundle.y_scaler.inverse_transform(trues_s.reshape(-1,1)).reshape(trues_s.shape)\n    mae = np.mean(np.abs(preds - trues))\n    return mae, (mse_s / n)\n\ndef train_transformer(\n    X_train, Y_train_s, X_val, Y_val_s, Z_train=None, Z_val=None,\n    d_model=256, nhead=8, layers=3, d_ff=512, dropout=0.1,\n    batch=128, epochs=80, lr=2e-4, weight_decay=2e-4, horizon_weighted=True, seed: int = SEED\n):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    Din, H = X_train.shape[2], Y_train_s.shape[1]\n    Ff = 0 if Z_train is None else Z_train.shape[2]\n    tr_loader, va_loader = loaders(X_train, Y_train_s, X_val, Y_val_s, Z_train, Z_val,\n                                   batch=batch, workers=0, seed=seed)\n\n    model = TransformerForecaster(Din, horizon=H, d_model=d_model, nhead=nhead,\n                                  num_layers=layers, d_ff=d_ff, dropout=dropout,\n                                  future_feat_dim=Ff).to(device)\n    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\", factor=0.5, patience=2)\n    scaler = GradScaler(enabled=(USE_AMP and torch.cuda.is_available()))\n\n    loss_fn = nn.SmoothL1Loss(reduction=\"none\")\n    w = torch.linspace(1.20, 0.90, H, device=device).view(1, H) if horizon_weighted else None\n\n    best_mae, best = float(\"inf\"), None\n    patience, left = 8, 8\n    MIN_DELTA = 0.0\n\n    for epoch in range(1, epochs+1):\n        model.train()\n        run, nitems = 0.0, 0\n        for batch in tr_loader:\n            if len(batch)==2: xb, yb = batch; zb = None\n            else: xb, yb, zb = batch\n            xb = xb.to(device); yb = yb.to(device); zb = None if zb is None else zb.to(device)\n            opt.zero_grad(set_to_none=True)\n            with amp_ctx():\n                yhat_s = model(xb, zb)\n                L = loss_fn(yhat_s, yb)\n                if w is not None: L = L * w\n                L = L.mean()\n            scaler.scale(L).backward()\n            scaler.unscale_(opt); nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            scaler.step(opt); scaler.update()\n            run += L.item() * xb.size(0); nitems += xb.size(0)\n\n        val_mae_mw, val_mse_s = val_metrics_MW(model, va_loader, bundle.y_scaler)\n        sched.step(val_mae_mw)\n        print(f\"Epoch {epoch:02d} | train loss {run/nitems:.4f} | val MAE(MW) {val_mae_mw:.2f} | val MSE(scaled) {val_mse_s:.4f}\")\n\n        if best_mae - val_mae_mw > MIN_DELTA:\n            best_mae, best, left = val_mae_mw, {k: v.detach().cpu().clone() for k,v in model.state_dict().items()}, patience\n        else:\n            left -= 1\n            if left == 0:\n                print(\"Early stopping.\"); break\n\n    if best is not None: model.load_state_dict(best)\n    return model\n\n@torch.no_grad()\ndef test_report_tf(model, X_test, Y_test_s, y_scaler, Z_test=None, batch=256):\n    dl = DataLoader(SeqDataset(X_test, Y_test_s, Z_test), batch_size=batch, shuffle=False)\n    device = next(model.parameters()).device\n    preds_s, trues_s = [], []\n    for batch in dl:\n        if len(batch)==2: xb, yb = batch; zb = None\n        else: xb, yb, zb = batch\n        xb = xb.to(device); zb = None if zb is None else zb.to(device)\n        yhat_s = model(xb, zb)\n        preds_s.append(yhat_s.cpu().numpy()); trues_s.append(yb.numpy())\n    preds_s, trues_s = np.concatenate(preds_s), np.concatenate(trues_s)\n    preds = y_scaler.inverse_transform(preds_s.reshape(-1,1)).reshape(preds_s.shape)\n    trues = y_scaler.inverse_transform(trues_s.reshape(-1,1)).reshape(trues_s.shape)\n    mae = np.mean(np.abs(preds - trues))\n    rmse = np.sqrt(np.mean((preds - trues)**2))\n    mae_h = np.mean(np.abs(preds - trues), axis=0)\n    print(f\"\\nTest  MAE (MW):  {mae:.2f}\")\n    print(f\"Test RMSE (MW): {rmse:.2f}\")\n    print(\"Horizon-wise MAE (MW):\", np.round(mae_h, 2))\n    return {\"MAE\": mae, \"RMSE\": rmse, \"MAE_by_h\": mae_h}\n\n# ---------- Run ----------\nif __name__ == \"__main__\":\n    # \"Large\" \n    model = train_transformer(\n        X_train, Y_train_s, X_val, Y_val_s,\n        Z_train=Z_train, Z_val=Z_val,\n        d_model=512, nhead=8, layers=6, d_ff=2048, dropout=0.20,\n        batch=128, epochs=120, lr=1.8e-4, weight_decay=8e-4,\n        horizon_weighted=True, seed=42\n    )\n    _ = test_report_tf(model, X_test, Y_test_s, bundle.y_scaler, Z_test=Z_test)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T15:20:29.479347Z","iopub.execute_input":"2025-09-22T15:20:29.480265Z","iopub.status.idle":"2025-09-22T15:27:38.836979Z","shell.execute_reply.started":"2025-09-22T15:20:29.480238Z","shell.execute_reply":"2025-09-22T15:27:38.836324Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 01 | train loss 0.1072 | val MAE(MW) 163.71 | val MSE(scaled) 0.0365\nEpoch 02 | train loss 0.0560 | val MAE(MW) 146.05 | val MSE(scaled) 0.0294\nEpoch 03 | train loss 0.0538 | val MAE(MW) 155.62 | val MSE(scaled) 0.0311\nEpoch 04 | train loss 0.0525 | val MAE(MW) 151.90 | val MSE(scaled) 0.0309\nEpoch 05 | train loss 0.0517 | val MAE(MW) 146.79 | val MSE(scaled) 0.0296\nEpoch 06 | train loss 0.0500 | val MAE(MW) 142.88 | val MSE(scaled) 0.0291\nEpoch 07 | train loss 0.0497 | val MAE(MW) 143.38 | val MSE(scaled) 0.0289\nEpoch 08 | train loss 0.0494 | val MAE(MW) 150.46 | val MSE(scaled) 0.0321\nEpoch 09 | train loss 0.0492 | val MAE(MW) 157.57 | val MSE(scaled) 0.0339\nEpoch 10 | train loss 0.0478 | val MAE(MW) 155.62 | val MSE(scaled) 0.0338\nEpoch 11 | train loss 0.0472 | val MAE(MW) 143.70 | val MSE(scaled) 0.0293\nEpoch 12 | train loss 0.0466 | val MAE(MW) 147.42 | val MSE(scaled) 0.0308\nEpoch 13 | train loss 0.0464 | val MAE(MW) 147.16 | val MSE(scaled) 0.0296\nEpoch 14 | train loss 0.0449 | val MAE(MW) 142.97 | val MSE(scaled) 0.0283\nEarly stopping.\n\nTest  MAE (MW):  206.50\nTest RMSE (MW): 285.37\nHorizon-wise MAE (MW): [208.46 209.02 207.34 206.93 207.02 207.78 207.15 204.39 204.3  203.26\n 203.78 203.29 203.16 204.3  203.77 205.   205.81 206.39 208.11 209.41\n 209.3  209.39 209.18 209.42]\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"print(df.)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Transformer (XL)","metadata":{}},{"cell_type":"code","source":"torch.save(model.state_dict(), \"transformer_horizon_queries_xl.pt\")","metadata":{"id":"JzRBBiR6cNuz","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train_transformer_baseline_fixed.py  (minimal deterministic-friendly patch)\n\nimport os, random, math, numpy as np\n\n# ---- Toggle: set True for strict determinism (requires kernel restart) ----\nSTRICT_DETERMINISM = False\n\n# If strict, cuBLAS needs this BEFORE importing torch (and a fresh kernel)\nif STRICT_DETERMINISM:\n    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"  # or \":16:8\"\n\nimport torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# --- seeding (simple & safe) ---\nSEED = 42\ndef seed_everything(seed=42):\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    random.seed(seed); np.random.seed(seed)\n    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n    # keep deterministic cuDNN kernels, but don't force torch.use_deterministic_algorithms\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n    # TF32 off helps reproducibility\n    torch.backends.cuda.matmul.allow_tf32 = False\n    torch.backends.cudnn.allow_tf32 = False\n\nseed_everything(SEED)\n\n# --- AMP (new API with fallback); set USE_AMP=False for tighter reproducibility ---\nUSE_AMP = True\ntry:\n    from torch.amp import autocast, GradScaler\n    def amp_ctx():\n        if not USE_AMP:\n            class _NoOp:\n                def __enter__(self): return None\n                def __exit__(self, *a): return False\n            return _NoOp()\n        use_bf16 = (torch.cuda.is_available()\n                    and torch.cuda.get_device_capability()[0] >= 8)\n        return autocast(device_type=\"cuda\", dtype=torch.bfloat16 if use_bf16 else torch.float16)\nexcept Exception:\n    from torch.cuda.amp import autocast, GradScaler\n    def amp_ctx():\n        return autocast(enabled=(USE_AMP and torch.cuda.is_available()))\n\n# ========== Data ==========\nclass SeqDataset(Dataset):\n    \"\"\"Returns (X, Y) if Z is None; otherwise (X, Y, Z). Avoids collating None.\"\"\"\n    def __init__(self, X, Y, Z_future=None):\n        self.X = torch.from_numpy(X)              # (N, L, D)\n        self.Y = torch.from_numpy(Y)              # (N, H)\n        self.Z = None if Z_future is None else torch.from_numpy(Z_future.astype(np.float32))\n        self.has_future = self.Z is not None\n    def __len__(self): return self.X.shape[0]\n    def __getitem__(self, i):\n        if self.has_future:\n            return self.X[i], self.Y[i], self.Z[i]\n        else:\n            return self.X[i], self.Y[i]\n\ndef _seed_worker(worker_id: int):\n    wseed = torch.initial_seed() % 2**32\n    np.random.seed(wseed); random.seed(wseed)\n\ndef loaders(Xtr,Ytr,Xva,Yva, Ztr=None, Zva=None, batch=128, workers=0, seed: int = SEED):\n    g = torch.Generator().manual_seed(seed)  # deterministic shuffle\n    return (\n        DataLoader(SeqDataset(Xtr,Ytr,Ztr), batch_size=batch, shuffle=True,  drop_last=True,\n                   num_workers=workers, pin_memory=True, worker_init_fn=_seed_worker, generator=g),\n        DataLoader(SeqDataset(Xva,Yva,Zva), batch_size=batch, shuffle=False, drop_last=False,\n                   num_workers=workers, pin_memory=True, worker_init_fn=_seed_worker, generator=g),\n    )\n\n# ========== Model ==========\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=1000):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div); pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))  # (1, max_len, d_model)\n    def forward(self, x): return x + self.pe[:, :x.size(1)]\n\nclass TransformerForecaster(nn.Module):\n    def __init__(self, input_dim, horizon=24, d_model=256, nhead=8, num_layers=3,\n                 d_ff=512, dropout=0.1, future_feat_dim=0):\n        super().__init__()\n        self.horizon = horizon\n        self.input_proj = nn.Linear(input_dim, d_model)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=d_model, nhead=nhead, dim_feedforward=d_ff,\n            dropout=dropout, batch_first=True, norm_first=True\n        )\n        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n        self.pos = PositionalEncoding(d_model)\n        self.cls = nn.Parameter(torch.randn(1, 1, d_model) * 0.02)\n        self.horz_emb = nn.Parameter(torch.randn(horizon, d_model) * 0.02)\n        self.future_proj = nn.Linear(future_feat_dim, d_model) if future_feat_dim and future_feat_dim>0 else None\n        head_in = d_model + d_model + (d_model if self.future_proj else 0)\n        self.head = nn.Sequential(nn.LayerNorm(head_in), nn.Linear(head_in, 256),\n                                  nn.ReLU(), nn.Dropout(dropout), nn.Linear(256, 1))\n\n    def forward(self, x, z_future=None):\n        B, L, _ = x.shape\n        x = self.pos(self.input_proj(x))             # (B,L,d)\n        cls = self.cls.expand(B, 1, -1)\n        enc = self.encoder(torch.cat([cls, x], dim=1))\n        context = enc[:, 0, :]                       # (B,d)\n        H = self.horizon\n        he = self.horz_emb.unsqueeze(0).expand(B, H, -1)  # (B,H,d)\n        ctx = context.unsqueeze(1).expand(B, H, -1)       # (B,H,d)\n        if self.future_proj is not None and z_future is not None:\n            z_proj = self.future_proj(z_future)           # (B,H,d)\n            fuse = torch.cat([ctx, he, z_proj], dim=-1)\n        else:\n            fuse = torch.cat([ctx, he], dim=-1)\n        return self.head(fuse).squeeze(-1)                # (B,H)\n\n# ========== Metrics & training ==========\n@torch.no_grad()\ndef val_metrics_MW(model, loader, y_scaler):\n    model.eval()\n    preds_s, trues_s, n, mse_s = [], [], 0, 0.0\n    for batch in loader:\n        if len(batch)==2: xb, yb = batch; zb = None\n        else: xb, yb, zb = batch\n        xb = xb.to(next(model.parameters()).device)\n        yb = yb.to(next(model.parameters()).device)\n        zb = None if zb is None else zb.to(xb.device)\n        yhat_s = model(xb, zb)\n        mse_s += torch.mean((yhat_s - yb)**2).item() * xb.size(0); n += xb.size(0)\n        preds_s.append(yhat_s.detach().cpu().numpy()); trues_s.append(yb.detach().cpu().numpy())\n    preds_s, trues_s = np.concatenate(preds_s), np.concatenate(trues_s)\n    preds = bundle.y_scaler.inverse_transform(preds_s.reshape(-1,1)).reshape(preds_s.shape)\n    trues = bundle.y_scaler.inverse_transform(trues_s.reshape(-1,1)).reshape(trues_s.shape)\n    mae = np.mean(np.abs(preds - trues))\n    return mae, (mse_s / n)\n\ndef train_transformer(\n    X_train, Y_train_s, X_val, Y_val_s, Z_train=None, Z_val=None,\n    d_model=256, nhead=8, layers=3, d_ff=512, dropout=0.1,\n    batch=128, epochs=80, lr=2e-4, weight_decay=2e-4, horizon_weighted=True, seed: int = SEED\n):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    Din, H = X_train.shape[2], Y_train_s.shape[1]\n    Ff = 0 if Z_train is None else Z_train.shape[2]\n    tr_loader, va_loader = loaders(X_train, Y_train_s, X_val, Y_val_s, Z_train, Z_val,\n                                   batch=batch, workers=0, seed=seed)\n\n    model = TransformerForecaster(Din, horizon=H, d_model=d_model, nhead=nhead,\n                                  num_layers=layers, d_ff=d_ff, dropout=dropout,\n                                  future_feat_dim=Ff).to(device)\n    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\", factor=0.5, patience=2)\n    scaler = GradScaler(enabled=(USE_AMP and torch.cuda.is_available()))\n\n    loss_fn = nn.SmoothL1Loss(reduction=\"none\")\n    w = torch.linspace(1.20, 0.90, H, device=device).view(1, H) if horizon_weighted else None\n\n    best_mae, best = float(\"inf\"), None\n    patience, left = 8, 8\n    MIN_DELTA = 0.0\n\n    for epoch in range(1, epochs+1):\n        model.train()\n        run, nitems = 0.0, 0\n        for batch in tr_loader:\n            if len(batch)==2: xb, yb = batch; zb = None\n            else: xb, yb, zb = batch\n            xb = xb.to(device); yb = yb.to(device); zb = None if zb is None else zb.to(device)\n            opt.zero_grad(set_to_none=True)\n            with amp_ctx():\n                yhat_s = model(xb, zb)\n                L = loss_fn(yhat_s, yb)\n                if w is not None: L = L * w\n                L = L.mean()\n            scaler.scale(L).backward()\n            scaler.unscale_(opt); nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            scaler.step(opt); scaler.update()\n            run += L.item() * xb.size(0); nitems += xb.size(0)\n\n        val_mae_mw, val_mse_s = val_metrics_MW(model, va_loader, bundle.y_scaler)\n        sched.step(val_mae_mw)\n        print(f\"Epoch {epoch:02d} | train loss {run/nitems:.4f} | val MAE(MW) {val_mae_mw:.2f} | val MSE(scaled) {val_mse_s:.4f}\")\n\n        if best_mae - val_mae_mw > MIN_DELTA:\n            best_mae, best, left = val_mae_mw, {k: v.detach().cpu().clone() for k,v in model.state_dict().items()}, patience\n        else:\n            left -= 1\n            if left == 0:\n                print(\"Early stopping.\"); break\n\n    if best is not None: model.load_state_dict(best)\n    return model\n\n@torch.no_grad()\ndef test_report_tf(model, X_test, Y_test_s, y_scaler, Z_test=None, batch=256):\n    dl = DataLoader(SeqDataset(X_test, Y_test_s, Z_test), batch_size=batch, shuffle=False)\n    device = next(model.parameters()).device\n    preds_s, trues_s = [], []\n    for batch in dl:\n        if len(batch)==2: xb, yb = batch; zb = None\n        else: xb, yb, zb = batch\n        xb = xb.to(device); zb = None if zb is None else zb.to(device)\n        yhat_s = model(xb, zb)\n        preds_s.append(yhat_s.cpu().numpy()); trues_s.append(yb.numpy())\n    preds_s, trues_s = np.concatenate(preds_s), np.concatenate(trues_s)\n    preds = y_scaler.inverse_transform(preds_s.reshape(-1,1)).reshape(preds_s.shape)\n    trues = y_scaler.inverse_transform(trues_s.reshape(-1,1)).reshape(trues_s.shape)\n    mae = np.mean(np.abs(preds - trues))\n    rmse = np.sqrt(np.mean((preds - trues)**2))\n    mae_h = np.mean(np.abs(preds - trues), axis=0)\n    print(f\"\\nTest  MAE (MW):  {mae:.2f}\")\n    print(f\"Test RMSE (MW): {rmse:.2f}\")\n    print(\"Horizon-wise MAE (MW):\", np.round(mae_h, 2))\n    return {\"MAE\": mae, \"RMSE\": rmse, \"MAE_by_h\": mae_h}\n\n# ---------- Run ----------\nif __name__ == \"__main__\":\n    # \"XL\"\n    model = train_transformer(\n        X_train, Y_train_s, X_val, Y_val_s,\n        Z_train=Z_train, Z_val=Z_val,\n        d_model=640, nhead=10, layers=8, d_ff=2560, dropout=0.22,\n        batch=96, epochs=140, lr=1.6e-4, weight_decay=1.0e-3,\n        horizon_weighted=True, seed=42\n    )\n    _ = test_report_tf(model, X_test, Y_test_s, bundle.y_scaler, Z_test=Z_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T15:32:06.437460Z","iopub.execute_input":"2025-09-22T15:32:06.437916Z","iopub.status.idle":"2025-09-22T15:45:52.897036Z","shell.execute_reply.started":"2025-09-22T15:32:06.437895Z","shell.execute_reply":"2025-09-22T15:45:52.896360Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 01 | train loss 0.1127 | val MAE(MW) 176.08 | val MSE(scaled) 0.0389\nEpoch 02 | train loss 0.0568 | val MAE(MW) 149.13 | val MSE(scaled) 0.0299\nEpoch 03 | train loss 0.0544 | val MAE(MW) 160.79 | val MSE(scaled) 0.0327\nEpoch 04 | train loss 0.0535 | val MAE(MW) 162.87 | val MSE(scaled) 0.0343\nEpoch 05 | train loss 0.0524 | val MAE(MW) 162.02 | val MSE(scaled) 0.0364\nEpoch 06 | train loss 0.0504 | val MAE(MW) 142.93 | val MSE(scaled) 0.0287\nEpoch 07 | train loss 0.0505 | val MAE(MW) 149.64 | val MSE(scaled) 0.0302\nEpoch 08 | train loss 0.0497 | val MAE(MW) 149.16 | val MSE(scaled) 0.0303\nEpoch 09 | train loss 0.0494 | val MAE(MW) 151.89 | val MSE(scaled) 0.0318\nEpoch 10 | train loss 0.0473 | val MAE(MW) 155.92 | val MSE(scaled) 0.0333\nEpoch 11 | train loss 0.0462 | val MAE(MW) 156.65 | val MSE(scaled) 0.0330\nEpoch 12 | train loss 0.0454 | val MAE(MW) 159.36 | val MSE(scaled) 0.0336\nEpoch 13 | train loss 0.0389 | val MAE(MW) 157.71 | val MSE(scaled) 0.0325\nEpoch 14 | train loss 0.0369 | val MAE(MW) 156.06 | val MSE(scaled) 0.0321\nEarly stopping.\n\nTest  MAE (MW):  208.74\nTest RMSE (MW): 284.19\nHorizon-wise MAE (MW): [209.19 210.38 209.27 209.23 207.96 208.51 209.16 207.95 207.88 208.3\n 207.39 207.54 207.29 206.81 207.5  208.44 208.88 209.12 208.98 209.88\n 209.12 209.79 210.15 211.11]\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"# Tranformer (tiny)","metadata":{}},{"cell_type":"code","source":"# train_transformer_baseline_fixed.py  (minimal deterministic-friendly patch)\n\nimport os, random, math, numpy as np\n\n# ---- Toggle: set True for strict determinism (requires kernel restart) ----\nSTRICT_DETERMINISM = False\n\n# If strict, cuBLAS needs this BEFORE importing torch (and a fresh kernel)\nif STRICT_DETERMINISM:\n    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"  # or \":16:8\"\n\nimport torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# --- seeding (simple & safe) ---\nSEED = 42\ndef seed_everything(seed=42):\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    random.seed(seed); np.random.seed(seed)\n    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n    # keep deterministic cuDNN kernels, but don't force torch.use_deterministic_algorithms\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n    # TF32 off helps reproducibility\n    torch.backends.cuda.matmul.allow_tf32 = False\n    torch.backends.cudnn.allow_tf32 = False\n\nseed_everything(SEED)\n\n# --- AMP (new API with fallback); set USE_AMP=False for tighter reproducibility ---\nUSE_AMP = True\ntry:\n    from torch.amp import autocast, GradScaler\n    def amp_ctx():\n        if not USE_AMP:\n            class _NoOp:\n                def __enter__(self): return None\n                def __exit__(self, *a): return False\n            return _NoOp()\n        use_bf16 = (torch.cuda.is_available()\n                    and torch.cuda.get_device_capability()[0] >= 8)\n        return autocast(device_type=\"cuda\", dtype=torch.bfloat16 if use_bf16 else torch.float16)\nexcept Exception:\n    from torch.cuda.amp import autocast, GradScaler\n    def amp_ctx():\n        return autocast(enabled=(USE_AMP and torch.cuda.is_available()))\n\n# ========== Data ==========\nclass SeqDataset(Dataset):\n    \"\"\"Returns (X, Y) if Z is None; otherwise (X, Y, Z). Avoids collating None.\"\"\"\n    def __init__(self, X, Y, Z_future=None):\n        self.X = torch.from_numpy(X)              # (N, L, D)\n        self.Y = torch.from_numpy(Y)              # (N, H)\n        self.Z = None if Z_future is None else torch.from_numpy(Z_future.astype(np.float32))\n        self.has_future = self.Z is not None\n    def __len__(self): return self.X.shape[0]\n    def __getitem__(self, i):\n        if self.has_future:\n            return self.X[i], self.Y[i], self.Z[i]\n        else:\n            return self.X[i], self.Y[i]\n\ndef _seed_worker(worker_id: int):\n    wseed = torch.initial_seed() % 2**32\n    np.random.seed(wseed); random.seed(wseed)\n\ndef loaders(Xtr,Ytr,Xva,Yva, Ztr=None, Zva=None, batch=128, workers=0, seed: int = SEED):\n    g = torch.Generator().manual_seed(seed)  # deterministic shuffle\n    return (\n        DataLoader(SeqDataset(Xtr,Ytr,Ztr), batch_size=batch, shuffle=True,  drop_last=True,\n                   num_workers=workers, pin_memory=True, worker_init_fn=_seed_worker, generator=g),\n        DataLoader(SeqDataset(Xva,Yva,Zva), batch_size=batch, shuffle=False, drop_last=False,\n                   num_workers=workers, pin_memory=True, worker_init_fn=_seed_worker, generator=g),\n    )\n\n# ========== Model ==========\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=1000):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div); pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))  # (1, max_len, d_model)\n    def forward(self, x): return x + self.pe[:, :x.size(1)]\n\nclass TransformerForecaster(nn.Module):\n    def __init__(self, input_dim, horizon=24, d_model=256, nhead=8, num_layers=3,\n                 d_ff=512, dropout=0.1, future_feat_dim=0):\n        super().__init__()\n        self.horizon = horizon\n        self.input_proj = nn.Linear(input_dim, d_model)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=d_model, nhead=nhead, dim_feedforward=d_ff,\n            dropout=dropout, batch_first=True, norm_first=True\n        )\n        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n        self.pos = PositionalEncoding(d_model)\n        self.cls = nn.Parameter(torch.randn(1, 1, d_model) * 0.02)\n        self.horz_emb = nn.Parameter(torch.randn(horizon, d_model) * 0.02)\n        self.future_proj = nn.Linear(future_feat_dim, d_model) if future_feat_dim and future_feat_dim>0 else None\n        head_in = d_model + d_model + (d_model if self.future_proj else 0)\n        self.head = nn.Sequential(nn.LayerNorm(head_in), nn.Linear(head_in, 256),\n                                  nn.ReLU(), nn.Dropout(dropout), nn.Linear(256, 1))\n\n    def forward(self, x, z_future=None):\n        B, L, _ = x.shape\n        x = self.pos(self.input_proj(x))             # (B,L,d)\n        cls = self.cls.expand(B, 1, -1)\n        enc = self.encoder(torch.cat([cls, x], dim=1))\n        context = enc[:, 0, :]                       # (B,d)\n        H = self.horizon\n        he = self.horz_emb.unsqueeze(0).expand(B, H, -1)  # (B,H,d)\n        ctx = context.unsqueeze(1).expand(B, H, -1)       # (B,H,d)\n        if self.future_proj is not None and z_future is not None:\n            z_proj = self.future_proj(z_future)           # (B,H,d)\n            fuse = torch.cat([ctx, he, z_proj], dim=-1)\n        else:\n            fuse = torch.cat([ctx, he], dim=-1)\n        return self.head(fuse).squeeze(-1)                # (B,H)\n\n# ========== Metrics & training ==========\n@torch.no_grad()\ndef val_metrics_MW(model, loader, y_scaler):\n    model.eval()\n    preds_s, trues_s, n, mse_s = [], [], 0, 0.0\n    for batch in loader:\n        if len(batch)==2: xb, yb = batch; zb = None\n        else: xb, yb, zb = batch\n        xb = xb.to(next(model.parameters()).device)\n        yb = yb.to(next(model.parameters()).device)\n        zb = None if zb is None else zb.to(xb.device)\n        yhat_s = model(xb, zb)\n        mse_s += torch.mean((yhat_s - yb)**2).item() * xb.size(0); n += xb.size(0)\n        preds_s.append(yhat_s.detach().cpu().numpy()); trues_s.append(yb.detach().cpu().numpy())\n    preds_s, trues_s = np.concatenate(preds_s), np.concatenate(trues_s)\n    preds = bundle.y_scaler.inverse_transform(preds_s.reshape(-1,1)).reshape(preds_s.shape)\n    trues = bundle.y_scaler.inverse_transform(trues_s.reshape(-1,1)).reshape(trues_s.shape)\n    mae = np.mean(np.abs(preds - trues))\n    return mae, (mse_s / n)\n\ndef train_transformer(\n    X_train, Y_train_s, X_val, Y_val_s, Z_train=None, Z_val=None,\n    d_model=256, nhead=8, layers=3, d_ff=512, dropout=0.1,\n    batch=128, epochs=80, lr=2e-4, weight_decay=2e-4, horizon_weighted=True, seed: int = SEED\n):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    Din, H = X_train.shape[2], Y_train_s.shape[1]\n    Ff = 0 if Z_train is None else Z_train.shape[2]\n    tr_loader, va_loader = loaders(X_train, Y_train_s, X_val, Y_val_s, Z_train, Z_val,\n                                   batch=batch, workers=0, seed=seed)\n\n    model = TransformerForecaster(Din, horizon=H, d_model=d_model, nhead=nhead,\n                                  num_layers=layers, d_ff=d_ff, dropout=dropout,\n                                  future_feat_dim=Ff).to(device)\n    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\", factor=0.5, patience=2)\n    scaler = GradScaler(enabled=(USE_AMP and torch.cuda.is_available()))\n\n    loss_fn = nn.SmoothL1Loss(reduction=\"none\")\n    w = torch.linspace(1.20, 0.90, H, device=device).view(1, H) if horizon_weighted else None\n\n    best_mae, best = float(\"inf\"), None\n    patience, left = 8, 8\n    MIN_DELTA = 0.0\n\n    for epoch in range(1, epochs+1):\n        model.train()\n        run, nitems = 0.0, 0\n        for batch in tr_loader:\n            if len(batch)==2: xb, yb = batch; zb = None\n            else: xb, yb, zb = batch\n            xb = xb.to(device); yb = yb.to(device); zb = None if zb is None else zb.to(device)\n            opt.zero_grad(set_to_none=True)\n            with amp_ctx():\n                yhat_s = model(xb, zb)\n                L = loss_fn(yhat_s, yb)\n                if w is not None: L = L * w\n                L = L.mean()\n            scaler.scale(L).backward()\n            scaler.unscale_(opt); nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            scaler.step(opt); scaler.update()\n            run += L.item() * xb.size(0); nitems += xb.size(0)\n\n        val_mae_mw, val_mse_s = val_metrics_MW(model, va_loader, bundle.y_scaler)\n        sched.step(val_mae_mw)\n        print(f\"Epoch {epoch:02d} | train loss {run/nitems:.4f} | val MAE(MW) {val_mae_mw:.2f} | val MSE(scaled) {val_mse_s:.4f}\")\n\n        if best_mae - val_mae_mw > MIN_DELTA:\n            best_mae, best, left = val_mae_mw, {k: v.detach().cpu().clone() for k,v in model.state_dict().items()}, patience\n        else:\n            left -= 1\n            if left == 0:\n                print(\"Early stopping.\"); break\n\n    if best is not None: model.load_state_dict(best)\n    return model\n\n@torch.no_grad()\ndef test_report_tf(model, X_test, Y_test_s, y_scaler, Z_test=None, batch=256):\n    dl = DataLoader(SeqDataset(X_test, Y_test_s, Z_test), batch_size=batch, shuffle=False)\n    device = next(model.parameters()).device\n    preds_s, trues_s = [], []\n    for batch in dl:\n        if len(batch)==2: xb, yb = batch; zb = None\n        else: xb, yb, zb = batch\n        xb = xb.to(device); zb = None if zb is None else zb.to(device)\n        yhat_s = model(xb, zb)\n        preds_s.append(yhat_s.cpu().numpy()); trues_s.append(yb.numpy())\n    preds_s, trues_s = np.concatenate(preds_s), np.concatenate(trues_s)\n    preds = y_scaler.inverse_transform(preds_s.reshape(-1,1)).reshape(preds_s.shape)\n    trues = y_scaler.inverse_transform(trues_s.reshape(-1,1)).reshape(trues_s.shape)\n    mae = np.mean(np.abs(preds - trues))\n    rmse = np.sqrt(np.mean((preds - trues)**2))\n    mae_h = np.mean(np.abs(preds - trues), axis=0)\n    print(f\"\\nTest  MAE (MW):  {mae:.2f}\")\n    print(f\"Test RMSE (MW): {rmse:.2f}\")\n    print(\"Horizon-wise MAE (MW):\", np.round(mae_h, 2))\n    return {\"MAE\": mae, \"RMSE\": rmse, \"MAE_by_h\": mae_h}\n\n# ---------- Run ----------\nif __name__ == \"__main__\":\n    model = train_transformer(\n        X_train, Y_train_s, X_val, Y_val_s,\n        Z_train=Z_train, Z_val=Z_val,\n        d_model=128,    # much smaller hidden size\n        nhead=4,        # fewer attention heads\n        layers=2,       # shallower stack\n        d_ff=256,       # reduced feedforward\n        dropout=0.1,    # lighter regularization\n        batch=128,\n        epochs=80,\n        lr=3e-4,        # slightly higher LR works better with small nets\n        weight_decay=5e-4,\n        horizon_weighted=True,\n        seed=42\n    )\n    _ = test_report_tf(model, X_test, Y_test_s, bundle.y_scaler, Z_test=Z_test)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T16:05:02.576259Z","iopub.execute_input":"2025-09-22T16:05:02.576773Z","iopub.status.idle":"2025-09-22T16:06:56.833139Z","shell.execute_reply.started":"2025-09-22T16:05:02.576748Z","shell.execute_reply":"2025-09-22T16:06:56.832498Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 01 | train loss 0.0800 | val MAE(MW) 154.40 | val MSE(scaled) 0.0334\nEpoch 02 | train loss 0.0528 | val MAE(MW) 146.51 | val MSE(scaled) 0.0300\nEpoch 03 | train loss 0.0508 | val MAE(MW) 137.59 | val MSE(scaled) 0.0278\nEpoch 04 | train loss 0.0497 | val MAE(MW) 143.23 | val MSE(scaled) 0.0289\nEpoch 05 | train loss 0.0487 | val MAE(MW) 144.89 | val MSE(scaled) 0.0292\nEpoch 06 | train loss 0.0483 | val MAE(MW) 143.43 | val MSE(scaled) 0.0301\nEpoch 07 | train loss 0.0472 | val MAE(MW) 140.38 | val MSE(scaled) 0.0288\nEpoch 08 | train loss 0.0469 | val MAE(MW) 144.01 | val MSE(scaled) 0.0303\nEpoch 09 | train loss 0.0466 | val MAE(MW) 135.81 | val MSE(scaled) 0.0272\nEpoch 10 | train loss 0.0463 | val MAE(MW) 146.70 | val MSE(scaled) 0.0317\nEpoch 11 | train loss 0.0460 | val MAE(MW) 134.38 | val MSE(scaled) 0.0274\nEpoch 12 | train loss 0.0457 | val MAE(MW) 136.54 | val MSE(scaled) 0.0284\nEpoch 13 | train loss 0.0455 | val MAE(MW) 136.83 | val MSE(scaled) 0.0272\nEpoch 14 | train loss 0.0453 | val MAE(MW) 131.94 | val MSE(scaled) 0.0269\nEpoch 15 | train loss 0.0446 | val MAE(MW) 140.59 | val MSE(scaled) 0.0289\nEpoch 16 | train loss 0.0445 | val MAE(MW) 134.74 | val MSE(scaled) 0.0270\nEpoch 17 | train loss 0.0433 | val MAE(MW) 139.90 | val MSE(scaled) 0.0276\nEpoch 18 | train loss 0.0412 | val MAE(MW) 136.22 | val MSE(scaled) 0.0270\nEpoch 19 | train loss 0.0405 | val MAE(MW) 133.97 | val MSE(scaled) 0.0265\nEpoch 20 | train loss 0.0398 | val MAE(MW) 145.21 | val MSE(scaled) 0.0290\nEpoch 21 | train loss 0.0379 | val MAE(MW) 135.86 | val MSE(scaled) 0.0272\nEpoch 22 | train loss 0.0375 | val MAE(MW) 133.99 | val MSE(scaled) 0.0268\nEarly stopping.\n\nTest  MAE (MW):  195.32\nTest RMSE (MW): 273.24\nHorizon-wise MAE (MW): [195.4  194.39 194.93 194.75 194.32 194.83 195.05 195.16 195.05 194.89\n 194.12 193.99 194.29 193.48 194.21 193.92 194.36 195.25 196.24 197.42\n 197.13 197.54 198.52 198.43]\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# train_transformer_baseline_fixed.py  (minimal deterministic-friendly patch)\n\nimport os, random, math, numpy as np\n\n# ---- Toggle: set True for strict determinism (requires kernel restart) ----\nSTRICT_DETERMINISM = False\n\n# If strict, cuBLAS needs this BEFORE importing torch (and a fresh kernel)\nif STRICT_DETERMINISM:\n    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"  # or \":16:8\"\n\nimport torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# --- seeding (simple & safe) ---\nSEED = 42\ndef seed_everything(seed=42):\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    random.seed(seed); np.random.seed(seed)\n    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n    # keep deterministic cuDNN kernels, but don't force torch.use_deterministic_algorithms\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n    # TF32 off helps reproducibility\n    torch.backends.cuda.matmul.allow_tf32 = False\n    torch.backends.cudnn.allow_tf32 = False\n\nseed_everything(SEED)\n\n# --- AMP (new API with fallback); set USE_AMP=False for tighter reproducibility ---\nUSE_AMP = True\ntry:\n    from torch.amp import autocast, GradScaler\n    def amp_ctx():\n        if not USE_AMP:\n            class _NoOp:\n                def __enter__(self): return None\n                def __exit__(self, *a): return False\n            return _NoOp()\n        use_bf16 = (torch.cuda.is_available()\n                    and torch.cuda.get_device_capability()[0] >= 8)\n        return autocast(device_type=\"cuda\", dtype=torch.bfloat16 if use_bf16 else torch.float16)\nexcept Exception:\n    from torch.cuda.amp import autocast, GradScaler\n    def amp_ctx():\n        return autocast(enabled=(USE_AMP and torch.cuda.is_available()))\n\n# ========== Data ==========\nclass SeqDataset(Dataset):\n    \"\"\"Returns (X, Y) if Z is None; otherwise (X, Y, Z). Avoids collating None.\"\"\"\n    def __init__(self, X, Y, Z_future=None):\n        self.X = torch.from_numpy(X)              # (N, L, D)\n        self.Y = torch.from_numpy(Y)              # (N, H)\n        self.Z = None if Z_future is None else torch.from_numpy(Z_future.astype(np.float32))\n        self.has_future = self.Z is not None\n    def __len__(self): return self.X.shape[0]\n    def __getitem__(self, i):\n        if self.has_future:\n            return self.X[i], self.Y[i], self.Z[i]\n        else:\n            return self.X[i], self.Y[i]\n\ndef _seed_worker(worker_id: int):\n    wseed = torch.initial_seed() % 2**32\n    np.random.seed(wseed); random.seed(wseed)\n\ndef loaders(Xtr,Ytr,Xva,Yva, Ztr=None, Zva=None, batch=128, workers=0, seed: int = SEED):\n    g = torch.Generator().manual_seed(seed)  # deterministic shuffle\n    return (\n        DataLoader(SeqDataset(Xtr,Ytr,Ztr), batch_size=batch, shuffle=True,  drop_last=True,\n                   num_workers=workers, pin_memory=True, worker_init_fn=_seed_worker, generator=g),\n        DataLoader(SeqDataset(Xva,Yva,Zva), batch_size=batch, shuffle=False, drop_last=False,\n                   num_workers=workers, pin_memory=True, worker_init_fn=_seed_worker, generator=g),\n    )\n\n# ========== Model ==========\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=1000):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div); pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))  # (1, max_len, d_model)\n    def forward(self, x): return x + self.pe[:, :x.size(1)]\n\nclass TransformerForecaster(nn.Module):\n    def __init__(self, input_dim, horizon=24, d_model=256, nhead=8, num_layers=3,\n                 d_ff=512, dropout=0.1, future_feat_dim=0):\n        super().__init__()\n        self.horizon = horizon\n        self.input_proj = nn.Linear(input_dim, d_model)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=d_model, nhead=nhead, dim_feedforward=d_ff,\n            dropout=dropout, batch_first=True, norm_first=True\n        )\n        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n        self.pos = PositionalEncoding(d_model)\n        self.cls = nn.Parameter(torch.randn(1, 1, d_model) * 0.02)\n        self.horz_emb = nn.Parameter(torch.randn(horizon, d_model) * 0.02)\n        self.future_proj = nn.Linear(future_feat_dim, d_model) if future_feat_dim and future_feat_dim>0 else None\n        head_in = d_model + d_model + (d_model if self.future_proj else 0)\n        self.head = nn.Sequential(nn.LayerNorm(head_in), nn.Linear(head_in, 256),\n                                  nn.ReLU(), nn.Dropout(dropout), nn.Linear(256, 1))\n\n    def forward(self, x, z_future=None):\n        B, L, _ = x.shape\n        x = self.pos(self.input_proj(x))             # (B,L,d)\n        cls = self.cls.expand(B, 1, -1)\n        enc = self.encoder(torch.cat([cls, x], dim=1))\n        context = enc[:, 0, :]                       # (B,d)\n        H = self.horizon\n        he = self.horz_emb.unsqueeze(0).expand(B, H, -1)  # (B,H,d)\n        ctx = context.unsqueeze(1).expand(B, H, -1)       # (B,H,d)\n        if self.future_proj is not None and z_future is not None:\n            z_proj = self.future_proj(z_future)           # (B,H,d)\n            fuse = torch.cat([ctx, he, z_proj], dim=-1)\n        else:\n            fuse = torch.cat([ctx, he], dim=-1)\n        return self.head(fuse).squeeze(-1)                # (B,H)\n\n# ========== Metrics & training ==========\n@torch.no_grad()\ndef val_metrics_MW(model, loader, y_scaler):\n    model.eval()\n    preds_s, trues_s, n, mse_s = [], [], 0, 0.0\n    for batch in loader:\n        if len(batch)==2: xb, yb = batch; zb = None\n        else: xb, yb, zb = batch\n        xb = xb.to(next(model.parameters()).device)\n        yb = yb.to(next(model.parameters()).device)\n        zb = None if zb is None else zb.to(xb.device)\n        yhat_s = model(xb, zb)\n        mse_s += torch.mean((yhat_s - yb)**2).item() * xb.size(0); n += xb.size(0)\n        preds_s.append(yhat_s.detach().cpu().numpy()); trues_s.append(yb.detach().cpu().numpy())\n    preds_s, trues_s = np.concatenate(preds_s), np.concatenate(trues_s)\n    preds = bundle.y_scaler.inverse_transform(preds_s.reshape(-1,1)).reshape(preds_s.shape)\n    trues = bundle.y_scaler.inverse_transform(trues_s.reshape(-1,1)).reshape(trues_s.shape)\n    mae = np.mean(np.abs(preds - trues))\n    return mae, (mse_s / n)\n\ndef train_transformer(\n    X_train, Y_train_s, X_val, Y_val_s, Z_train=None, Z_val=None,\n    d_model=256, nhead=8, layers=3, d_ff=512, dropout=0.1,\n    batch=128, epochs=80, lr=2e-4, weight_decay=2e-4, horizon_weighted=True, seed: int = SEED\n):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    Din, H = X_train.shape[2], Y_train_s.shape[1]\n    Ff = 0 if Z_train is None else Z_train.shape[2]\n    tr_loader, va_loader = loaders(X_train, Y_train_s, X_val, Y_val_s, Z_train, Z_val,\n                                   batch=batch, workers=0, seed=seed)\n\n    model = TransformerForecaster(Din, horizon=H, d_model=d_model, nhead=nhead,\n                                  num_layers=layers, d_ff=d_ff, dropout=dropout,\n                                  future_feat_dim=Ff).to(device)\n    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\", factor=0.5, patience=2)\n    scaler = GradScaler(enabled=(USE_AMP and torch.cuda.is_available()))\n\n    loss_fn = nn.SmoothL1Loss(reduction=\"none\")\n    w = torch.linspace(1.20, 0.90, H, device=device).view(1, H) if horizon_weighted else None\n\n    best_mae, best = float(\"inf\"), None\n    patience, left = 8, 8\n    MIN_DELTA = 0.0\n\n    for epoch in range(1, epochs+1):\n        model.train()\n        run, nitems = 0.0, 0\n        for batch in tr_loader:\n            if len(batch)==2: xb, yb = batch; zb = None\n            else: xb, yb, zb = batch\n            xb = xb.to(device); yb = yb.to(device); zb = None if zb is None else zb.to(device)\n            opt.zero_grad(set_to_none=True)\n            with amp_ctx():\n                yhat_s = model(xb, zb)\n                L = loss_fn(yhat_s, yb)\n                if w is not None: L = L * w\n                L = L.mean()\n            scaler.scale(L).backward()\n            scaler.unscale_(opt); nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            scaler.step(opt); scaler.update()\n            run += L.item() * xb.size(0); nitems += xb.size(0)\n\n        val_mae_mw, val_mse_s = val_metrics_MW(model, va_loader, bundle.y_scaler)\n        sched.step(val_mae_mw)\n        print(f\"Epoch {epoch:02d} | train loss {run/nitems:.4f} | val MAE(MW) {val_mae_mw:.2f} | val MSE(scaled) {val_mse_s:.4f}\")\n\n        if best_mae - val_mae_mw > MIN_DELTA:\n            best_mae, best, left = val_mae_mw, {k: v.detach().cpu().clone() for k,v in model.state_dict().items()}, patience\n        else:\n            left -= 1\n            if left == 0:\n                print(\"Early stopping.\"); break\n\n    if best is not None: model.load_state_dict(best)\n    return model\n\n@torch.no_grad()\ndef test_report_tf(model, X_test, Y_test_s, y_scaler, Z_test=None, batch=256):\n    dl = DataLoader(SeqDataset(X_test, Y_test_s, Z_test), batch_size=batch, shuffle=False)\n    device = next(model.parameters()).device\n    preds_s, trues_s = [], []\n    for batch in dl:\n        if len(batch)==2: xb, yb = batch; zb = None\n        else: xb, yb, zb = batch\n        xb = xb.to(device); zb = None if zb is None else zb.to(device)\n        yhat_s = model(xb, zb)\n        preds_s.append(yhat_s.cpu().numpy()); trues_s.append(yb.numpy())\n    preds_s, trues_s = np.concatenate(preds_s), np.concatenate(trues_s)\n    preds = y_scaler.inverse_transform(preds_s.reshape(-1,1)).reshape(preds_s.shape)\n    trues = y_scaler.inverse_transform(trues_s.reshape(-1,1)).reshape(trues_s.shape)\n    mae = np.mean(np.abs(preds - trues))\n    rmse = np.sqrt(np.mean((preds - trues)**2))\n    mae_h = np.mean(np.abs(preds - trues), axis=0)\n    print(f\"\\nTest  MAE (MW):  {mae:.2f}\")\n    print(f\"Test RMSE (MW): {rmse:.2f}\")\n    print(\"Horizon-wise MAE (MW):\", np.round(mae_h, 2))\n    return {\"MAE\": mae, \"RMSE\": rmse, \"MAE_by_h\": mae_h}\n\n# ---------- Run ----------\nif __name__ == \"__main__\":\n    model = train_transformer(\n        X_train, Y_train_s, X_val, Y_val_s,\n        Z_train=Z_train, Z_val=Z_val,\n        d_model=160,    # middle ground, not too tiny\n        nhead=4,        # keeps per-head dim = 40\n        layers=3,       # slightly deeper than 2\n        d_ff=320,       # ~2× d_model (lightweight FFN)\n        dropout=0.12,   # modest regularization\n        batch=128,\n        epochs=100,     # let it converge longer\n        lr=2.5e-4,      # tuned for stability\n        weight_decay=5e-4,\n        horizon_weighted=True,\n        seed=42\n    )\n    _ = test_report_tf(model, X_test, Y_test_s, bundle.y_scaler, Z_test=Z_test)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T20:09:43.804969Z","iopub.execute_input":"2025-09-24T20:09:43.805275Z","iopub.status.idle":"2025-09-24T20:12:05.325614Z","shell.execute_reply.started":"2025-09-24T20:09:43.805253Z","shell.execute_reply":"2025-09-24T20:12:05.324647Z"}},"outputs":[{"name":"stdout","text":"Epoch 01 | train loss 0.0821 | val MAE(MW) 152.35 | val MSE(scaled) 0.0329\nEpoch 02 | train loss 0.0533 | val MAE(MW) 181.14 | val MSE(scaled) 0.0406\nEpoch 03 | train loss 0.0512 | val MAE(MW) 151.35 | val MSE(scaled) 0.0322\nEpoch 04 | train loss 0.0500 | val MAE(MW) 151.32 | val MSE(scaled) 0.0324\nEpoch 05 | train loss 0.0491 | val MAE(MW) 146.47 | val MSE(scaled) 0.0312\nEpoch 06 | train loss 0.0488 | val MAE(MW) 146.16 | val MSE(scaled) 0.0312\nEpoch 07 | train loss 0.0483 | val MAE(MW) 144.22 | val MSE(scaled) 0.0318\nEpoch 08 | train loss 0.0478 | val MAE(MW) 147.73 | val MSE(scaled) 0.0312\nEpoch 09 | train loss 0.0474 | val MAE(MW) 138.10 | val MSE(scaled) 0.0291\nEpoch 10 | train loss 0.0470 | val MAE(MW) 137.60 | val MSE(scaled) 0.0288\nEpoch 11 | train loss 0.0470 | val MAE(MW) 139.35 | val MSE(scaled) 0.0297\nEpoch 12 | train loss 0.0464 | val MAE(MW) 132.37 | val MSE(scaled) 0.0270\nEpoch 13 | train loss 0.0465 | val MAE(MW) 137.84 | val MSE(scaled) 0.0287\nEpoch 14 | train loss 0.0459 | val MAE(MW) 141.21 | val MSE(scaled) 0.0295\nEpoch 15 | train loss 0.0456 | val MAE(MW) 140.07 | val MSE(scaled) 0.0299\nEpoch 16 | train loss 0.0439 | val MAE(MW) 135.85 | val MSE(scaled) 0.0274\nEpoch 17 | train loss 0.0437 | val MAE(MW) 134.17 | val MSE(scaled) 0.0280\nEpoch 18 | train loss 0.0430 | val MAE(MW) 140.64 | val MSE(scaled) 0.0298\nEpoch 19 | train loss 0.0389 | val MAE(MW) 138.74 | val MSE(scaled) 0.0296\nEpoch 20 | train loss 0.0379 | val MAE(MW) 137.86 | val MSE(scaled) 0.0291\nEarly stopping.\n\nTest  MAE (MW):  185.25\nTest RMSE (MW): 257.21\nHorizon-wise MAE (MW): [185.54 184.33 184.63 185.84 185.76 185.13 185.73 184.89 185.1  184.68\n 184.73 184.56 184.44 184.42 184.72 185.39 185.47 184.52 184.72 184.84\n 185.48 186.5  187.66 186.89]\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# --- AMP that works on both old/new PyTorch ---\nUSE_AMP = True\ntry:\n    # New API (PyTorch ≥ 2.0)\n    from torch.amp import autocast as _autocast_new, GradScaler as _GradScaler_new\n    def amp_ctx():\n        if not (USE_AMP and torch.cuda.is_available()):\n            class _NoOp:\n                def __enter__(self): return None\n                def __exit__(self, *a): return False\n            return _NoOp()\n        use_bf16 = (torch.cuda.get_device_capability()[0] >= 8)\n        return _autocast_new(device_type=\"cuda\", dtype=torch.bfloat16 if use_bf16 else torch.float16)\n    GradScaler = _GradScaler_new\nexcept Exception:\n    # Old API fallback\n    from torch.cuda.amp import autocast as _autocast_old, GradScaler as _GradScaler_old\n    def amp_ctx():\n        return _autocast_old(enabled=(USE_AMP and torch.cuda.is_available()))\n    GradScaler = _GradScaler_old\n\n# ======== Auto-tune with Optuna (tiny, safe search) ========\n# pip install optuna==3.*  (once)\n\nimport math\nimport optuna\nfrom optuna.samplers import TPESampler\nfrom optuna.pruners import MedianPruner\n\ndef _valid_heads(d_model):\n    # prefer heads with >=16 dims/head (stability) and common divisors\n    cands = [2, 3, 4, 5, 6, 8, 10]\n    return [h for h in cands if d_model % h == 0 and (d_model // h) >= 16]\n\ndef objective(trial: optuna.Trial):\n    d_model = trial.suggest_categorical(\"d_model\", [96, 128, 160, 192, 224])\n\n    # pick heads with unique names per d_model (so distributions never change)\n    if d_model == 96:\n        nhead = trial.suggest_categorical(\"nhead_96\", [3, 4, 6, 8])      # 96 / h is integer\n    elif d_model == 128:\n        nhead = trial.suggest_categorical(\"nhead_128\", [4, 8, 16])\n    elif d_model == 160:\n        nhead = trial.suggest_categorical(\"nhead_160\", [4, 5, 10])\n    elif d_model == 192:\n        nhead = trial.suggest_categorical(\"nhead_192\", [3, 4, 6, 8, 12])\n    else:  # 224\n        nhead = trial.suggest_categorical(\"nhead_224\", [4, 7, 8, 14])\n\n    layers     = trial.suggest_categorical(\"layers\", [2, 3, 4])\n    dff_mult   = trial.suggest_categorical(\"d_ff_mult\", [2, 3, 4])\n    d_ff       = d_model * dff_mult\n    dropout    = trial.suggest_float(\"dropout\", 0.08, 0.18)\n    batch      = trial.suggest_categorical(\"batch\", [96, 128, 160])\n    lr         = trial.suggest_float(\"lr\", 1.5e-4, 4.0e-4, log=True)\n    weight_decay = trial.suggest_float(\"weight_decay\", 2e-5, 1e-3, log=True)\n\n    try:\n        model = train_transformer(\n            X_train, Y_train_s, X_val, Y_val_s,\n            Z_train=Z_train, Z_val=Z_val,\n            d_model=d_model, nhead=nhead, layers=layers, d_ff=d_ff,\n            dropout=dropout, batch=batch, epochs=100 if d_model <= 160 else 90,\n            lr=lr, weight_decay=weight_decay, horizon_weighted=True, seed=SEED\n        )\n        # reuse your MW-scale validation metric for selection\n        va_loader = DataLoader(SeqDataset(X_val, Y_val_s, Z_val),\n                               batch_size=256, shuffle=False)\n        val_mae_mw, _ = val_metrics_MW(model, va_loader, bundle.y_scaler)\n        # report so pruners/visualizers can see it\n        trial.set_user_attr(\"val_mae_mw\", float(val_mae_mw))\n        return float(val_mae_mw)\n\n    except RuntimeError as e:\n        # Handle occasional CUDA OOM / numerical hiccups gracefully\n        if \"CUDA\" in str(e) or \"out of memory\" in str(e).lower():\n            raise optuna.TrialPruned()  # prune the bad combo\n        raise\n\ndef run_study(n_trials=24, study_name=\"tf_tiny_sweep\", seed=42):\n    study = optuna.create_study(\n        direction=\"minimize\",\n        study_name=study_name,\n        sampler=TPESampler(seed=seed, multivariate=True, n_startup_trials=8),\n        pruner=MedianPruner(n_startup_trials=6),\n        storage=\"sqlite:///optuna_tf.db\",\n        load_if_exists=True\n    )\n    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n\n    print(\"\\nBest trial:\")\n    bt = study.best_trial\n    print(f\"  val_MAE(MW): {bt.value:.3f}\")\n    for k, v in bt.params.items():\n        print(f\"  {k}: {v}\")\n\n    # ------- retrain on train+val, then test --------\n    import numpy as np\n    X_all = np.concatenate([X_train, X_val], axis=0)\n    Y_all = np.concatenate([Y_train_s, Y_val_s], axis=0)\n    Z_all = None if Z_train is None else np.concatenate([Z_train, Z_val], axis=0)\n\n    best = bt.params\n    nhead_key = next(k for k in best_params if k.startswith(\"nhead\"))\n    model = train_transformer(\n        X_all, Y_all, X_val, Y_val_s,   # dummy val to drive early-stopping; you can pass a small holdout, too\n        Z_train=Z_all, Z_val=Z_val,     # keep Z for shape; early-stop will trigger quickly\n        d_model=best[\"d_model\"],\n        nhead=best[nhead_key],\n        layers=best[\"layers\"],\n        d_ff=best[\"d_model\"] * best[\"d_ff_mult\"],\n        dropout=best[\"dropout\"],\n        batch=best[\"batch\"],\n        epochs=110 if best[\"d_model\"] <= 160 else 100,\n        lr=best[\"lr\"],\n        weight_decay=best[\"weight_decay\"],\n        horizon_weighted=True,\n        seed=SEED\n    )\n\n    print(\"\\n=== Final Test Report with best params ===\")\n    _ = test_report_tf(model, X_test, Y_test_s, bundle.y_scaler, Z_test=Z_test)\n    return study\n\n# ----- entry point for tuning -----\nif __name__ == \"__main__\":\n    # Comment out your manual single-run block and run the tuner:\n    study = run_study(n_trials=24, study_name=\"tf_tiny_sweep_v1\", seed=42)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T20:14:37.495971Z","iopub.execute_input":"2025-09-24T20:14:37.496860Z","iopub.status.idle":"2025-09-24T21:21:36.084552Z","shell.execute_reply.started":"2025-09-24T20:14:37.496834Z","shell.execute_reply":"2025-09-24T21:21:36.083292Z"}},"outputs":[{"name":"stderr","text":"[I 2025-09-24 20:14:39,498] A new study created in RDB with name: tf_tiny_sweep_v1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/24 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fbe5fc337b94961b7952468b63277f9"}},"metadata":{}},{"name":"stdout","text":"Epoch 01 | train loss 0.0965 | val MAE(MW) 162.07 | val MSE(scaled) 0.0400\nEpoch 02 | train loss 0.0555 | val MAE(MW) 160.96 | val MSE(scaled) 0.0362\nEpoch 03 | train loss 0.0525 | val MAE(MW) 147.98 | val MSE(scaled) 0.0322\nEpoch 04 | train loss 0.0510 | val MAE(MW) 145.68 | val MSE(scaled) 0.0318\nEpoch 05 | train loss 0.0501 | val MAE(MW) 160.27 | val MSE(scaled) 0.0356\nEpoch 06 | train loss 0.0493 | val MAE(MW) 138.20 | val MSE(scaled) 0.0297\nEpoch 07 | train loss 0.0487 | val MAE(MW) 140.45 | val MSE(scaled) 0.0297\nEpoch 08 | train loss 0.0481 | val MAE(MW) 135.19 | val MSE(scaled) 0.0289\nEpoch 09 | train loss 0.0478 | val MAE(MW) 136.43 | val MSE(scaled) 0.0286\nEpoch 10 | train loss 0.0474 | val MAE(MW) 153.78 | val MSE(scaled) 0.0356\nEpoch 11 | train loss 0.0469 | val MAE(MW) 144.18 | val MSE(scaled) 0.0321\nEpoch 12 | train loss 0.0459 | val MAE(MW) 138.70 | val MSE(scaled) 0.0301\nEpoch 13 | train loss 0.0456 | val MAE(MW) 146.88 | val MSE(scaled) 0.0332\nEpoch 14 | train loss 0.0454 | val MAE(MW) 140.43 | val MSE(scaled) 0.0308\nEpoch 15 | train loss 0.0442 | val MAE(MW) 137.47 | val MSE(scaled) 0.0295\nEpoch 16 | train loss 0.0434 | val MAE(MW) 135.91 | val MSE(scaled) 0.0285\nEarly stopping.\n[I 2025-09-24 20:16:15,483] Trial 0 finished with value: 135.19117736816406 and parameters: {'d_model': 128, 'nhead_128': 16, 'layers': 3, 'd_ff_mult': 2, 'dropout': 0.09818249672071006, 'batch': 160, 'lr': 0.00022913230872594366, 'weight_decay': 6.249130142521746e-05}. Best is trial 0 with value: 135.19117736816406.\nEpoch 01 | train loss 0.1083 | val MAE(MW) 178.88 | val MSE(scaled) 0.0485\nEpoch 02 | train loss 0.0581 | val MAE(MW) 145.03 | val MSE(scaled) 0.0326\nEpoch 03 | train loss 0.0545 | val MAE(MW) 146.03 | val MSE(scaled) 0.0309\nEpoch 04 | train loss 0.0530 | val MAE(MW) 140.88 | val MSE(scaled) 0.0295\nEpoch 05 | train loss 0.0519 | val MAE(MW) 153.46 | val MSE(scaled) 0.0327\nEpoch 06 | train loss 0.0511 | val MAE(MW) 138.05 | val MSE(scaled) 0.0287\nEpoch 07 | train loss 0.0505 | val MAE(MW) 139.29 | val MSE(scaled) 0.0293\nEpoch 08 | train loss 0.0502 | val MAE(MW) 141.24 | val MSE(scaled) 0.0306\nEpoch 09 | train loss 0.0498 | val MAE(MW) 139.36 | val MSE(scaled) 0.0298\nEpoch 10 | train loss 0.0488 | val MAE(MW) 139.08 | val MSE(scaled) 0.0300\nEpoch 11 | train loss 0.0486 | val MAE(MW) 138.72 | val MSE(scaled) 0.0293\nEpoch 12 | train loss 0.0484 | val MAE(MW) 137.70 | val MSE(scaled) 0.0293\nEpoch 13 | train loss 0.0481 | val MAE(MW) 143.19 | val MSE(scaled) 0.0313\nEpoch 14 | train loss 0.0480 | val MAE(MW) 141.12 | val MSE(scaled) 0.0306\nEpoch 15 | train loss 0.0478 | val MAE(MW) 137.56 | val MSE(scaled) 0.0295\nEpoch 16 | train loss 0.0473 | val MAE(MW) 135.30 | val MSE(scaled) 0.0285\nEpoch 17 | train loss 0.0472 | val MAE(MW) 141.46 | val MSE(scaled) 0.0304\nEpoch 18 | train loss 0.0465 | val MAE(MW) 139.99 | val MSE(scaled) 0.0300\nEpoch 19 | train loss 0.0458 | val MAE(MW) 144.93 | val MSE(scaled) 0.0313\nEpoch 20 | train loss 0.0438 | val MAE(MW) 140.67 | val MSE(scaled) 0.0306\nEpoch 21 | train loss 0.0433 | val MAE(MW) 143.29 | val MSE(scaled) 0.0315\nEpoch 22 | train loss 0.0432 | val MAE(MW) 143.14 | val MSE(scaled) 0.0313\nEpoch 23 | train loss 0.0424 | val MAE(MW) 139.50 | val MSE(scaled) 0.0298\nEpoch 24 | train loss 0.0422 | val MAE(MW) 141.18 | val MSE(scaled) 0.0308\nEarly stopping.\n[I 2025-09-24 20:18:36,902] Trial 1 finished with value: 135.2974090576172 and parameters: {'d_model': 96, 'nhead_96': 3, 'layers': 3, 'd_ff_mult': 4, 'dropout': 0.1608397348116461, 'batch': 160, 'lr': 0.00023098429835955887, 'weight_decay': 3.2238089455218364e-05}. Best is trial 0 with value: 135.19117736816406.\nEpoch 01 | train loss 0.0946 | val MAE(MW) 165.76 | val MSE(scaled) 0.0383\nEpoch 02 | train loss 0.0562 | val MAE(MW) 167.71 | val MSE(scaled) 0.0383\nEpoch 03 | train loss 0.0537 | val MAE(MW) 148.26 | val MSE(scaled) 0.0314\nEpoch 04 | train loss 0.0524 | val MAE(MW) 154.27 | val MSE(scaled) 0.0331\nEpoch 05 | train loss 0.0515 | val MAE(MW) 154.20 | val MSE(scaled) 0.0339\nEpoch 06 | train loss 0.0509 | val MAE(MW) 149.54 | val MSE(scaled) 0.0322\nEpoch 07 | train loss 0.0497 | val MAE(MW) 145.91 | val MSE(scaled) 0.0315\nEpoch 08 | train loss 0.0494 | val MAE(MW) 145.50 | val MSE(scaled) 0.0315\nEpoch 09 | train loss 0.0492 | val MAE(MW) 142.11 | val MSE(scaled) 0.0305\nEpoch 10 | train loss 0.0488 | val MAE(MW) 161.14 | val MSE(scaled) 0.0370\nEpoch 11 | train loss 0.0486 | val MAE(MW) 148.43 | val MSE(scaled) 0.0330\nEpoch 12 | train loss 0.0483 | val MAE(MW) 146.75 | val MSE(scaled) 0.0327\nEpoch 13 | train loss 0.0476 | val MAE(MW) 150.63 | val MSE(scaled) 0.0336\nEpoch 14 | train loss 0.0474 | val MAE(MW) 147.62 | val MSE(scaled) 0.0330\nEpoch 15 | train loss 0.0471 | val MAE(MW) 149.77 | val MSE(scaled) 0.0336\nEpoch 16 | train loss 0.0463 | val MAE(MW) 144.73 | val MSE(scaled) 0.0312\nEpoch 17 | train loss 0.0458 | val MAE(MW) 145.96 | val MSE(scaled) 0.0318\nEarly stopping.\n[I 2025-09-24 20:20:40,107] Trial 2 finished with value: 142.114990234375 and parameters: {'d_model': 160, 'nhead_160': 10, 'layers': 3, 'd_ff_mult': 2, 'dropout': 0.17218742350231167, 'batch': 128, 'lr': 0.00020638199564507485, 'weight_decay': 9.149156410950807e-05}. Best is trial 0 with value: 135.19117736816406.\nEpoch 01 | train loss 0.0923 | val MAE(MW) 162.21 | val MSE(scaled) 0.0387\nEpoch 02 | train loss 0.0557 | val MAE(MW) 152.52 | val MSE(scaled) 0.0321\nEpoch 03 | train loss 0.0530 | val MAE(MW) 144.00 | val MSE(scaled) 0.0299\nEpoch 04 | train loss 0.0516 | val MAE(MW) 137.95 | val MSE(scaled) 0.0285\nEpoch 05 | train loss 0.0505 | val MAE(MW) 145.54 | val MSE(scaled) 0.0311\nEpoch 06 | train loss 0.0499 | val MAE(MW) 137.90 | val MSE(scaled) 0.0285\nEpoch 07 | train loss 0.0494 | val MAE(MW) 136.54 | val MSE(scaled) 0.0283\nEpoch 08 | train loss 0.0488 | val MAE(MW) 143.17 | val MSE(scaled) 0.0306\nEpoch 09 | train loss 0.0486 | val MAE(MW) 139.64 | val MSE(scaled) 0.0302\nEpoch 10 | train loss 0.0481 | val MAE(MW) 138.88 | val MSE(scaled) 0.0292\nEpoch 11 | train loss 0.0472 | val MAE(MW) 132.25 | val MSE(scaled) 0.0270\nEpoch 12 | train loss 0.0470 | val MAE(MW) 134.52 | val MSE(scaled) 0.0281\nEpoch 13 | train loss 0.0468 | val MAE(MW) 133.93 | val MSE(scaled) 0.0275\nEpoch 14 | train loss 0.0466 | val MAE(MW) 138.39 | val MSE(scaled) 0.0298\nEpoch 15 | train loss 0.0460 | val MAE(MW) 135.56 | val MSE(scaled) 0.0285\nEpoch 16 | train loss 0.0456 | val MAE(MW) 132.12 | val MSE(scaled) 0.0271\nEpoch 17 | train loss 0.0451 | val MAE(MW) 134.37 | val MSE(scaled) 0.0278\nEpoch 18 | train loss 0.0450 | val MAE(MW) 138.36 | val MSE(scaled) 0.0295\nEpoch 19 | train loss 0.0445 | val MAE(MW) 135.87 | val MSE(scaled) 0.0284\nEpoch 20 | train loss 0.0435 | val MAE(MW) 132.59 | val MSE(scaled) 0.0273\nEpoch 21 | train loss 0.0430 | val MAE(MW) 133.38 | val MSE(scaled) 0.0271\nEpoch 22 | train loss 0.0426 | val MAE(MW) 134.89 | val MSE(scaled) 0.0276\nEpoch 23 | train loss 0.0422 | val MAE(MW) 133.51 | val MSE(scaled) 0.0272\nEpoch 24 | train loss 0.0420 | val MAE(MW) 133.30 | val MSE(scaled) 0.0274\nEarly stopping.\n[I 2025-09-24 20:23:39,134] Trial 3 finished with value: 132.11605834960938 and parameters: {'d_model': 128, 'nhead_128': 8, 'layers': 2, 'd_ff_mult': 3, 'dropout': 0.15290071680409872, 'batch': 96, 'lr': 0.00016805361362242909, 'weight_decay': 0.0005853516230124262}. Best is trial 3 with value: 132.11605834960938.\nEpoch 01 | train loss 0.1072 | val MAE(MW) 178.91 | val MSE(scaled) 0.0473\nEpoch 02 | train loss 0.0584 | val MAE(MW) 188.21 | val MSE(scaled) 0.0443\nEpoch 03 | train loss 0.0544 | val MAE(MW) 171.14 | val MSE(scaled) 0.0385\nEpoch 04 | train loss 0.0527 | val MAE(MW) 164.01 | val MSE(scaled) 0.0362\nEpoch 05 | train loss 0.0515 | val MAE(MW) 162.14 | val MSE(scaled) 0.0363\nEpoch 06 | train loss 0.0508 | val MAE(MW) 149.01 | val MSE(scaled) 0.0324\nEpoch 07 | train loss 0.0502 | val MAE(MW) 147.42 | val MSE(scaled) 0.0318\nEpoch 08 | train loss 0.0496 | val MAE(MW) 151.80 | val MSE(scaled) 0.0343\nEpoch 09 | train loss 0.0491 | val MAE(MW) 143.12 | val MSE(scaled) 0.0305\nEpoch 10 | train loss 0.0487 | val MAE(MW) 158.56 | val MSE(scaled) 0.0354\nEpoch 11 | train loss 0.0484 | val MAE(MW) 146.18 | val MSE(scaled) 0.0323\nEpoch 12 | train loss 0.0481 | val MAE(MW) 148.13 | val MSE(scaled) 0.0332\nEpoch 13 | train loss 0.0473 | val MAE(MW) 146.53 | val MSE(scaled) 0.0329\nEpoch 14 | train loss 0.0470 | val MAE(MW) 143.10 | val MSE(scaled) 0.0324\nEpoch 15 | train loss 0.0469 | val MAE(MW) 146.47 | val MSE(scaled) 0.0335\nEpoch 16 | train loss 0.0466 | val MAE(MW) 141.04 | val MSE(scaled) 0.0308\nEpoch 17 | train loss 0.0462 | val MAE(MW) 144.33 | val MSE(scaled) 0.0323\nEpoch 18 | train loss 0.0455 | val MAE(MW) 143.51 | val MSE(scaled) 0.0318\nEpoch 19 | train loss 0.0451 | val MAE(MW) 144.38 | val MSE(scaled) 0.0326\nEpoch 20 | train loss 0.0432 | val MAE(MW) 145.72 | val MSE(scaled) 0.0325\nEpoch 21 | train loss 0.0426 | val MAE(MW) 144.76 | val MSE(scaled) 0.0322\nEpoch 22 | train loss 0.0421 | val MAE(MW) 146.32 | val MSE(scaled) 0.0330\nEpoch 23 | train loss 0.0411 | val MAE(MW) 146.67 | val MSE(scaled) 0.0332\nEpoch 24 | train loss 0.0410 | val MAE(MW) 148.25 | val MSE(scaled) 0.0342\nEarly stopping.\n[I 2025-09-24 20:28:19,707] Trial 4 finished with value: 141.04151916503906 and parameters: {'d_model': 96, 'nhead_96': 6, 'layers': 4, 'd_ff_mult': 3, 'dropout': 0.1322732829381994, 'batch': 96, 'lr': 0.0001546960089828645, 'weight_decay': 0.0002411416314677822}. Best is trial 3 with value: 132.11605834960938.\nEpoch 01 | train loss 0.0881 | val MAE(MW) 160.97 | val MSE(scaled) 0.0352\nEpoch 02 | train loss 0.0548 | val MAE(MW) 158.74 | val MSE(scaled) 0.0338\nEpoch 03 | train loss 0.0527 | val MAE(MW) 140.22 | val MSE(scaled) 0.0281\nEpoch 04 | train loss 0.0513 | val MAE(MW) 149.62 | val MSE(scaled) 0.0312\nEpoch 05 | train loss 0.0505 | val MAE(MW) 141.14 | val MSE(scaled) 0.0286\nEpoch 06 | train loss 0.0499 | val MAE(MW) 136.35 | val MSE(scaled) 0.0278\nEpoch 07 | train loss 0.0494 | val MAE(MW) 129.85 | val MSE(scaled) 0.0262\nEpoch 08 | train loss 0.0488 | val MAE(MW) 131.49 | val MSE(scaled) 0.0265\nEpoch 09 | train loss 0.0484 | val MAE(MW) 141.95 | val MSE(scaled) 0.0295\nEpoch 10 | train loss 0.0479 | val MAE(MW) 135.74 | val MSE(scaled) 0.0274\nEpoch 11 | train loss 0.0465 | val MAE(MW) 137.71 | val MSE(scaled) 0.0280\nEpoch 12 | train loss 0.0453 | val MAE(MW) 133.98 | val MSE(scaled) 0.0271\nEpoch 13 | train loss 0.0444 | val MAE(MW) 135.41 | val MSE(scaled) 0.0266\nEpoch 14 | train loss 0.0410 | val MAE(MW) 135.20 | val MSE(scaled) 0.0274\nEpoch 15 | train loss 0.0399 | val MAE(MW) 132.67 | val MSE(scaled) 0.0266\nEarly stopping.\n[I 2025-09-24 20:30:36,060] Trial 5 finished with value: 129.84996032714844 and parameters: {'d_model': 160, 'nhead_160': 4, 'layers': 4, 'd_ff_mult': 4, 'dropout': 0.16036720768991145, 'batch': 128, 'lr': 0.00033115820381518263, 'weight_decay': 0.0006659816075173326}. Best is trial 5 with value: 129.84996032714844.\nEpoch 01 | train loss 0.0895 | val MAE(MW) 152.98 | val MSE(scaled) 0.0367\nEpoch 02 | train loss 0.0550 | val MAE(MW) 171.04 | val MSE(scaled) 0.0366\nEpoch 03 | train loss 0.0526 | val MAE(MW) 138.41 | val MSE(scaled) 0.0283\nEpoch 04 | train loss 0.0512 | val MAE(MW) 136.88 | val MSE(scaled) 0.0274\nEpoch 05 | train loss 0.0503 | val MAE(MW) 133.57 | val MSE(scaled) 0.0269\nEpoch 06 | train loss 0.0496 | val MAE(MW) 132.61 | val MSE(scaled) 0.0269\nEpoch 07 | train loss 0.0494 | val MAE(MW) 131.18 | val MSE(scaled) 0.0265\nEpoch 08 | train loss 0.0487 | val MAE(MW) 130.82 | val MSE(scaled) 0.0256\nEpoch 09 | train loss 0.0483 | val MAE(MW) 132.33 | val MSE(scaled) 0.0264\nEpoch 10 | train loss 0.0477 | val MAE(MW) 132.30 | val MSE(scaled) 0.0263\nEpoch 11 | train loss 0.0477 | val MAE(MW) 128.94 | val MSE(scaled) 0.0257\nEpoch 12 | train loss 0.0472 | val MAE(MW) 130.67 | val MSE(scaled) 0.0262\nEpoch 13 | train loss 0.0468 | val MAE(MW) 132.27 | val MSE(scaled) 0.0262\nEpoch 14 | train loss 0.0470 | val MAE(MW) 142.43 | val MSE(scaled) 0.0297\nEpoch 15 | train loss 0.0456 | val MAE(MW) 135.31 | val MSE(scaled) 0.0273\nEpoch 16 | train loss 0.0449 | val MAE(MW) 130.90 | val MSE(scaled) 0.0260\nEpoch 17 | train loss 0.0440 | val MAE(MW) 135.22 | val MSE(scaled) 0.0269\nEpoch 18 | train loss 0.0404 | val MAE(MW) 133.75 | val MSE(scaled) 0.0266\nEpoch 19 | train loss 0.0391 | val MAE(MW) 134.02 | val MSE(scaled) 0.0268\nEarly stopping.\n[I 2025-09-24 20:33:24,902] Trial 6 finished with value: 128.9370880126953 and parameters: {'d_model': 224, 'nhead_224': 4, 'layers': 4, 'd_ff_mult': 2, 'dropout': 0.15030189588951778, 'batch': 128, 'lr': 0.00019201831060831865, 'weight_decay': 0.00013990727307918162}. Best is trial 6 with value: 128.9370880126953.\nEpoch 01 | train loss 0.0882 | val MAE(MW) 149.37 | val MSE(scaled) 0.0331\nEpoch 02 | train loss 0.0550 | val MAE(MW) 144.15 | val MSE(scaled) 0.0303\nEpoch 03 | train loss 0.0526 | val MAE(MW) 149.49 | val MSE(scaled) 0.0313\nEpoch 04 | train loss 0.0513 | val MAE(MW) 151.98 | val MSE(scaled) 0.0323\nEpoch 05 | train loss 0.0505 | val MAE(MW) 159.29 | val MSE(scaled) 0.0336\nEpoch 06 | train loss 0.0491 | val MAE(MW) 140.99 | val MSE(scaled) 0.0290\nEpoch 07 | train loss 0.0488 | val MAE(MW) 140.15 | val MSE(scaled) 0.0279\nEpoch 08 | train loss 0.0484 | val MAE(MW) 139.54 | val MSE(scaled) 0.0284\nEpoch 09 | train loss 0.0482 | val MAE(MW) 135.82 | val MSE(scaled) 0.0280\nEpoch 10 | train loss 0.0479 | val MAE(MW) 144.58 | val MSE(scaled) 0.0305\nEpoch 11 | train loss 0.0476 | val MAE(MW) 133.87 | val MSE(scaled) 0.0273\nEpoch 12 | train loss 0.0473 | val MAE(MW) 133.69 | val MSE(scaled) 0.0274\nEpoch 13 | train loss 0.0470 | val MAE(MW) 135.33 | val MSE(scaled) 0.0269\nEpoch 14 | train loss 0.0463 | val MAE(MW) 138.94 | val MSE(scaled) 0.0282\nEpoch 15 | train loss 0.0462 | val MAE(MW) 140.38 | val MSE(scaled) 0.0284\nEpoch 16 | train loss 0.0443 | val MAE(MW) 138.55 | val MSE(scaled) 0.0294\nEpoch 17 | train loss 0.0430 | val MAE(MW) 132.36 | val MSE(scaled) 0.0269\nEpoch 18 | train loss 0.0421 | val MAE(MW) 134.00 | val MSE(scaled) 0.0273\nEpoch 19 | train loss 0.0416 | val MAE(MW) 138.87 | val MSE(scaled) 0.0287\nEpoch 20 | train loss 0.0411 | val MAE(MW) 135.60 | val MSE(scaled) 0.0275\nEpoch 21 | train loss 0.0392 | val MAE(MW) 134.49 | val MSE(scaled) 0.0273\nEpoch 22 | train loss 0.0392 | val MAE(MW) 135.62 | val MSE(scaled) 0.0277\nEpoch 23 | train loss 0.0386 | val MAE(MW) 135.74 | val MSE(scaled) 0.0279\nEpoch 24 | train loss 0.0379 | val MAE(MW) 135.69 | val MSE(scaled) 0.0279\nEpoch 25 | train loss 0.0375 | val MAE(MW) 134.37 | val MSE(scaled) 0.0275\nEarly stopping.\n[I 2025-09-24 20:36:03,793] Trial 7 finished with value: 132.35667419433594 and parameters: {'d_model': 192, 'nhead_192': 6, 'layers': 3, 'd_ff_mult': 3, 'dropout': 0.15282163486118594, 'batch': 160, 'lr': 0.00025369652703685836, 'weight_decay': 2.8472762574471476e-05}. Best is trial 6 with value: 128.9370880126953.\n[W 2025-09-24 20:36:03,866] The parameter 'nhead_224' in trial#8 is sampled independently instead of being sampled by multivariate TPE sampler. (optimization performance may be degraded). You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler`, if this independent sampling is intended behavior.\nEpoch 01 | train loss 0.0919 | val MAE(MW) 162.36 | val MSE(scaled) 0.0360\nEpoch 02 | train loss 0.0557 | val MAE(MW) 186.89 | val MSE(scaled) 0.0460\nEpoch 03 | train loss 0.0533 | val MAE(MW) 161.99 | val MSE(scaled) 0.0359\nEpoch 04 | train loss 0.0522 | val MAE(MW) 163.24 | val MSE(scaled) 0.0376\nEpoch 05 | train loss 0.0511 | val MAE(MW) 161.97 | val MSE(scaled) 0.0382\nEpoch 06 | train loss 0.0505 | val MAE(MW) 146.71 | val MSE(scaled) 0.0330\nEpoch 07 | train loss 0.0502 | val MAE(MW) 142.08 | val MSE(scaled) 0.0311\nEpoch 08 | train loss 0.0494 | val MAE(MW) 138.39 | val MSE(scaled) 0.0294\nEpoch 09 | train loss 0.0491 | val MAE(MW) 138.02 | val MSE(scaled) 0.0291\nEpoch 10 | train loss 0.0486 | val MAE(MW) 138.95 | val MSE(scaled) 0.0288\nEpoch 11 | train loss 0.0485 | val MAE(MW) 141.68 | val MSE(scaled) 0.0300\nEpoch 12 | train loss 0.0479 | val MAE(MW) 135.96 | val MSE(scaled) 0.0288\nEpoch 13 | train loss 0.0476 | val MAE(MW) 142.21 | val MSE(scaled) 0.0295\nEpoch 14 | train loss 0.0476 | val MAE(MW) 140.24 | val MSE(scaled) 0.0298\nEpoch 15 | train loss 0.0467 | val MAE(MW) 136.85 | val MSE(scaled) 0.0279\nEpoch 16 | train loss 0.0460 | val MAE(MW) 135.57 | val MSE(scaled) 0.0270\nEpoch 17 | train loss 0.0437 | val MAE(MW) 136.81 | val MSE(scaled) 0.0276\nEpoch 18 | train loss 0.0418 | val MAE(MW) 135.85 | val MSE(scaled) 0.0278\nEpoch 19 | train loss 0.0411 | val MAE(MW) 138.65 | val MSE(scaled) 0.0282\nEpoch 20 | train loss 0.0381 | val MAE(MW) 134.41 | val MSE(scaled) 0.0271\nEpoch 21 | train loss 0.0379 | val MAE(MW) 134.37 | val MSE(scaled) 0.0270\nEpoch 22 | train loss 0.0369 | val MAE(MW) 135.66 | val MSE(scaled) 0.0274\nEpoch 23 | train loss 0.0365 | val MAE(MW) 136.10 | val MSE(scaled) 0.0277\nEpoch 24 | train loss 0.0350 | val MAE(MW) 136.75 | val MSE(scaled) 0.0279\nEpoch 25 | train loss 0.0324 | val MAE(MW) 134.81 | val MSE(scaled) 0.0274\nEpoch 26 | train loss 0.0317 | val MAE(MW) 135.07 | val MSE(scaled) 0.0272\nEpoch 27 | train loss 0.0314 | val MAE(MW) 134.58 | val MSE(scaled) 0.0271\nEpoch 28 | train loss 0.0307 | val MAE(MW) 134.58 | val MSE(scaled) 0.0272\nEpoch 29 | train loss 0.0306 | val MAE(MW) 133.77 | val MSE(scaled) 0.0268\nEpoch 30 | train loss 0.0303 | val MAE(MW) 135.53 | val MSE(scaled) 0.0276\nEpoch 31 | train loss 0.0305 | val MAE(MW) 135.80 | val MSE(scaled) 0.0275\nEpoch 32 | train loss 0.0302 | val MAE(MW) 134.28 | val MSE(scaled) 0.0271\nEpoch 33 | train loss 0.0301 | val MAE(MW) 134.67 | val MSE(scaled) 0.0272\nEpoch 34 | train loss 0.0301 | val MAE(MW) 134.31 | val MSE(scaled) 0.0271\nEpoch 35 | train loss 0.0299 | val MAE(MW) 135.11 | val MSE(scaled) 0.0273\nEpoch 36 | train loss 0.0300 | val MAE(MW) 134.97 | val MSE(scaled) 0.0272\nEpoch 37 | train loss 0.0299 | val MAE(MW) 134.50 | val MSE(scaled) 0.0271\nEarly stopping.\n[I 2025-09-24 20:42:20,153] Trial 8 finished with value: 133.76670837402344 and parameters: {'d_model': 224, 'nhead_224': 4, 'layers': 4, 'd_ff_mult': 3, 'dropout': 0.17052690909700813, 'batch': 128, 'lr': 0.00018729553787637097, 'weight_decay': 3.592008253137522e-05}. Best is trial 6 with value: 128.9370880126953.\n[W 2025-09-24 20:42:20,210] The parameter 'nhead_224' in trial#9 is sampled independently instead of being sampled by multivariate TPE sampler. (optimization performance may be degraded). You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler`, if this independent sampling is intended behavior.\nEpoch 01 | train loss 0.0898 | val MAE(MW) 158.60 | val MSE(scaled) 0.0352\nEpoch 02 | train loss 0.0550 | val MAE(MW) 165.65 | val MSE(scaled) 0.0372\nEpoch 03 | train loss 0.0527 | val MAE(MW) 145.59 | val MSE(scaled) 0.0296\nEpoch 04 | train loss 0.0514 | val MAE(MW) 145.54 | val MSE(scaled) 0.0303\nEpoch 05 | train loss 0.0504 | val MAE(MW) 151.62 | val MSE(scaled) 0.0322\nEpoch 06 | train loss 0.0497 | val MAE(MW) 139.71 | val MSE(scaled) 0.0289\nEpoch 07 | train loss 0.0494 | val MAE(MW) 139.49 | val MSE(scaled) 0.0296\nEpoch 08 | train loss 0.0487 | val MAE(MW) 135.50 | val MSE(scaled) 0.0279\nEpoch 09 | train loss 0.0482 | val MAE(MW) 142.70 | val MSE(scaled) 0.0298\nEpoch 10 | train loss 0.0477 | val MAE(MW) 137.98 | val MSE(scaled) 0.0277\nEpoch 11 | train loss 0.0476 | val MAE(MW) 141.92 | val MSE(scaled) 0.0295\nEpoch 12 | train loss 0.0457 | val MAE(MW) 135.50 | val MSE(scaled) 0.0282\nEpoch 13 | train loss 0.0451 | val MAE(MW) 141.57 | val MSE(scaled) 0.0290\nEpoch 14 | train loss 0.0453 | val MAE(MW) 137.30 | val MSE(scaled) 0.0288\nEpoch 15 | train loss 0.0430 | val MAE(MW) 139.48 | val MSE(scaled) 0.0290\nEpoch 16 | train loss 0.0407 | val MAE(MW) 137.90 | val MSE(scaled) 0.0284\nEpoch 17 | train loss 0.0394 | val MAE(MW) 137.31 | val MSE(scaled) 0.0282\nEpoch 18 | train loss 0.0383 | val MAE(MW) 137.05 | val MSE(scaled) 0.0281\nEpoch 19 | train loss 0.0380 | val MAE(MW) 136.67 | val MSE(scaled) 0.0280\nEpoch 20 | train loss 0.0376 | val MAE(MW) 137.88 | val MSE(scaled) 0.0282\nEarly stopping.\n[I 2025-09-24 20:45:36,858] Trial 9 finished with value: 135.49758911132812 and parameters: {'d_model': 224, 'nhead_224': 4, 'layers': 4, 'd_ff_mult': 2, 'dropout': 0.1516128513878765, 'batch': 128, 'lr': 0.00020042967724461113, 'weight_decay': 0.0003399123152253358}. Best is trial 6 with value: 128.9370880126953.\n[W 2025-09-24 20:45:36,918] The parameter 'nhead_160' in trial#10 is sampled independently instead of being sampled by multivariate TPE sampler. (optimization performance may be degraded). You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler`, if this independent sampling is intended behavior.\nEpoch 01 | train loss 0.0902 | val MAE(MW) 160.27 | val MSE(scaled) 0.0375\nEpoch 02 | train loss 0.0546 | val MAE(MW) 176.85 | val MSE(scaled) 0.0404\nEpoch 03 | train loss 0.0519 | val MAE(MW) 151.13 | val MSE(scaled) 0.0330\nEpoch 04 | train loss 0.0506 | val MAE(MW) 155.84 | val MSE(scaled) 0.0344\nEpoch 05 | train loss 0.0497 | val MAE(MW) 154.00 | val MSE(scaled) 0.0354\nEpoch 06 | train loss 0.0493 | val MAE(MW) 147.53 | val MSE(scaled) 0.0326\nEpoch 07 | train loss 0.0490 | val MAE(MW) 149.05 | val MSE(scaled) 0.0331\nEpoch 08 | train loss 0.0483 | val MAE(MW) 142.11 | val MSE(scaled) 0.0304\nEpoch 09 | train loss 0.0479 | val MAE(MW) 139.90 | val MSE(scaled) 0.0300\nEpoch 10 | train loss 0.0474 | val MAE(MW) 141.27 | val MSE(scaled) 0.0292\nEpoch 11 | train loss 0.0470 | val MAE(MW) 141.52 | val MSE(scaled) 0.0310\nEpoch 12 | train loss 0.0462 | val MAE(MW) 144.46 | val MSE(scaled) 0.0314\nEpoch 13 | train loss 0.0441 | val MAE(MW) 144.65 | val MSE(scaled) 0.0314\nEpoch 14 | train loss 0.0428 | val MAE(MW) 140.43 | val MSE(scaled) 0.0306\nEpoch 15 | train loss 0.0421 | val MAE(MW) 137.74 | val MSE(scaled) 0.0291\nEpoch 16 | train loss 0.0411 | val MAE(MW) 135.97 | val MSE(scaled) 0.0286\nEpoch 17 | train loss 0.0407 | val MAE(MW) 136.54 | val MSE(scaled) 0.0285\nEpoch 18 | train loss 0.0392 | val MAE(MW) 140.78 | val MSE(scaled) 0.0308\nEpoch 19 | train loss 0.0396 | val MAE(MW) 139.69 | val MSE(scaled) 0.0298\nEpoch 20 | train loss 0.0367 | val MAE(MW) 136.84 | val MSE(scaled) 0.0287\nEpoch 21 | train loss 0.0361 | val MAE(MW) 140.31 | val MSE(scaled) 0.0299\nEpoch 22 | train loss 0.0371 | val MAE(MW) 137.10 | val MSE(scaled) 0.0289\nEpoch 23 | train loss 0.0357 | val MAE(MW) 138.28 | val MSE(scaled) 0.0293\nEpoch 24 | train loss 0.0353 | val MAE(MW) 137.67 | val MSE(scaled) 0.0290\nEarly stopping.\n[I 2025-09-24 20:49:41,789] Trial 10 finished with value: 135.96963500976562 and parameters: {'d_model': 160, 'nhead_160': 5, 'layers': 4, 'd_ff_mult': 4, 'dropout': 0.11103564133543821, 'batch': 128, 'lr': 0.0002142439746343746, 'weight_decay': 9.278334525716651e-05}. Best is trial 6 with value: 128.9370880126953.\n[W 2025-09-24 20:49:41,852] The parameter 'nhead_128' in trial#11 is sampled independently instead of being sampled by multivariate TPE sampler. (optimization performance may be degraded). You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler`, if this independent sampling is intended behavior.\nEpoch 01 | train loss 0.0921 | val MAE(MW) 151.72 | val MSE(scaled) 0.0334\nEpoch 02 | train loss 0.0557 | val MAE(MW) 167.50 | val MSE(scaled) 0.0381\nEpoch 03 | train loss 0.0531 | val MAE(MW) 144.79 | val MSE(scaled) 0.0293\nEpoch 04 | train loss 0.0518 | val MAE(MW) 149.01 | val MSE(scaled) 0.0306\nEpoch 05 | train loss 0.0508 | val MAE(MW) 149.20 | val MSE(scaled) 0.0321\nEpoch 06 | train loss 0.0500 | val MAE(MW) 131.56 | val MSE(scaled) 0.0267\nEpoch 07 | train loss 0.0497 | val MAE(MW) 135.14 | val MSE(scaled) 0.0278\nEpoch 08 | train loss 0.0490 | val MAE(MW) 138.49 | val MSE(scaled) 0.0296\nEpoch 09 | train loss 0.0484 | val MAE(MW) 132.18 | val MSE(scaled) 0.0267\nEpoch 10 | train loss 0.0472 | val MAE(MW) 143.26 | val MSE(scaled) 0.0304\nEpoch 11 | train loss 0.0468 | val MAE(MW) 132.02 | val MSE(scaled) 0.0273\nEpoch 12 | train loss 0.0463 | val MAE(MW) 134.50 | val MSE(scaled) 0.0276\nEpoch 13 | train loss 0.0448 | val MAE(MW) 141.02 | val MSE(scaled) 0.0296\nEpoch 14 | train loss 0.0434 | val MAE(MW) 138.93 | val MSE(scaled) 0.0293\nEarly stopping.\n[I 2025-09-24 20:52:06,209] Trial 11 finished with value: 131.56398010253906 and parameters: {'d_model': 128, 'nhead_128': 4, 'layers': 4, 'd_ff_mult': 4, 'dropout': 0.175019612624307, 'batch': 128, 'lr': 0.0002950625282898273, 'weight_decay': 0.0008375236985227232}. Best is trial 6 with value: 128.9370880126953.\n[W 2025-09-24 20:52:06,268] The parameter 'nhead_160' in trial#12 is sampled independently instead of being sampled by multivariate TPE sampler. (optimization performance may be degraded). You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler`, if this independent sampling is intended behavior.\nEpoch 01 | train loss 0.0903 | val MAE(MW) 147.96 | val MSE(scaled) 0.0347\nEpoch 02 | train loss 0.0552 | val MAE(MW) 160.10 | val MSE(scaled) 0.0348\nEpoch 03 | train loss 0.0531 | val MAE(MW) 138.23 | val MSE(scaled) 0.0275\nEpoch 04 | train loss 0.0517 | val MAE(MW) 151.24 | val MSE(scaled) 0.0330\nEpoch 05 | train loss 0.0507 | val MAE(MW) 149.93 | val MSE(scaled) 0.0336\nEpoch 06 | train loss 0.0501 | val MAE(MW) 139.70 | val MSE(scaled) 0.0293\nEpoch 07 | train loss 0.0488 | val MAE(MW) 140.35 | val MSE(scaled) 0.0296\nEpoch 08 | train loss 0.0483 | val MAE(MW) 134.57 | val MSE(scaled) 0.0281\nEpoch 09 | train loss 0.0480 | val MAE(MW) 133.41 | val MSE(scaled) 0.0267\nEpoch 10 | train loss 0.0478 | val MAE(MW) 148.39 | val MSE(scaled) 0.0338\nEpoch 11 | train loss 0.0474 | val MAE(MW) 141.96 | val MSE(scaled) 0.0317\nEpoch 12 | train loss 0.0471 | val MAE(MW) 135.83 | val MSE(scaled) 0.0282\nEpoch 13 | train loss 0.0469 | val MAE(MW) 136.80 | val MSE(scaled) 0.0289\nEpoch 14 | train loss 0.0464 | val MAE(MW) 138.28 | val MSE(scaled) 0.0297\nEpoch 15 | train loss 0.0458 | val MAE(MW) 142.98 | val MSE(scaled) 0.0307\nEpoch 16 | train loss 0.0424 | val MAE(MW) 133.95 | val MSE(scaled) 0.0274\nEpoch 17 | train loss 0.0416 | val MAE(MW) 136.21 | val MSE(scaled) 0.0281\nEarly stopping.\n[I 2025-09-24 20:54:51,669] Trial 12 finished with value: 133.4055938720703 and parameters: {'d_model': 160, 'nhead_160': 4, 'layers': 4, 'd_ff_mult': 4, 'dropout': 0.1724052434218871, 'batch': 128, 'lr': 0.00033997780265254316, 'weight_decay': 0.00013797046495953665}. Best is trial 6 with value: 128.9370880126953.\n[W 2025-09-24 20:54:51,724] The parameter 'nhead_96' in trial#13 is sampled independently instead of being sampled by multivariate TPE sampler. (optimization performance may be degraded). You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler`, if this independent sampling is intended behavior.\nEpoch 01 | train loss 0.0851 | val MAE(MW) 170.93 | val MSE(scaled) 0.0388\nEpoch 02 | train loss 0.0542 | val MAE(MW) 167.37 | val MSE(scaled) 0.0376\nEpoch 03 | train loss 0.0518 | val MAE(MW) 153.53 | val MSE(scaled) 0.0334\nEpoch 04 | train loss 0.0506 | val MAE(MW) 160.51 | val MSE(scaled) 0.0363\nEpoch 05 | train loss 0.0497 | val MAE(MW) 160.14 | val MSE(scaled) 0.0356\nEpoch 06 | train loss 0.0491 | val MAE(MW) 152.56 | val MSE(scaled) 0.0354\nEpoch 07 | train loss 0.0486 | val MAE(MW) 160.61 | val MSE(scaled) 0.0383\nEpoch 08 | train loss 0.0480 | val MAE(MW) 168.91 | val MSE(scaled) 0.0431\nEpoch 09 | train loss 0.0475 | val MAE(MW) 144.59 | val MSE(scaled) 0.0320\nEpoch 10 | train loss 0.0471 | val MAE(MW) 152.19 | val MSE(scaled) 0.0364\nEpoch 11 | train loss 0.0469 | val MAE(MW) 152.96 | val MSE(scaled) 0.0370\nEpoch 12 | train loss 0.0464 | val MAE(MW) 146.41 | val MSE(scaled) 0.0338\nEpoch 13 | train loss 0.0450 | val MAE(MW) 144.41 | val MSE(scaled) 0.0332\nEpoch 14 | train loss 0.0447 | val MAE(MW) 145.40 | val MSE(scaled) 0.0339\nEpoch 15 | train loss 0.0452 | val MAE(MW) 142.24 | val MSE(scaled) 0.0322\nEpoch 16 | train loss 0.0431 | val MAE(MW) 143.76 | val MSE(scaled) 0.0331\nEpoch 17 | train loss 0.0432 | val MAE(MW) 140.91 | val MSE(scaled) 0.0321\nEpoch 18 | train loss 0.0416 | val MAE(MW) 136.81 | val MSE(scaled) 0.0304\nEpoch 19 | train loss 0.0405 | val MAE(MW) 147.44 | val MSE(scaled) 0.0341\nEpoch 20 | train loss 0.0405 | val MAE(MW) 138.74 | val MSE(scaled) 0.0303\nEpoch 21 | train loss 0.0407 | val MAE(MW) 133.79 | val MSE(scaled) 0.0286\nEpoch 22 | train loss 0.0396 | val MAE(MW) 140.83 | val MSE(scaled) 0.0308\nEpoch 23 | train loss 0.0390 | val MAE(MW) 143.93 | val MSE(scaled) 0.0329\nEpoch 24 | train loss 0.0390 | val MAE(MW) 138.72 | val MSE(scaled) 0.0307\nEpoch 25 | train loss 0.0375 | val MAE(MW) 131.56 | val MSE(scaled) 0.0284\nEpoch 26 | train loss 0.0373 | val MAE(MW) 133.82 | val MSE(scaled) 0.0288\nEpoch 27 | train loss 0.0367 | val MAE(MW) 127.48 | val MSE(scaled) 0.0263\nEpoch 28 | train loss 0.0365 | val MAE(MW) 127.94 | val MSE(scaled) 0.0268\nEpoch 29 | train loss 0.0362 | val MAE(MW) 132.69 | val MSE(scaled) 0.0284\nEpoch 30 | train loss 0.0363 | val MAE(MW) 134.15 | val MSE(scaled) 0.0291\nEpoch 31 | train loss 0.0346 | val MAE(MW) 129.37 | val MSE(scaled) 0.0271\nEpoch 32 | train loss 0.0347 | val MAE(MW) 131.46 | val MSE(scaled) 0.0281\nEpoch 33 | train loss 0.0350 | val MAE(MW) 130.50 | val MSE(scaled) 0.0277\nEpoch 34 | train loss 0.0341 | val MAE(MW) 129.63 | val MSE(scaled) 0.0275\nEpoch 35 | train loss 0.0336 | val MAE(MW) 131.82 | val MSE(scaled) 0.0283\nEarly stopping.\n[I 2025-09-24 20:58:28,372] Trial 13 finished with value: 127.47830200195312 and parameters: {'d_model': 96, 'nhead_96': 4, 'layers': 2, 'd_ff_mult': 4, 'dropout': 0.12566444220531062, 'batch': 128, 'lr': 0.0002976966516891762, 'weight_decay': 0.0002489591056441535}. Best is trial 13 with value: 127.47830200195312.\n[W 2025-09-24 20:58:28,429] The parameter 'nhead_96' in trial#14 is sampled independently instead of being sampled by multivariate TPE sampler. (optimization performance may be degraded). You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler`, if this independent sampling is intended behavior.\nEpoch 01 | train loss 0.0870 | val MAE(MW) 160.82 | val MSE(scaled) 0.0358\nEpoch 02 | train loss 0.0542 | val MAE(MW) 159.78 | val MSE(scaled) 0.0355\nEpoch 03 | train loss 0.0518 | val MAE(MW) 150.36 | val MSE(scaled) 0.0324\nEpoch 04 | train loss 0.0505 | val MAE(MW) 157.73 | val MSE(scaled) 0.0355\nEpoch 05 | train loss 0.0496 | val MAE(MW) 155.06 | val MSE(scaled) 0.0347\nEpoch 06 | train loss 0.0490 | val MAE(MW) 155.34 | val MSE(scaled) 0.0366\nEpoch 07 | train loss 0.0480 | val MAE(MW) 143.63 | val MSE(scaled) 0.0324\nEpoch 08 | train loss 0.0477 | val MAE(MW) 143.94 | val MSE(scaled) 0.0329\nEpoch 09 | train loss 0.0475 | val MAE(MW) 141.96 | val MSE(scaled) 0.0323\nEpoch 10 | train loss 0.0471 | val MAE(MW) 158.52 | val MSE(scaled) 0.0379\nEpoch 11 | train loss 0.0467 | val MAE(MW) 141.34 | val MSE(scaled) 0.0312\nEpoch 12 | train loss 0.0461 | val MAE(MW) 141.80 | val MSE(scaled) 0.0322\nEpoch 13 | train loss 0.0455 | val MAE(MW) 149.82 | val MSE(scaled) 0.0349\nEpoch 14 | train loss 0.0450 | val MAE(MW) 150.89 | val MSE(scaled) 0.0349\nEpoch 15 | train loss 0.0427 | val MAE(MW) 144.09 | val MSE(scaled) 0.0330\nEpoch 16 | train loss 0.0420 | val MAE(MW) 146.41 | val MSE(scaled) 0.0333\nEpoch 17 | train loss 0.0415 | val MAE(MW) 143.39 | val MSE(scaled) 0.0318\nEpoch 18 | train loss 0.0404 | val MAE(MW) 145.57 | val MSE(scaled) 0.0333\nEpoch 19 | train loss 0.0401 | val MAE(MW) 143.16 | val MSE(scaled) 0.0326\nEarly stopping.\n[I 2025-09-24 21:00:27,166] Trial 14 finished with value: 141.33998107910156 and parameters: {'d_model': 96, 'nhead_96': 4, 'layers': 2, 'd_ff_mult': 4, 'dropout': 0.12092819058449675, 'batch': 128, 'lr': 0.00029064251780455295, 'weight_decay': 0.000203417839816972}. Best is trial 13 with value: 127.47830200195312.\n[W 2025-09-24 21:00:27,223] The parameter 'nhead_224' in trial#15 is sampled independently instead of being sampled by multivariate TPE sampler. (optimization performance may be degraded). You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler`, if this independent sampling is intended behavior.\nEpoch 01 | train loss 0.0763 | val MAE(MW) 150.40 | val MSE(scaled) 0.0313\nEpoch 02 | train loss 0.0535 | val MAE(MW) 167.44 | val MSE(scaled) 0.0383\nEpoch 03 | train loss 0.0517 | val MAE(MW) 143.80 | val MSE(scaled) 0.0311\nEpoch 04 | train loss 0.0507 | val MAE(MW) 139.63 | val MSE(scaled) 0.0290\nEpoch 05 | train loss 0.0497 | val MAE(MW) 157.20 | val MSE(scaled) 0.0370\nEpoch 06 | train loss 0.0492 | val MAE(MW) 138.33 | val MSE(scaled) 0.0291\nEpoch 07 | train loss 0.0489 | val MAE(MW) 143.59 | val MSE(scaled) 0.0322\nEpoch 08 | train loss 0.0482 | val MAE(MW) 137.72 | val MSE(scaled) 0.0300\nEpoch 09 | train loss 0.0480 | val MAE(MW) 144.21 | val MSE(scaled) 0.0329\nEpoch 10 | train loss 0.0475 | val MAE(MW) 135.60 | val MSE(scaled) 0.0288\nEpoch 11 | train loss 0.0476 | val MAE(MW) 144.05 | val MSE(scaled) 0.0341\nEpoch 12 | train loss 0.0473 | val MAE(MW) 141.10 | val MSE(scaled) 0.0321\nEpoch 13 | train loss 0.0474 | val MAE(MW) 144.78 | val MSE(scaled) 0.0310\nEpoch 14 | train loss 0.0456 | val MAE(MW) 140.08 | val MSE(scaled) 0.0313\nEpoch 15 | train loss 0.0447 | val MAE(MW) 139.94 | val MSE(scaled) 0.0299\nEpoch 16 | train loss 0.0439 | val MAE(MW) 139.67 | val MSE(scaled) 0.0300\nEpoch 17 | train loss 0.0423 | val MAE(MW) 141.24 | val MSE(scaled) 0.0301\nEpoch 18 | train loss 0.0398 | val MAE(MW) 141.72 | val MSE(scaled) 0.0306\nEarly stopping.\n[I 2025-09-24 21:02:27,968] Trial 15 finished with value: 135.60182189941406 and parameters: {'d_model': 224, 'nhead_224': 8, 'layers': 2, 'd_ff_mult': 2, 'dropout': 0.1688833740840417, 'batch': 128, 'lr': 0.0003100861339653652, 'weight_decay': 0.00012194081932827042}. Best is trial 13 with value: 127.47830200195312.\n[W 2025-09-24 21:02:28,025] The parameter 'nhead_224' in trial#16 is sampled independently instead of being sampled by multivariate TPE sampler. (optimization performance may be degraded). You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler`, if this independent sampling is intended behavior.\nEpoch 01 | train loss 0.0784 | val MAE(MW) 148.24 | val MSE(scaled) 0.0331\nEpoch 02 | train loss 0.0528 | val MAE(MW) 172.29 | val MSE(scaled) 0.0373\nEpoch 03 | train loss 0.0509 | val MAE(MW) 144.66 | val MSE(scaled) 0.0307\nEpoch 04 | train loss 0.0499 | val MAE(MW) 144.93 | val MSE(scaled) 0.0307\nEpoch 05 | train loss 0.0491 | val MAE(MW) 143.16 | val MSE(scaled) 0.0312\nEpoch 06 | train loss 0.0487 | val MAE(MW) 137.52 | val MSE(scaled) 0.0288\nEpoch 07 | train loss 0.0483 | val MAE(MW) 133.48 | val MSE(scaled) 0.0274\nEpoch 08 | train loss 0.0476 | val MAE(MW) 143.71 | val MSE(scaled) 0.0305\nEpoch 09 | train loss 0.0473 | val MAE(MW) 134.85 | val MSE(scaled) 0.0284\nEpoch 10 | train loss 0.0468 | val MAE(MW) 132.52 | val MSE(scaled) 0.0268\nEpoch 11 | train loss 0.0466 | val MAE(MW) 141.48 | val MSE(scaled) 0.0304\nEpoch 12 | train loss 0.0463 | val MAE(MW) 140.26 | val MSE(scaled) 0.0297\nEpoch 13 | train loss 0.0462 | val MAE(MW) 135.52 | val MSE(scaled) 0.0276\nEpoch 14 | train loss 0.0434 | val MAE(MW) 137.15 | val MSE(scaled) 0.0286\nEpoch 15 | train loss 0.0430 | val MAE(MW) 140.07 | val MSE(scaled) 0.0288\nEpoch 16 | train loss 0.0431 | val MAE(MW) 138.33 | val MSE(scaled) 0.0288\nEpoch 17 | train loss 0.0432 | val MAE(MW) 142.60 | val MSE(scaled) 0.0300\nEpoch 18 | train loss 0.0406 | val MAE(MW) 141.67 | val MSE(scaled) 0.0295\nEarly stopping.\n[I 2025-09-24 21:04:09,061] Trial 16 finished with value: 132.5224151611328 and parameters: {'d_model': 224, 'nhead_224': 14, 'layers': 2, 'd_ff_mult': 2, 'dropout': 0.11866880489337342, 'batch': 128, 'lr': 0.0002056769438773759, 'weight_decay': 8.088648192862286e-05}. Best is trial 13 with value: 127.47830200195312.\n[W 2025-09-24 21:04:09,112] The parameter 'nhead_224' in trial#17 is sampled independently instead of being sampled by multivariate TPE sampler. (optimization performance may be degraded). You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler`, if this independent sampling is intended behavior.\nEpoch 01 | train loss 0.0737 | val MAE(MW) 146.74 | val MSE(scaled) 0.0312\nEpoch 02 | train loss 0.0521 | val MAE(MW) 159.16 | val MSE(scaled) 0.0347\nEpoch 03 | train loss 0.0505 | val MAE(MW) 139.01 | val MSE(scaled) 0.0276\nEpoch 04 | train loss 0.0494 | val MAE(MW) 130.71 | val MSE(scaled) 0.0269\nEpoch 05 | train loss 0.0485 | val MAE(MW) 140.50 | val MSE(scaled) 0.0300\nEpoch 06 | train loss 0.0479 | val MAE(MW) 134.37 | val MSE(scaled) 0.0268\nEpoch 07 | train loss 0.0478 | val MAE(MW) 133.44 | val MSE(scaled) 0.0269\nEpoch 08 | train loss 0.0461 | val MAE(MW) 135.15 | val MSE(scaled) 0.0275\nEpoch 09 | train loss 0.0456 | val MAE(MW) 133.67 | val MSE(scaled) 0.0264\nEpoch 10 | train loss 0.0452 | val MAE(MW) 132.64 | val MSE(scaled) 0.0276\nEpoch 11 | train loss 0.0418 | val MAE(MW) 131.39 | val MSE(scaled) 0.0265\nEpoch 12 | train loss 0.0410 | val MAE(MW) 131.25 | val MSE(scaled) 0.0263\nEarly stopping.\n[I 2025-09-24 21:05:15,811] Trial 17 finished with value: 130.70672607421875 and parameters: {'d_model': 224, 'nhead_224': 7, 'layers': 2, 'd_ff_mult': 3, 'dropout': 0.12730114953451566, 'batch': 128, 'lr': 0.0003745667854322322, 'weight_decay': 0.0003101151982156363}. Best is trial 13 with value: 127.47830200195312.\n[W 2025-09-24 21:05:15,864] The parameter 'nhead_192' in trial#18 is sampled independently instead of being sampled by multivariate TPE sampler. (optimization performance may be degraded). You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler`, if this independent sampling is intended behavior.\nEpoch 01 | train loss 0.0867 | val MAE(MW) 151.36 | val MSE(scaled) 0.0329\nEpoch 02 | train loss 0.0548 | val MAE(MW) 144.64 | val MSE(scaled) 0.0299\nEpoch 03 | train loss 0.0523 | val MAE(MW) 143.47 | val MSE(scaled) 0.0301\nEpoch 04 | train loss 0.0510 | val MAE(MW) 146.26 | val MSE(scaled) 0.0312\nEpoch 05 | train loss 0.0501 | val MAE(MW) 164.87 | val MSE(scaled) 0.0377\nEpoch 06 | train loss 0.0494 | val MAE(MW) 146.93 | val MSE(scaled) 0.0319\nEpoch 07 | train loss 0.0483 | val MAE(MW) 143.37 | val MSE(scaled) 0.0308\nEpoch 08 | train loss 0.0479 | val MAE(MW) 148.67 | val MSE(scaled) 0.0333\nEpoch 09 | train loss 0.0477 | val MAE(MW) 144.39 | val MSE(scaled) 0.0322\nEpoch 10 | train loss 0.0473 | val MAE(MW) 149.11 | val MSE(scaled) 0.0344\nEpoch 11 | train loss 0.0465 | val MAE(MW) 142.46 | val MSE(scaled) 0.0311\nEpoch 12 | train loss 0.0462 | val MAE(MW) 140.03 | val MSE(scaled) 0.0301\nEpoch 13 | train loss 0.0458 | val MAE(MW) 143.74 | val MSE(scaled) 0.0315\nEpoch 14 | train loss 0.0454 | val MAE(MW) 142.95 | val MSE(scaled) 0.0309\nEpoch 15 | train loss 0.0445 | val MAE(MW) 143.12 | val MSE(scaled) 0.0313\nEpoch 16 | train loss 0.0424 | val MAE(MW) 143.46 | val MSE(scaled) 0.0314\nEpoch 17 | train loss 0.0421 | val MAE(MW) 146.85 | val MSE(scaled) 0.0333\nEpoch 18 | train loss 0.0414 | val MAE(MW) 142.43 | val MSE(scaled) 0.0312\nEpoch 19 | train loss 0.0408 | val MAE(MW) 140.69 | val MSE(scaled) 0.0305\nEpoch 20 | train loss 0.0405 | val MAE(MW) 142.16 | val MSE(scaled) 0.0312\nEarly stopping.\n[I 2025-09-24 21:06:47,418] Trial 18 finished with value: 140.02691650390625 and parameters: {'d_model': 192, 'nhead_192': 4, 'layers': 2, 'd_ff_mult': 4, 'dropout': 0.14341333380453508, 'batch': 160, 'lr': 0.00020370321864342735, 'weight_decay': 0.00021590002998213636}. Best is trial 13 with value: 127.47830200195312.\n[W 2025-09-24 21:06:47,475] The parameter 'nhead_96' in trial#19 is sampled independently instead of being sampled by multivariate TPE sampler. (optimization performance may be degraded). You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler`, if this independent sampling is intended behavior.\nEpoch 01 | train loss 0.1039 | val MAE(MW) 167.22 | val MSE(scaled) 0.0416\nEpoch 02 | train loss 0.0580 | val MAE(MW) 144.82 | val MSE(scaled) 0.0317\nEpoch 03 | train loss 0.0541 | val MAE(MW) 148.85 | val MSE(scaled) 0.0316\nEpoch 04 | train loss 0.0524 | val MAE(MW) 148.92 | val MSE(scaled) 0.0313\nEpoch 05 | train loss 0.0515 | val MAE(MW) 155.78 | val MSE(scaled) 0.0335\nEpoch 06 | train loss 0.0504 | val MAE(MW) 145.24 | val MSE(scaled) 0.0308\nEpoch 07 | train loss 0.0500 | val MAE(MW) 148.04 | val MSE(scaled) 0.0318\nEpoch 08 | train loss 0.0497 | val MAE(MW) 145.23 | val MSE(scaled) 0.0308\nEpoch 09 | train loss 0.0491 | val MAE(MW) 142.38 | val MSE(scaled) 0.0305\nEpoch 10 | train loss 0.0490 | val MAE(MW) 145.42 | val MSE(scaled) 0.0313\nEpoch 11 | train loss 0.0488 | val MAE(MW) 140.85 | val MSE(scaled) 0.0298\nEpoch 12 | train loss 0.0485 | val MAE(MW) 138.40 | val MSE(scaled) 0.0294\nEpoch 13 | train loss 0.0484 | val MAE(MW) 145.00 | val MSE(scaled) 0.0314\nEpoch 14 | train loss 0.0482 | val MAE(MW) 142.49 | val MSE(scaled) 0.0306\nEpoch 15 | train loss 0.0480 | val MAE(MW) 138.55 | val MSE(scaled) 0.0293\nEpoch 16 | train loss 0.0477 | val MAE(MW) 139.18 | val MSE(scaled) 0.0295\nEpoch 17 | train loss 0.0475 | val MAE(MW) 145.01 | val MSE(scaled) 0.0311\nEpoch 18 | train loss 0.0474 | val MAE(MW) 140.52 | val MSE(scaled) 0.0297\nEpoch 19 | train loss 0.0471 | val MAE(MW) 137.90 | val MSE(scaled) 0.0291\nEpoch 20 | train loss 0.0470 | val MAE(MW) 139.91 | val MSE(scaled) 0.0298\nEpoch 21 | train loss 0.0469 | val MAE(MW) 137.82 | val MSE(scaled) 0.0291\nEpoch 22 | train loss 0.0468 | val MAE(MW) 142.67 | val MSE(scaled) 0.0306\nEpoch 23 | train loss 0.0466 | val MAE(MW) 138.10 | val MSE(scaled) 0.0292\nEpoch 24 | train loss 0.0465 | val MAE(MW) 138.45 | val MSE(scaled) 0.0294\nEpoch 25 | train loss 0.0462 | val MAE(MW) 139.97 | val MSE(scaled) 0.0298\nEpoch 26 | train loss 0.0461 | val MAE(MW) 138.12 | val MSE(scaled) 0.0292\nEpoch 27 | train loss 0.0460 | val MAE(MW) 141.15 | val MSE(scaled) 0.0300\nEpoch 28 | train loss 0.0458 | val MAE(MW) 140.01 | val MSE(scaled) 0.0298\nEpoch 29 | train loss 0.0458 | val MAE(MW) 139.38 | val MSE(scaled) 0.0296\nEarly stopping.\n[I 2025-09-24 21:10:18,047] Trial 19 finished with value: 137.81944274902344 and parameters: {'d_model': 96, 'nhead_96': 4, 'layers': 3, 'd_ff_mult': 4, 'dropout': 0.14719646532776814, 'batch': 128, 'lr': 0.00020620981950790205, 'weight_decay': 0.000699938364460188}. Best is trial 13 with value: 127.47830200195312.\n[W 2025-09-24 21:10:18,100] The parameter 'nhead_192' in trial#20 is sampled independently instead of being sampled by multivariate TPE sampler. (optimization performance may be degraded). You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler`, if this independent sampling is intended behavior.\nEpoch 01 | train loss 0.0840 | val MAE(MW) 156.45 | val MSE(scaled) 0.0364\nEpoch 02 | train loss 0.0543 | val MAE(MW) 172.32 | val MSE(scaled) 0.0391\nEpoch 03 | train loss 0.0522 | val MAE(MW) 141.46 | val MSE(scaled) 0.0290\nEpoch 04 | train loss 0.0509 | val MAE(MW) 148.62 | val MSE(scaled) 0.0316\nEpoch 05 | train loss 0.0499 | val MAE(MW) 145.75 | val MSE(scaled) 0.0307\nEpoch 06 | train loss 0.0492 | val MAE(MW) 133.82 | val MSE(scaled) 0.0270\nEpoch 07 | train loss 0.0489 | val MAE(MW) 133.98 | val MSE(scaled) 0.0273\nEpoch 08 | train loss 0.0483 | val MAE(MW) 128.86 | val MSE(scaled) 0.0255\nEpoch 09 | train loss 0.0477 | val MAE(MW) 134.40 | val MSE(scaled) 0.0282\nEpoch 10 | train loss 0.0472 | val MAE(MW) 128.75 | val MSE(scaled) 0.0255\nEpoch 11 | train loss 0.0474 | val MAE(MW) 127.81 | val MSE(scaled) 0.0252\nEpoch 12 | train loss 0.0465 | val MAE(MW) 128.12 | val MSE(scaled) 0.0250\nEpoch 13 | train loss 0.0463 | val MAE(MW) 137.73 | val MSE(scaled) 0.0288\nEpoch 14 | train loss 0.0459 | val MAE(MW) 126.09 | val MSE(scaled) 0.0244\nEpoch 15 | train loss 0.0452 | val MAE(MW) 138.96 | val MSE(scaled) 0.0299\nEpoch 16 | train loss 0.0436 | val MAE(MW) 126.33 | val MSE(scaled) 0.0254\nEpoch 17 | train loss 0.0428 | val MAE(MW) 131.40 | val MSE(scaled) 0.0261\nEpoch 18 | train loss 0.0383 | val MAE(MW) 131.77 | val MSE(scaled) 0.0266\nEpoch 19 | train loss 0.0376 | val MAE(MW) 133.76 | val MSE(scaled) 0.0274\nEpoch 20 | train loss 0.0359 | val MAE(MW) 131.58 | val MSE(scaled) 0.0269\nEpoch 21 | train loss 0.0333 | val MAE(MW) 130.36 | val MSE(scaled) 0.0265\nEpoch 22 | train loss 0.0305 | val MAE(MW) 134.12 | val MSE(scaled) 0.0280\nEarly stopping.\n[I 2025-09-24 21:13:27,422] Trial 20 finished with value: 126.08888244628906 and parameters: {'d_model': 192, 'nhead_192': 12, 'layers': 4, 'd_ff_mult': 2, 'dropout': 0.1407680866772012, 'batch': 128, 'lr': 0.0003091437635040244, 'weight_decay': 7.879380786509894e-05}. Best is trial 20 with value: 126.08888244628906.\n[W 2025-09-24 21:13:27,478] The parameter 'nhead_192' in trial#21 is sampled independently instead of being sampled by multivariate TPE sampler. (optimization performance may be degraded). You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler`, if this independent sampling is intended behavior.\nEpoch 01 | train loss 0.0858 | val MAE(MW) 167.28 | val MSE(scaled) 0.0387\nEpoch 02 | train loss 0.0537 | val MAE(MW) 171.07 | val MSE(scaled) 0.0400\nEpoch 03 | train loss 0.0518 | val MAE(MW) 149.89 | val MSE(scaled) 0.0322\nEpoch 04 | train loss 0.0505 | val MAE(MW) 157.50 | val MSE(scaled) 0.0349\nEpoch 05 | train loss 0.0495 | val MAE(MW) 157.05 | val MSE(scaled) 0.0361\nEpoch 06 | train loss 0.0489 | val MAE(MW) 146.05 | val MSE(scaled) 0.0305\nEpoch 07 | train loss 0.0486 | val MAE(MW) 148.04 | val MSE(scaled) 0.0321\nEpoch 08 | train loss 0.0479 | val MAE(MW) 150.55 | val MSE(scaled) 0.0313\nEpoch 09 | train loss 0.0475 | val MAE(MW) 145.85 | val MSE(scaled) 0.0312\nEpoch 10 | train loss 0.0471 | val MAE(MW) 139.68 | val MSE(scaled) 0.0288\nEpoch 11 | train loss 0.0465 | val MAE(MW) 136.59 | val MSE(scaled) 0.0289\nEpoch 12 | train loss 0.0463 | val MAE(MW) 135.63 | val MSE(scaled) 0.0275\nEpoch 13 | train loss 0.0459 | val MAE(MW) 134.22 | val MSE(scaled) 0.0268\nEpoch 14 | train loss 0.0453 | val MAE(MW) 141.26 | val MSE(scaled) 0.0296\nEpoch 15 | train loss 0.0457 | val MAE(MW) 145.20 | val MSE(scaled) 0.0313\nEpoch 16 | train loss 0.0439 | val MAE(MW) 125.11 | val MSE(scaled) 0.0250\nEpoch 17 | train loss 0.0443 | val MAE(MW) 136.93 | val MSE(scaled) 0.0282\nEpoch 18 | train loss 0.0417 | val MAE(MW) 138.52 | val MSE(scaled) 0.0290\nEpoch 19 | train loss 0.0416 | val MAE(MW) 143.82 | val MSE(scaled) 0.0308\nEpoch 20 | train loss 0.0377 | val MAE(MW) 139.34 | val MSE(scaled) 0.0289\nEpoch 21 | train loss 0.0375 | val MAE(MW) 135.29 | val MSE(scaled) 0.0272\nEpoch 22 | train loss 0.0367 | val MAE(MW) 141.82 | val MSE(scaled) 0.0296\nEpoch 23 | train loss 0.0322 | val MAE(MW) 141.84 | val MSE(scaled) 0.0302\nEpoch 24 | train loss 0.0298 | val MAE(MW) 141.07 | val MSE(scaled) 0.0302\nEarly stopping.\n[I 2025-09-24 21:17:04,106] Trial 21 finished with value: 125.10549926757812 and parameters: {'d_model': 192, 'nhead_192': 12, 'layers': 4, 'd_ff_mult': 2, 'dropout': 0.126650994140276, 'batch': 128, 'lr': 0.0002954546896729206, 'weight_decay': 4.6605719333012e-05}. Best is trial 21 with value: 125.10549926757812.\n[W 2025-09-24 21:17:04,166] The parameter 'nhead_192' in trial#22 is sampled independently instead of being sampled by multivariate TPE sampler. (optimization performance may be degraded). You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler`, if this independent sampling is intended behavior.\nEpoch 01 | train loss 0.0836 | val MAE(MW) 161.86 | val MSE(scaled) 0.0356\nEpoch 02 | train loss 0.0538 | val MAE(MW) 154.56 | val MSE(scaled) 0.0334\nEpoch 03 | train loss 0.0519 | val MAE(MW) 143.36 | val MSE(scaled) 0.0292\nEpoch 04 | train loss 0.0508 | val MAE(MW) 141.93 | val MSE(scaled) 0.0292\nEpoch 05 | train loss 0.0499 | val MAE(MW) 149.88 | val MSE(scaled) 0.0320\nEpoch 06 | train loss 0.0493 | val MAE(MW) 137.95 | val MSE(scaled) 0.0281\nEpoch 07 | train loss 0.0490 | val MAE(MW) 136.61 | val MSE(scaled) 0.0287\nEpoch 08 | train loss 0.0484 | val MAE(MW) 132.58 | val MSE(scaled) 0.0276\nEpoch 09 | train loss 0.0479 | val MAE(MW) 144.97 | val MSE(scaled) 0.0313\nEpoch 10 | train loss 0.0474 | val MAE(MW) 137.40 | val MSE(scaled) 0.0276\nEpoch 11 | train loss 0.0473 | val MAE(MW) 135.76 | val MSE(scaled) 0.0285\nEpoch 12 | train loss 0.0450 | val MAE(MW) 137.06 | val MSE(scaled) 0.0286\nEpoch 13 | train loss 0.0439 | val MAE(MW) 141.41 | val MSE(scaled) 0.0288\nEpoch 14 | train loss 0.0428 | val MAE(MW) 135.91 | val MSE(scaled) 0.0279\nEpoch 15 | train loss 0.0403 | val MAE(MW) 138.64 | val MSE(scaled) 0.0282\nEpoch 16 | train loss 0.0384 | val MAE(MW) 134.05 | val MSE(scaled) 0.0269\nEarly stopping.\n[I 2025-09-24 21:19:30,713] Trial 22 finished with value: 132.58395385742188 and parameters: {'d_model': 192, 'nhead_192': 12, 'layers': 4, 'd_ff_mult': 2, 'dropout': 0.13744770104425064, 'batch': 128, 'lr': 0.0003408730615309076, 'weight_decay': 4.186082561644667e-05}. Best is trial 21 with value: 125.10549926757812.\n[W 2025-09-24 21:19:30,764] The parameter 'nhead_192' in trial#23 is sampled independently instead of being sampled by multivariate TPE sampler. (optimization performance may be degraded). You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler`, if this independent sampling is intended behavior.\nEpoch 01 | train loss 0.0917 | val MAE(MW) 168.16 | val MSE(scaled) 0.0386\nEpoch 02 | train loss 0.0547 | val MAE(MW) 161.07 | val MSE(scaled) 0.0360\nEpoch 03 | train loss 0.0523 | val MAE(MW) 146.72 | val MSE(scaled) 0.0313\nEpoch 04 | train loss 0.0510 | val MAE(MW) 143.90 | val MSE(scaled) 0.0297\nEpoch 05 | train loss 0.0502 | val MAE(MW) 155.50 | val MSE(scaled) 0.0338\nEpoch 06 | train loss 0.0497 | val MAE(MW) 142.61 | val MSE(scaled) 0.0295\nEpoch 07 | train loss 0.0492 | val MAE(MW) 137.22 | val MSE(scaled) 0.0280\nEpoch 08 | train loss 0.0488 | val MAE(MW) 144.53 | val MSE(scaled) 0.0311\nEpoch 09 | train loss 0.0483 | val MAE(MW) 130.77 | val MSE(scaled) 0.0266\nEpoch 10 | train loss 0.0479 | val MAE(MW) 138.98 | val MSE(scaled) 0.0289\nEpoch 11 | train loss 0.0477 | val MAE(MW) 143.94 | val MSE(scaled) 0.0330\nEpoch 12 | train loss 0.0475 | val MAE(MW) 138.40 | val MSE(scaled) 0.0294\nEpoch 13 | train loss 0.0463 | val MAE(MW) 140.58 | val MSE(scaled) 0.0305\nEpoch 14 | train loss 0.0462 | val MAE(MW) 142.17 | val MSE(scaled) 0.0316\nEpoch 15 | train loss 0.0454 | val MAE(MW) 138.84 | val MSE(scaled) 0.0294\nEpoch 16 | train loss 0.0420 | val MAE(MW) 136.39 | val MSE(scaled) 0.0290\nEpoch 17 | train loss 0.0409 | val MAE(MW) 135.32 | val MSE(scaled) 0.0287\nEarly stopping.\n[I 2025-09-24 21:21:35,855] Trial 23 finished with value: 130.76649475097656 and parameters: {'d_model': 192, 'nhead_192': 12, 'layers': 4, 'd_ff_mult': 2, 'dropout': 0.1288402493809898, 'batch': 160, 'lr': 0.00025582880885470554, 'weight_decay': 6.771472305812162e-05}. Best is trial 21 with value: 125.10549926757812.\n\nBest trial:\n  val_MAE(MW): 125.105\n  d_model: 192\n  nhead_192: 12\n  layers: 4\n  d_ff_mult: 2\n  dropout: 0.126650994140276\n  batch: 128\n  lr: 0.0002954546896729206\n  weight_decay: 4.6605719333012e-05\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_95/1859335986.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;31m# Comment out your manual single-run block and run the tuner:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tf_tiny_sweep_v1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_95/1859335986.py\u001b[0m in \u001b[0;36mrun_study\u001b[0;34m(n_trials, study_name, seed)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mZ_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mZ_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mZ_val\u001b[0m\u001b[0;34m,\u001b[0m     \u001b[0;31m# keep Z for shape; early-stop will trigger quickly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0md_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"d_model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mnhead\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"nhead\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0mlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"layers\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0md_ff\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"d_model\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"d_ff_mult\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'nhead'"],"ename":"KeyError","evalue":"'nhead'","output_type":"error"}],"execution_count":26},{"cell_type":"code","source":"def canonicalize_best(best_params):\n    d_model = best_params[\"d_model\"]\n    # find the only key that startswith \"nhead\"\n    nhead_key = next(k for k in best_params if k.startswith(\"nhead\"))\n    nhead = best_params[nhead_key]\n\n    layers   = best_params[\"layers\"]\n    d_ff     = d_model * best_params[\"d_ff_mult\"]\n    dropout  = best_params[\"dropout\"]\n    batch    = best_params[\"batch\"]\n    lr       = best_params[\"lr\"]\n    wd       = best_params.get(\"weight_decay\", best_params.get(\"wd\", 0.0))\n\n    return dict(\n        d_model=d_model, nhead=nhead, layers=layers, d_ff=d_ff,\n        dropout=dropout, batch=batch, lr=lr, weight_decay=wd\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T21:22:19.209475Z","iopub.execute_input":"2025-09-24T21:22:19.209828Z","iopub.status.idle":"2025-09-24T21:22:19.215557Z","shell.execute_reply.started":"2025-09-24T21:22:19.209804Z","shell.execute_reply":"2025-09-24T21:22:19.214749Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"bt = study.best_trial\nprint(\"Best trial:\\n \", bt.value, bt.params)\n\nbest = canonicalize_best(bt.params)\nbest","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T21:22:27.289114Z","iopub.execute_input":"2025-09-24T21:22:27.289615Z","iopub.status.idle":"2025-09-24T21:22:27.302514Z","shell.execute_reply.started":"2025-09-24T21:22:27.289591Z","shell.execute_reply":"2025-09-24T21:22:27.301542Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_95/3444517524.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best trial:\\n \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcanonicalize_best\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'study' is not defined"],"ename":"NameError","evalue":"name 'study' is not defined","output_type":"error"}],"execution_count":28},{"cell_type":"code","source":"\n\n# (optional) retrain on train+val and evaluate on test\nimport numpy as np\nX_all = np.concatenate([X_train, X_val], axis=0)\nY_all = np.concatenate([Y_train_s, Y_val_s], axis=0)\nZ_all = None if Z_train is None else np.concatenate([Z_train, Z_val], axis=0)\n\nmodel = train_transformer(\n    X_all, Y_all, X_val, Y_val_s,                       # dummy val to drive early stop\n    Z_train=Z_all, Z_val=Z_val,\n    d_model=best[\"d_model\"], nhead=best[\"nhead\"],\n    layers=best[\"layers\"], d_ff=best[\"d_ff\"],\n    dropout=best[\"dropout\"], batch=best[\"batch\"],\n    epochs=100 if best[\"d_model\"] <= 160 else 90,\n    lr=best[\"lr\"], weight_decay=best[\"weight_decay\"],\n    horizon_weighted=True, seed=SEED\n)\n\nprint(\"\\n=== Final Test Report (best params) ===\")\n_ = test_report_tf(model, X_test, Y_test_s, bundle.y_scaler, Z_test=Z_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train_transformer_baseline_fixed.py  (minimal deterministic-friendly patch)\n\nimport os, random, math, numpy as np\n\n# ---- Toggle: set True for strict determinism (requires kernel restart) ----\nSTRICT_DETERMINISM = False\n\n# If strict, cuBLAS needs this BEFORE importing torch (and a fresh kernel)\nif STRICT_DETERMINISM:\n    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"  # or \":16:8\"\n\nimport torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# --- seeding (simple & safe) ---\nSEED = 42\ndef seed_everything(seed=42):\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    random.seed(seed); np.random.seed(seed)\n    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n    # keep deterministic cuDNN kernels, but don't force torch.use_deterministic_algorithms\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n    # TF32 off helps reproducibility\n    torch.backends.cuda.matmul.allow_tf32 = False\n    torch.backends.cudnn.allow_tf32 = False\n\nseed_everything(SEED)\n\n# --- AMP (new API with fallback); set USE_AMP=False for tighter reproducibility ---\nUSE_AMP = True\ntry:\n    from torch.amp import autocast, GradScaler\n    def amp_ctx():\n        if not USE_AMP:\n            class _NoOp:\n                def __enter__(self): return None\n                def __exit__(self, *a): return False\n            return _NoOp()\n        use_bf16 = (torch.cuda.is_available()\n                    and torch.cuda.get_device_capability()[0] >= 8)\n        return autocast(device_type=\"cuda\", dtype=torch.bfloat16 if use_bf16 else torch.float16)\nexcept Exception:\n    from torch.cuda.amp import autocast, GradScaler\n    def amp_ctx():\n        return autocast(enabled=(USE_AMP and torch.cuda.is_available()))\n\n# ========== Data ==========\nclass SeqDataset(Dataset):\n    \"\"\"Returns (X, Y) if Z is None; otherwise (X, Y, Z). Avoids collating None.\"\"\"\n    def __init__(self, X, Y, Z_future=None):\n        self.X = torch.from_numpy(X)              # (N, L, D)\n        self.Y = torch.from_numpy(Y)              # (N, H)\n        self.Z = None if Z_future is None else torch.from_numpy(Z_future.astype(np.float32))\n        self.has_future = self.Z is not None\n    def __len__(self): return self.X.shape[0]\n    def __getitem__(self, i):\n        if self.has_future:\n            return self.X[i], self.Y[i], self.Z[i]\n        else:\n            return self.X[i], self.Y[i]\n\ndef _seed_worker(worker_id: int):\n    wseed = torch.initial_seed() % 2**32\n    np.random.seed(wseed); random.seed(wseed)\n\ndef loaders(Xtr,Ytr,Xva,Yva, Ztr=None, Zva=None, batch=128, workers=0, seed: int = SEED):\n    g = torch.Generator().manual_seed(seed)  # deterministic shuffle\n    return (\n        DataLoader(SeqDataset(Xtr,Ytr,Ztr), batch_size=batch, shuffle=True,  drop_last=True,\n                   num_workers=workers, pin_memory=True, worker_init_fn=_seed_worker, generator=g),\n        DataLoader(SeqDataset(Xva,Yva,Zva), batch_size=batch, shuffle=False, drop_last=False,\n                   num_workers=workers, pin_memory=True, worker_init_fn=_seed_worker, generator=g),\n    )\n\n# ========== Model ==========\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=1000):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div); pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))  # (1, max_len, d_model)\n    def forward(self, x): return x + self.pe[:, :x.size(1)]\n\nclass TransformerForecaster(nn.Module):\n    def __init__(self, input_dim, horizon=24, d_model=256, nhead=8, num_layers=3,\n                 d_ff=512, dropout=0.1, future_feat_dim=0):\n        super().__init__()\n        self.horizon = horizon\n        self.input_proj = nn.Linear(input_dim, d_model)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=d_model, nhead=nhead, dim_feedforward=d_ff,\n            dropout=dropout, batch_first=True, norm_first=True\n        )\n        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n        self.pos = PositionalEncoding(d_model)\n        self.cls = nn.Parameter(torch.randn(1, 1, d_model) * 0.02)\n        self.horz_emb = nn.Parameter(torch.randn(horizon, d_model) * 0.02)\n        self.future_proj = nn.Linear(future_feat_dim, d_model) if future_feat_dim and future_feat_dim>0 else None\n        head_in = d_model + d_model + (d_model if self.future_proj else 0)\n        self.head = nn.Sequential(nn.LayerNorm(head_in), nn.Linear(head_in, 256),\n                                  nn.ReLU(), nn.Dropout(dropout), nn.Linear(256, 1))\n\n    def forward(self, x, z_future=None):\n        B, L, _ = x.shape\n        x = self.pos(self.input_proj(x))             # (B,L,d)\n        cls = self.cls.expand(B, 1, -1)\n        enc = self.encoder(torch.cat([cls, x], dim=1))\n        context = enc[:, 0, :]                       # (B,d)\n        H = self.horizon\n        he = self.horz_emb.unsqueeze(0).expand(B, H, -1)  # (B,H,d)\n        ctx = context.unsqueeze(1).expand(B, H, -1)       # (B,H,d)\n        if self.future_proj is not None and z_future is not None:\n            z_proj = self.future_proj(z_future)           # (B,H,d)\n            fuse = torch.cat([ctx, he, z_proj], dim=-1)\n        else:\n            fuse = torch.cat([ctx, he], dim=-1)\n        return self.head(fuse).squeeze(-1)                # (B,H)\n\n# ========== Metrics & training ==========\n@torch.no_grad()\ndef val_metrics_MW(model, loader, y_scaler):\n    model.eval()\n    preds_s, trues_s, n, mse_s = [], [], 0, 0.0\n    for batch in loader:\n        if len(batch)==2: xb, yb = batch; zb = None\n        else: xb, yb, zb = batch\n        xb = xb.to(next(model.parameters()).device)\n        yb = yb.to(next(model.parameters()).device)\n        zb = None if zb is None else zb.to(xb.device)\n        yhat_s = model(xb, zb)\n        mse_s += torch.mean((yhat_s - yb)**2).item() * xb.size(0); n += xb.size(0)\n        preds_s.append(yhat_s.detach().cpu().numpy()); trues_s.append(yb.detach().cpu().numpy())\n    preds_s, trues_s = np.concatenate(preds_s), np.concatenate(trues_s)\n    preds = bundle.y_scaler.inverse_transform(preds_s.reshape(-1,1)).reshape(preds_s.shape)\n    trues = bundle.y_scaler.inverse_transform(trues_s.reshape(-1,1)).reshape(trues_s.shape)\n    mae = np.mean(np.abs(preds - trues))\n    return mae, (mse_s / n)\n\ndef train_transformer(\n    X_train, Y_train_s, X_val, Y_val_s, Z_train=None, Z_val=None,\n    d_model=256, nhead=8, layers=3, d_ff=512, dropout=0.1,\n    batch=128, epochs=80, lr=2e-4, weight_decay=2e-4, horizon_weighted=True, seed: int = SEED\n):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    Din, H = X_train.shape[2], Y_train_s.shape[1]\n    Ff = 0 if Z_train is None else Z_train.shape[2]\n    tr_loader, va_loader = loaders(X_train, Y_train_s, X_val, Y_val_s, Z_train, Z_val,\n                                   batch=batch, workers=0, seed=seed)\n\n    model = TransformerForecaster(Din, horizon=H, d_model=d_model, nhead=nhead,\n                                  num_layers=layers, d_ff=d_ff, dropout=dropout,\n                                  future_feat_dim=Ff).to(device)\n    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\", factor=0.5, patience=2)\n    scaler = GradScaler(enabled=(USE_AMP and torch.cuda.is_available()))\n\n    loss_fn = nn.SmoothL1Loss(reduction=\"none\")\n    w = torch.linspace(1.20, 0.90, H, device=device).view(1, H) if horizon_weighted else None\n\n    best_mae, best = float(\"inf\"), None\n    patience, left = 8, 8\n    MIN_DELTA = 0.0\n\n    for epoch in range(1, epochs+1):\n        model.train()\n        run, nitems = 0.0, 0\n        for batch in tr_loader:\n            if len(batch)==2: xb, yb = batch; zb = None\n            else: xb, yb, zb = batch\n            xb = xb.to(device); yb = yb.to(device); zb = None if zb is None else zb.to(device)\n            opt.zero_grad(set_to_none=True)\n            with amp_ctx():\n                yhat_s = model(xb, zb)\n                L = loss_fn(yhat_s, yb)\n                if w is not None: L = L * w\n                L = L.mean()\n            scaler.scale(L).backward()\n            scaler.unscale_(opt); nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            scaler.step(opt); scaler.update()\n            run += L.item() * xb.size(0); nitems += xb.size(0)\n\n        val_mae_mw, val_mse_s = val_metrics_MW(model, va_loader, bundle.y_scaler)\n        sched.step(val_mae_mw)\n        print(f\"Epoch {epoch:02d} | train loss {run/nitems:.4f} | val MAE(MW) {val_mae_mw:.2f} | val MSE(scaled) {val_mse_s:.4f}\")\n\n        if best_mae - val_mae_mw > MIN_DELTA:\n            best_mae, best, left = val_mae_mw, {k: v.detach().cpu().clone() for k,v in model.state_dict().items()}, patience\n        else:\n            left -= 1\n            if left == 0:\n                print(\"Early stopping.\"); break\n\n    if best is not None: model.load_state_dict(best)\n    return model\n\n@torch.no_grad()\ndef test_report_tf(model, X_test, Y_test_s, y_scaler, Z_test=None, batch=256):\n    dl = DataLoader(SeqDataset(X_test, Y_test_s, Z_test), batch_size=batch, shuffle=False)\n    device = next(model.parameters()).device\n    preds_s, trues_s = [], []\n    for batch in dl:\n        if len(batch)==2: xb, yb = batch; zb = None\n        else: xb, yb, zb = batch\n        xb = xb.to(device); zb = None if zb is None else zb.to(device)\n        yhat_s = model(xb, zb)\n        preds_s.append(yhat_s.cpu().numpy()); trues_s.append(yb.numpy())\n    preds_s, trues_s = np.concatenate(preds_s), np.concatenate(trues_s)\n    preds = y_scaler.inverse_transform(preds_s.reshape(-1,1)).reshape(preds_s.shape)\n    trues = y_scaler.inverse_transform(trues_s.reshape(-1,1)).reshape(trues_s.shape)\n    mae = np.mean(np.abs(preds - trues))\n    rmse = np.sqrt(np.mean((preds - trues)**2))\n    mae_h = np.mean(np.abs(preds - trues), axis=0)\n    print(f\"\\nTest  MAE (MW):  {mae:.2f}\")\n    print(f\"Test RMSE (MW): {rmse:.2f}\")\n    print(\"Horizon-wise MAE (MW):\", np.round(mae_h, 2))\n    return {\"MAE\": mae, \"RMSE\": rmse, \"MAE_by_h\": mae_h}\n\n# ---------- Run ----------\nif __name__ == \"__main__\":\n    model = train_transformer(\n        X_train, Y_train_s, X_val, Y_val_s,\n        Z_train=Z_train, Z_val=Z_val,\n        d_model=160,    # middle ground, not too tiny\n        nhead=4,        # keeps per-head dim = 40\n        layers=4,       # slightly deeper than 2\n        d_ff=640,       # ~2× d_model (lightweight FFN)\n        dropout=0.13600591019653185,   # modest regularization\n        batch=128,\n        epochs=100,     # let it converge longer\n        lr=0.00037664214231482003,      # tuned for stability\n        weight_decay=0.0004831779595293628,\n        horizon_weighted=True,\n        seed=42\n    )\n    _ = test_report_tf(model, X_test, Y_test_s, bundle.y_scaler, Z_test=Z_test)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T21:42:44.836294Z","iopub.execute_input":"2025-09-22T21:42:44.836819Z","iopub.status.idle":"2025-09-22T21:44:52.404682Z","shell.execute_reply.started":"2025-09-22T21:42:44.836795Z","shell.execute_reply":"2025-09-22T21:44:52.404009Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 01 | train loss 0.0849 | val MAE(MW) 166.08 | val MSE(scaled) 0.0361\nEpoch 02 | train loss 0.0538 | val MAE(MW) 155.65 | val MSE(scaled) 0.0335\nEpoch 03 | train loss 0.0519 | val MAE(MW) 144.63 | val MSE(scaled) 0.0294\nEpoch 04 | train loss 0.0507 | val MAE(MW) 142.90 | val MSE(scaled) 0.0294\nEpoch 05 | train loss 0.0497 | val MAE(MW) 150.81 | val MSE(scaled) 0.0321\nEpoch 06 | train loss 0.0491 | val MAE(MW) 137.15 | val MSE(scaled) 0.0278\nEpoch 07 | train loss 0.0489 | val MAE(MW) 138.29 | val MSE(scaled) 0.0287\nEpoch 08 | train loss 0.0483 | val MAE(MW) 133.04 | val MSE(scaled) 0.0270\nEpoch 09 | train loss 0.0479 | val MAE(MW) 143.96 | val MSE(scaled) 0.0300\nEpoch 10 | train loss 0.0482 | val MAE(MW) 144.10 | val MSE(scaled) 0.0292\nEpoch 11 | train loss 0.0474 | val MAE(MW) 140.22 | val MSE(scaled) 0.0288\nEpoch 12 | train loss 0.0456 | val MAE(MW) 135.72 | val MSE(scaled) 0.0277\nEpoch 13 | train loss 0.0447 | val MAE(MW) 138.43 | val MSE(scaled) 0.0275\nEpoch 14 | train loss 0.0438 | val MAE(MW) 138.53 | val MSE(scaled) 0.0287\nEpoch 15 | train loss 0.0404 | val MAE(MW) 136.79 | val MSE(scaled) 0.0281\nEpoch 16 | train loss 0.0396 | val MAE(MW) 141.63 | val MSE(scaled) 0.0286\nEarly stopping.\n\nTest  MAE (MW):  181.40\nTest RMSE (MW): 258.09\nHorizon-wise MAE (MW): [180.47 180.48 180.45 179.73 180.67 180.47 179.94 180.62 180.55 180.59\n 180.54 180.69 181.07 180.98 180.85 181.4  181.13 182.18 182.3  183.25\n 183.53 183.68 183.67 184.27]\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"# train_transformer_baseline_fixed.py  (minimal deterministic-friendly patch)\n\nimport os, random, math, numpy as np\n\n# ---- Toggle: set True for strict determinism (requires kernel restart) ----\nSTRICT_DETERMINISM = False\n\n# If strict, cuBLAS needs this BEFORE importing torch (and a fresh kernel)\nif STRICT_DETERMINISM:\n    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"  # or \":16:8\"\n\nimport torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# --- seeding (simple & safe) ---\nSEED = 42\ndef seed_everything(seed=42):\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    random.seed(seed); np.random.seed(seed)\n    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n    # keep deterministic cuDNN kernels, but don't force torch.use_deterministic_algorithms\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n    # TF32 off helps reproducibility\n    torch.backends.cuda.matmul.allow_tf32 = False\n    torch.backends.cudnn.allow_tf32 = False\n\nseed_everything(SEED)\n\n# --- AMP (new API with fallback); set USE_AMP=False for tighter reproducibility ---\nUSE_AMP = True\ntry:\n    from torch.amp import autocast, GradScaler\n    def amp_ctx():\n        if not USE_AMP:\n            class _NoOp:\n                def __enter__(self): return None\n                def __exit__(self, *a): return False\n            return _NoOp()\n        use_bf16 = (torch.cuda.is_available()\n                    and torch.cuda.get_device_capability()[0] >= 8)\n        return autocast(device_type=\"cuda\", dtype=torch.bfloat16 if use_bf16 else torch.float16)\nexcept Exception:\n    from torch.cuda.amp import autocast, GradScaler\n    def amp_ctx():\n        return autocast(enabled=(USE_AMP and torch.cuda.is_available()))\n\n# ========== Data ==========\nclass SeqDataset(Dataset):\n    \"\"\"Returns (X, Y) if Z is None; otherwise (X, Y, Z). Avoids collating None.\"\"\"\n    def __init__(self, X, Y, Z_future=None):\n        self.X = torch.from_numpy(X)              # (N, L, D)\n        self.Y = torch.from_numpy(Y)              # (N, H)\n        self.Z = None if Z_future is None else torch.from_numpy(Z_future.astype(np.float32))\n        self.has_future = self.Z is not None\n    def __len__(self): return self.X.shape[0]\n    def __getitem__(self, i):\n        if self.has_future:\n            return self.X[i], self.Y[i], self.Z[i]\n        else:\n            return self.X[i], self.Y[i]\n\ndef _seed_worker(worker_id: int):\n    wseed = torch.initial_seed() % 2**32\n    np.random.seed(wseed); random.seed(wseed)\n\ndef loaders(Xtr,Ytr,Xva,Yva, Ztr=None, Zva=None, batch=128, workers=0, seed: int = SEED):\n    g = torch.Generator().manual_seed(seed)  # deterministic shuffle\n    return (\n        DataLoader(SeqDataset(Xtr,Ytr,Ztr), batch_size=batch, shuffle=True,  drop_last=True,\n                   num_workers=workers, pin_memory=True, worker_init_fn=_seed_worker, generator=g),\n        DataLoader(SeqDataset(Xva,Yva,Zva), batch_size=batch, shuffle=False, drop_last=False,\n                   num_workers=workers, pin_memory=True, worker_init_fn=_seed_worker, generator=g),\n    )\n\n# ========== Model ==========\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=1000):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div); pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))  # (1, max_len, d_model)\n    def forward(self, x): return x + self.pe[:, :x.size(1)]\n\nclass TransformerForecaster(nn.Module):\n    def __init__(self, input_dim, horizon=24, d_model=256, nhead=8, num_layers=3,\n                 d_ff=512, dropout=0.1, future_feat_dim=0):\n        super().__init__()\n        self.horizon = horizon\n        self.input_proj = nn.Linear(input_dim, d_model)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=d_model, nhead=nhead, dim_feedforward=d_ff,\n            dropout=dropout, batch_first=True, norm_first=True\n        )\n        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n        self.pos = PositionalEncoding(d_model)\n        self.cls = nn.Parameter(torch.randn(1, 1, d_model) * 0.02)\n        self.horz_emb = nn.Parameter(torch.randn(horizon, d_model) * 0.02)\n        self.future_proj = nn.Linear(future_feat_dim, d_model) if future_feat_dim and future_feat_dim>0 else None\n        head_in = d_model + d_model + (d_model if self.future_proj else 0)\n        self.head = nn.Sequential(nn.LayerNorm(head_in), nn.Linear(head_in, 256),\n                                  nn.ReLU(), nn.Dropout(dropout), nn.Linear(256, 1))\n\n    def forward(self, x, z_future=None):\n        B, L, _ = x.shape\n        x = self.pos(self.input_proj(x))             # (B,L,d)\n        cls = self.cls.expand(B, 1, -1)\n        enc = self.encoder(torch.cat([cls, x], dim=1))\n        context = enc[:, 0, :]                       # (B,d)\n        H = self.horizon\n        he = self.horz_emb.unsqueeze(0).expand(B, H, -1)  # (B,H,d)\n        ctx = context.unsqueeze(1).expand(B, H, -1)       # (B,H,d)\n        if self.future_proj is not None and z_future is not None:\n            z_proj = self.future_proj(z_future)           # (B,H,d)\n            fuse = torch.cat([ctx, he, z_proj], dim=-1)\n        else:\n            fuse = torch.cat([ctx, he], dim=-1)\n        return self.head(fuse).squeeze(-1)                # (B,H)\n\n# ========== Metrics & training ==========\n@torch.no_grad()\ndef val_metrics_MW(model, loader, y_scaler):\n    model.eval()\n    preds_s, trues_s, n, mse_s = [], [], 0, 0.0\n    for batch in loader:\n        if len(batch)==2: xb, yb = batch; zb = None\n        else: xb, yb, zb = batch\n        xb = xb.to(next(model.parameters()).device)\n        yb = yb.to(next(model.parameters()).device)\n        zb = None if zb is None else zb.to(xb.device)\n        yhat_s = model(xb, zb)\n        mse_s += torch.mean((yhat_s - yb)**2).item() * xb.size(0); n += xb.size(0)\n        preds_s.append(yhat_s.detach().cpu().numpy()); trues_s.append(yb.detach().cpu().numpy())\n    preds_s, trues_s = np.concatenate(preds_s), np.concatenate(trues_s)\n    preds = bundle.y_scaler.inverse_transform(preds_s.reshape(-1,1)).reshape(preds_s.shape)\n    trues = bundle.y_scaler.inverse_transform(trues_s.reshape(-1,1)).reshape(trues_s.shape)\n    mae = np.mean(np.abs(preds - trues))\n    return mae, (mse_s / n)\n\ndef train_transformer(\n    X_train, Y_train_s, X_val, Y_val_s, Z_train=None, Z_val=None,\n    d_model=256, nhead=8, layers=3, d_ff=512, dropout=0.1,\n    batch=128, epochs=80, lr=2e-4, weight_decay=2e-4, horizon_weighted=True, seed: int = SEED\n):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    Din, H = X_train.shape[2], Y_train_s.shape[1]\n    Ff = 0 if Z_train is None else Z_train.shape[2]\n    tr_loader, va_loader = loaders(X_train, Y_train_s, X_val, Y_val_s, Z_train, Z_val,\n                                   batch=batch, workers=0, seed=seed)\n\n    model = TransformerForecaster(Din, horizon=H, d_model=d_model, nhead=nhead,\n                                  num_layers=layers, d_ff=d_ff, dropout=dropout,\n                                  future_feat_dim=Ff).to(device)\n    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\", factor=0.5, patience=2)\n    scaler = GradScaler(enabled=(USE_AMP and torch.cuda.is_available()))\n\n    loss_fn = nn.SmoothL1Loss(reduction=\"none\")\n    w = torch.linspace(1.20, 0.90, H, device=device).view(1, H) if horizon_weighted else None\n\n    best_mae, best = float(\"inf\"), None\n    patience, left = 8, 8\n    MIN_DELTA = 0.0\n\n    for epoch in range(1, epochs+1):\n        model.train()\n        run, nitems = 0.0, 0\n        for batch in tr_loader:\n            if len(batch)==2: xb, yb = batch; zb = None\n            else: xb, yb, zb = batch\n            xb = xb.to(device); yb = yb.to(device); zb = None if zb is None else zb.to(device)\n            opt.zero_grad(set_to_none=True)\n            with amp_ctx():\n                yhat_s = model(xb, zb)\n                L = loss_fn(yhat_s, yb)\n                if w is not None: L = L * w\n                L = L.mean()\n            scaler.scale(L).backward()\n            scaler.unscale_(opt); nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            scaler.step(opt); scaler.update()\n            run += L.item() * xb.size(0); nitems += xb.size(0)\n\n        val_mae_mw, val_mse_s = val_metrics_MW(model, va_loader, bundle.y_scaler)\n        sched.step(val_mae_mw)\n        print(f\"Epoch {epoch:02d} | train loss {run/nitems:.4f} | val MAE(MW) {val_mae_mw:.2f} | val MSE(scaled) {val_mse_s:.4f}\")\n\n        if best_mae - val_mae_mw > MIN_DELTA:\n            best_mae, best, left = val_mae_mw, {k: v.detach().cpu().clone() for k,v in model.state_dict().items()}, patience\n        else:\n            left -= 1\n            if left == 0:\n                print(\"Early stopping.\"); break\n\n    if best is not None: model.load_state_dict(best)\n    return model\n\n@torch.no_grad()\ndef test_report_tf(model, X_test, Y_test_s, y_scaler, Z_test=None, batch=256):\n    dl = DataLoader(SeqDataset(X_test, Y_test_s, Z_test), batch_size=batch, shuffle=False)\n    device = next(model.parameters()).device\n    preds_s, trues_s = [], []\n    for batch in dl:\n        if len(batch)==2: xb, yb = batch; zb = None\n        else: xb, yb, zb = batch\n        xb = xb.to(device); zb = None if zb is None else zb.to(device)\n        yhat_s = model(xb, zb)\n        preds_s.append(yhat_s.cpu().numpy()); trues_s.append(yb.numpy())\n    preds_s, trues_s = np.concatenate(preds_s), np.concatenate(trues_s)\n    preds = y_scaler.inverse_transform(preds_s.reshape(-1,1)).reshape(preds_s.shape)\n    trues = y_scaler.inverse_transform(trues_s.reshape(-1,1)).reshape(trues_s.shape)\n    mae = np.mean(np.abs(preds - trues))\n    rmse = np.sqrt(np.mean((preds - trues)**2))\n    mae_h = np.mean(np.abs(preds - trues), axis=0)\n    print(f\"\\nTest  MAE (MW):  {mae:.2f}\")\n    print(f\"Test RMSE (MW): {rmse:.2f}\")\n    print(\"Horizon-wise MAE (MW):\", np.round(mae_h, 2))\n    return {\"MAE\": mae, \"RMSE\": rmse, \"MAE_by_h\": mae_h}\n\n# ---------- Run ----------\nif __name__ == \"__main__\":\n    model = train_transformer(\n        X_train, Y_train_s, X_val, Y_val_s,\n        Z_train=Z_train, Z_val=Z_val,\n        d_model=160,    # middle ground, not too tiny\n        nhead=4,        # keeps per-head dim = 40\n        layers=2,       # slightly deeper than 2\n        d_ff=640,       # ~2× d_model (lightweight FFN)\n        dropout=0.1450990521919241,   # modest regularization\n        batch=96,\n        epochs=100,     # let it converge longer\n        lr=0.0002934709738664784,      # tuned for stability\n        weight_decay=0.0009177960560335439,\n        horizon_weighted=True,\n        seed=42\n    )\n    _ = test_report_tf(model, X_test, Y_test_s, bundle.y_scaler, Z_test=Z_test)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T21:50:07.512848Z","iopub.execute_input":"2025-09-22T21:50:07.513129Z","iopub.status.idle":"2025-09-22T21:51:48.959259Z","shell.execute_reply.started":"2025-09-22T21:50:07.513110Z","shell.execute_reply":"2025-09-22T21:51:48.958483Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 01 | train loss 0.0746 | val MAE(MW) 161.62 | val MSE(scaled) 0.0346\nEpoch 02 | train loss 0.0532 | val MAE(MW) 163.89 | val MSE(scaled) 0.0364\nEpoch 03 | train loss 0.0515 | val MAE(MW) 148.26 | val MSE(scaled) 0.0319\nEpoch 04 | train loss 0.0502 | val MAE(MW) 143.16 | val MSE(scaled) 0.0305\nEpoch 05 | train loss 0.0492 | val MAE(MW) 145.67 | val MSE(scaled) 0.0307\nEpoch 06 | train loss 0.0486 | val MAE(MW) 135.78 | val MSE(scaled) 0.0279\nEpoch 07 | train loss 0.0483 | val MAE(MW) 132.26 | val MSE(scaled) 0.0265\nEpoch 08 | train loss 0.0479 | val MAE(MW) 135.29 | val MSE(scaled) 0.0291\nEpoch 09 | train loss 0.0474 | val MAE(MW) 137.29 | val MSE(scaled) 0.0295\nEpoch 10 | train loss 0.0469 | val MAE(MW) 136.06 | val MSE(scaled) 0.0276\nEpoch 11 | train loss 0.0454 | val MAE(MW) 137.51 | val MSE(scaled) 0.0298\nEpoch 12 | train loss 0.0447 | val MAE(MW) 133.59 | val MSE(scaled) 0.0277\nEpoch 13 | train loss 0.0446 | val MAE(MW) 142.58 | val MSE(scaled) 0.0292\nEpoch 14 | train loss 0.0413 | val MAE(MW) 134.99 | val MSE(scaled) 0.0279\nEpoch 15 | train loss 0.0402 | val MAE(MW) 134.50 | val MSE(scaled) 0.0276\nEarly stopping.\n\nTest  MAE (MW):  177.82\nTest RMSE (MW): 247.16\nHorizon-wise MAE (MW): [177.03 176.56 176.4  176.69 176.97 177.27 176.72 176.67 176.7  177.\n 177.32 177.62 178.27 178.11 178.08 178.69 179.04 179.25 178.54 178.82\n 178.77 178.53 178.96 179.72]\n","output_type":"stream"}],"execution_count":50},{"cell_type":"code","source":"{'d_model': 160, 'nhead_160': 4, 'layers': 2, 'd_ff_mult': 4, 'dropout': 0.1450990521919241, 'batch': 96, 'lr': 0.0002934709738664784, 'weight_decay': 0.0009177960560335439}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@torch.no_grad()\ndef predict_tf(model, X, Y_s, y_scaler, Z=None, batch=256):\n    \"\"\"\n    Returns:\n      P (MW):  predictions, shape (N, H)\n      Y (MW):  ground-truth, shape (N, H)  (inverse-transformed from *_s)\n    \"\"\"\n    model.eval()\n    dl = DataLoader(SeqDataset(X, Y_s, Z), batch_size=batch, shuffle=False)\n    device = next(model.parameters()).device\n\n    preds_s, trues_s = [], []\n    for batch in dl:\n        if len(batch) == 2:\n            xb, yb = batch; zb = None\n        else:\n            xb, yb, zb = batch\n        xb = xb.to(device)\n        zb = None if zb is None else zb.to(device)\n\n        yhat_s = model(xb, zb)                     # (B, H) in *scaled* space\n        preds_s.append(yhat_s.cpu().numpy())\n        trues_s.append(yb.cpu().numpy())\n\n    preds_s = np.concatenate(preds_s, axis=0)      # (N, H), scaled\n    trues_s = np.concatenate(trues_s, axis=0)      # (N, H), scaled\n\n    # inverse-transform per element (your scaler expects 2D)\n    P = y_scaler.inverse_transform(preds_s.reshape(-1,1)).reshape(preds_s.shape)  # (N,H) MW\n    Y = y_scaler.inverse_transform(trues_s.reshape(-1,1)).reshape(trues_s.shape)  # (N,H) MW\n    return P, Y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T16:20:27.523299Z","iopub.execute_input":"2025-09-24T16:20:27.523977Z","iopub.status.idle":"2025-09-24T16:20:27.530543Z","shell.execute_reply.started":"2025-09-24T16:20:27.523950Z","shell.execute_reply":"2025-09-24T16:20:27.529666Z"}},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"code","source":"# train_transformer_baseline_fixed.py  (minimal deterministic-friendly patch)\n\nimport os, random, math, numpy as np\n\n# ---- Toggle: set True for strict determinism (requires kernel restart) ----\nSTRICT_DETERMINISM = False\n\n# If strict, cuBLAS needs this BEFORE importing torch (and a fresh kernel)\nif STRICT_DETERMINISM:\n    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"  # or \":16:8\"\n\nimport torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# --- seeding (simple & safe) ---\nSEED = 42\ndef seed_everything(seed=42):\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    random.seed(seed); np.random.seed(seed)\n    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n    # keep deterministic cuDNN kernels, but don't force torch.use_deterministic_algorithms\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n    # TF32 off helps reproducibility\n    torch.backends.cuda.matmul.allow_tf32 = False\n    torch.backends.cudnn.allow_tf32 = False\n\nseed_everything(SEED)\n\n# --- AMP (new API with fallback); set USE_AMP=False for tighter reproducibility ---\nUSE_AMP = True\ntry:\n    from torch.amp import autocast, GradScaler\n    def amp_ctx():\n        if not USE_AMP:\n            class _NoOp:\n                def __enter__(self): return None\n                def __exit__(self, *a): return False\n            return _NoOp()\n        use_bf16 = (torch.cuda.is_available()\n                    and torch.cuda.get_device_capability()[0] >= 8)\n        return autocast(device_type=\"cuda\", dtype=torch.bfloat16 if use_bf16 else torch.float16)\nexcept Exception:\n    from torch.cuda.amp import autocast, GradScaler\n    def amp_ctx():\n        return autocast(enabled=(USE_AMP and torch.cuda.is_available()))\n\n# ========== Data ==========\nclass SeqDataset(Dataset):\n    \"\"\"Returns (X, Y) if Z is None; otherwise (X, Y, Z). Avoids collating None.\"\"\"\n    def __init__(self, X, Y, Z_future=None):\n        self.X = torch.from_numpy(X)              # (N, L, D)\n        self.Y = torch.from_numpy(Y)              # (N, H)\n        self.Z = None if Z_future is None else torch.from_numpy(Z_future.astype(np.float32))\n        self.has_future = self.Z is not None\n    def __len__(self): return self.X.shape[0]\n    def __getitem__(self, i):\n        if self.has_future:\n            return self.X[i], self.Y[i], self.Z[i]\n        else:\n            return self.X[i], self.Y[i]\n\ndef _seed_worker(worker_id: int):\n    wseed = torch.initial_seed() % 2**32\n    np.random.seed(wseed); random.seed(wseed)\n\ndef loaders(Xtr,Ytr,Xva,Yva, Ztr=None, Zva=None, batch=128, workers=0, seed: int = SEED):\n    g = torch.Generator().manual_seed(seed)  # deterministic shuffle\n    return (\n        DataLoader(SeqDataset(Xtr,Ytr,Ztr), batch_size=batch, shuffle=True,  drop_last=True,\n                   num_workers=workers, pin_memory=True, worker_init_fn=_seed_worker, generator=g),\n        DataLoader(SeqDataset(Xva,Yva,Zva), batch_size=batch, shuffle=False, drop_last=False,\n                   num_workers=workers, pin_memory=True, worker_init_fn=_seed_worker, generator=g),\n    )\n\n# ========== Model ==========\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=1000):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div); pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))  # (1, max_len, d_model)\n    def forward(self, x): return x + self.pe[:, :x.size(1)]\n\nclass TransformerForecaster(nn.Module):\n    def __init__(self, input_dim, horizon=24, d_model=256, nhead=8, num_layers=3,\n                 d_ff=512, dropout=0.1, future_feat_dim=0):\n        super().__init__()\n        self.horizon = horizon\n        self.input_proj = nn.Linear(input_dim, d_model)\n        enc_layer = nn.TransformerEncoderLayer(\n            d_model=d_model, nhead=nhead, dim_feedforward=d_ff,\n            dropout=dropout, batch_first=True, norm_first=True\n        )\n        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n        self.pos = PositionalEncoding(d_model)\n        self.cls = nn.Parameter(torch.randn(1, 1, d_model) * 0.02)\n        self.horz_emb = nn.Parameter(torch.randn(horizon, d_model) * 0.02)\n        self.future_proj = nn.Linear(future_feat_dim, d_model) if future_feat_dim and future_feat_dim>0 else None\n        head_in = d_model + d_model + (d_model if self.future_proj else 0)\n        self.head = nn.Sequential(nn.LayerNorm(head_in), nn.Linear(head_in, 256),\n                                  nn.ReLU(), nn.Dropout(dropout), nn.Linear(256, 1))\n\n    def forward(self, x, z_future=None):\n        B, L, _ = x.shape\n        x = self.pos(self.input_proj(x))             # (B,L,d)\n        cls = self.cls.expand(B, 1, -1)\n        enc = self.encoder(torch.cat([cls, x], dim=1))\n        context = enc[:, 0, :]                       # (B,d)\n        H = self.horizon\n        he = self.horz_emb.unsqueeze(0).expand(B, H, -1)  # (B,H,d)\n        ctx = context.unsqueeze(1).expand(B, H, -1)       # (B,H,d)\n        if self.future_proj is not None and z_future is not None:\n            z_proj = self.future_proj(z_future)           # (B,H,d)\n            fuse = torch.cat([ctx, he, z_proj], dim=-1)\n        else:\n            fuse = torch.cat([ctx, he], dim=-1)\n        return self.head(fuse).squeeze(-1)                # (B,H)\n\n# ========== Metrics & training ==========\n@torch.no_grad()\ndef val_metrics_MW(model, loader, y_scaler):\n    model.eval()\n    preds_s, trues_s, n, mse_s = [], [], 0, 0.0\n    for batch in loader:\n        if len(batch)==2: xb, yb = batch; zb = None\n        else: xb, yb, zb = batch\n        xb = xb.to(next(model.parameters()).device)\n        yb = yb.to(next(model.parameters()).device)\n        zb = None if zb is None else zb.to(xb.device)\n        yhat_s = model(xb, zb)\n        mse_s += torch.mean((yhat_s - yb)**2).item() * xb.size(0); n += xb.size(0)\n        preds_s.append(yhat_s.detach().cpu().numpy()); trues_s.append(yb.detach().cpu().numpy())\n    preds_s, trues_s = np.concatenate(preds_s), np.concatenate(trues_s)\n    preds = bundle.y_scaler.inverse_transform(preds_s.reshape(-1,1)).reshape(preds_s.shape)\n    trues = bundle.y_scaler.inverse_transform(trues_s.reshape(-1,1)).reshape(trues_s.shape)\n    mae = np.mean(np.abs(preds - trues))\n    return mae, (mse_s / n), preds\n\ndef train_transformer(\n    X_train, Y_train_s, X_val, Y_val_s, Z_train=None, Z_val=None,\n    d_model=256, nhead=8, layers=3, d_ff=512, dropout=0.1,\n    batch=128, epochs=80, lr=2e-4, weight_decay=2e-4, horizon_weighted=True, seed: int = SEED\n):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    Din, H = X_train.shape[2], Y_train_s.shape[1]\n    Ff = 0 if Z_train is None else Z_train.shape[2]\n    tr_loader, va_loader = loaders(X_train, Y_train_s, X_val, Y_val_s, Z_train, Z_val,\n                                   batch=batch, workers=0, seed=seed)\n\n    model = TransformerForecaster(Din, horizon=H, d_model=d_model, nhead=nhead,\n                                  num_layers=layers, d_ff=d_ff, dropout=dropout,\n                                  future_feat_dim=Ff).to(device)\n    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\", factor=0.5, patience=2)\n    scaler = GradScaler(enabled=(USE_AMP and torch.cuda.is_available()))\n\n    loss_fn = nn.SmoothL1Loss(reduction=\"none\")\n    w = torch.linspace(1.20, 0.90, H, device=device).view(1, H) if horizon_weighted else None\n\n    best_mae, best = float(\"inf\"), None\n    patience, left = 8, 8\n    MIN_DELTA = 0.0\n\n    for epoch in range(1, epochs+1):\n        model.train()\n        run, nitems = 0.0, 0\n        for batch in tr_loader:\n            if len(batch)==2: xb, yb = batch; zb = None\n            else: xb, yb, zb = batch\n            xb = xb.to(device); yb = yb.to(device); zb = None if zb is None else zb.to(device)\n            opt.zero_grad(set_to_none=True)\n            with amp_ctx():\n                yhat_s = model(xb, zb)\n                L = loss_fn(yhat_s, yb)\n                if w is not None: L = L * w\n                L = L.mean()\n            scaler.scale(L).backward()\n            scaler.unscale_(opt); nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            scaler.step(opt); scaler.update()\n            run += L.item() * xb.size(0); nitems += xb.size(0)\n\n        val_mae_mw, val_mse_s, yhat_val = val_metrics_MW(model, va_loader, bundle.y_scaler)\n        sched.step(val_mae_mw)\n        print(f\"Epoch {epoch:02d} | train loss {run/nitems:.4f} | val MAE(MW) {val_mae_mw:.2f} | val MSE(scaled) {val_mse_s:.4f}\")\n\n        if best_mae - val_mae_mw > MIN_DELTA:\n            best_mae, best, left = val_mae_mw, {k: v.detach().cpu().clone() for k,v in model.state_dict().items()}, patience\n        else:\n            left -= 1\n            if left == 0:\n                print(\"Early stopping.\"); break\n\n    if best is not None: model.load_state_dict(best)\n    return model, yhat_val\n\n@torch.no_grad()\ndef test_report_tf(model, X_test, Y_test_s, y_scaler, Z_test=None, batch=256):\n    dl = DataLoader(SeqDataset(X_test, Y_test_s, Z_test), batch_size=batch, shuffle=False)\n    device = next(model.parameters()).device\n    preds_s, trues_s = [], []\n    for batch in dl:\n        if len(batch)==2: xb, yb = batch; zb = None\n        else: xb, yb, zb = batch\n        xb = xb.to(device); zb = None if zb is None else zb.to(device)\n        yhat_s = model(xb, zb)\n        preds_s.append(yhat_s.cpu().numpy()); trues_s.append(yb.numpy())\n    preds_s, trues_s = np.concatenate(preds_s), np.concatenate(trues_s)\n    preds = y_scaler.inverse_transform(preds_s.reshape(-1,1)).reshape(preds_s.shape)\n    trues = y_scaler.inverse_transform(trues_s.reshape(-1,1)).reshape(trues_s.shape)\n    \n    mae = np.mean(np.abs(preds - trues))\n    rmse = np.sqrt(np.mean((preds - trues)**2))\n    mae_h = np.mean(np.abs(preds - trues), axis=0)\n    def safe_mape(y_true, y_pred, eps=1e-6):\n        y_true = np.asarray(y_true)\n        return np.mean(np.abs((y_true - y_pred) / np.clip(np.abs(y_true), eps, None))) * 100.0\n    mape = safe_mape(trues, preds)\n    return {\"MAE\": mae, \"RMSE\": rmse, \"MAE_by_h\": mae_h, \"MAPE\": mape}, preds\n\n# ---------- Run ----------\nif __name__ == \"__main__\":\n    model_transformer, P_trf_val = train_transformer(\n        X_train, Y_train_s, X_val, Y_val_s,\n        Z_train=Z_train, Z_val=Z_val,\n        d_model=160,    # middle ground, not too tiny\n        nhead=4,        # keeps per-head dim = 40\n        layers=2,       # slightly deeper than 2\n        d_ff=640,       # ~2× d_model (lightweight FFN)\n        dropout=0.1450990521919241,   # modest regularization\n        batch=96,\n        epochs=100,     # let it converge longer\n        lr=0.0002934709738664784,      # tuned for stability\n        weight_decay=0.0009177960560335439,\n        horizon_weighted=True,\n        seed=42\n    )\n    metrics_transformer, P_trf_test = test_report_tf(model_transformer, X_test, Y_test_s, bundle.y_scaler, Z_test=Z_test)\n    print(f\"\\nTest MAE (MW):  {metrics_transformer['MAE']:.2f}\")\n    print(f\"Test RMSE (MW): {metrics_transformer['RMSE']:.2f}\")\n    print(f\"Test MAPE (MW): {metrics_transformer['MAPE']:.2f}\")\n    print(\"Horizon-wise MAE (MW):\", np.round(metrics_transformer[\"MAE_by_h\"], 2))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:29:57.664021Z","iopub.execute_input":"2025-09-25T09:29:57.664346Z","iopub.status.idle":"2025-09-25T09:31:39.079508Z","shell.execute_reply.started":"2025-09-25T09:29:57.664323Z","shell.execute_reply":"2025-09-25T09:31:39.078868Z"}},"outputs":[{"name":"stdout","text":"Epoch 01 | train loss 0.0746 | val MAE(MW) 161.62 | val MSE(scaled) 0.0346\nEpoch 02 | train loss 0.0532 | val MAE(MW) 163.89 | val MSE(scaled) 0.0364\nEpoch 03 | train loss 0.0515 | val MAE(MW) 148.26 | val MSE(scaled) 0.0319\nEpoch 04 | train loss 0.0502 | val MAE(MW) 143.16 | val MSE(scaled) 0.0305\nEpoch 05 | train loss 0.0492 | val MAE(MW) 145.67 | val MSE(scaled) 0.0307\nEpoch 06 | train loss 0.0486 | val MAE(MW) 135.78 | val MSE(scaled) 0.0279\nEpoch 07 | train loss 0.0483 | val MAE(MW) 132.26 | val MSE(scaled) 0.0265\nEpoch 08 | train loss 0.0479 | val MAE(MW) 135.29 | val MSE(scaled) 0.0291\nEpoch 09 | train loss 0.0474 | val MAE(MW) 137.29 | val MSE(scaled) 0.0295\nEpoch 10 | train loss 0.0469 | val MAE(MW) 136.06 | val MSE(scaled) 0.0276\nEpoch 11 | train loss 0.0454 | val MAE(MW) 137.51 | val MSE(scaled) 0.0298\nEpoch 12 | train loss 0.0447 | val MAE(MW) 133.59 | val MSE(scaled) 0.0277\nEpoch 13 | train loss 0.0446 | val MAE(MW) 142.58 | val MSE(scaled) 0.0292\nEpoch 14 | train loss 0.0413 | val MAE(MW) 134.99 | val MSE(scaled) 0.0279\nEpoch 15 | train loss 0.0402 | val MAE(MW) 134.50 | val MSE(scaled) 0.0276\nEarly stopping.\n\nTest MAE (MW):  177.82\nTest RMSE (MW): 247.16\nTest MAPE (MW): 7.17\nHorizon-wise MAE (MW): [177.03 176.56 176.4  176.69 176.97 177.27 176.72 176.67 176.7  177.\n 177.32 177.62 178.27 178.11 178.08 178.69 179.04 179.25 178.54 178.82\n 178.77 178.53 178.96 179.72]\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"print(P_trf_val.shape, P_trf_test.shape)   # both -> (N, 24)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:33:02.690697Z","iopub.execute_input":"2025-09-25T09:33:02.691441Z","iopub.status.idle":"2025-09-25T09:33:02.695162Z","shell.execute_reply.started":"2025-09-25T09:33:02.691417Z","shell.execute_reply":"2025-09-25T09:33:02.694606Z"}},"outputs":[{"name":"stdout","text":"(1344, 24) (1344, 24)\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"torch.save(model_transformer.state_dict(), \"transformer_forecaster.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:33:24.535752Z","iopub.execute_input":"2025-09-25T09:33:24.536460Z","iopub.status.idle":"2025-09-25T09:33:24.549945Z","shell.execute_reply.started":"2025-09-25T09:33:24.536440Z","shell.execute_reply":"2025-09-25T09:33:24.549184Z"}},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"# XGBoost/LightGBM combining with Transformer","metadata":{}},{"cell_type":"code","source":"from sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\nfrom sklearn.metrics import make_scorer, mean_squared_error\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:51:04.346253Z","iopub.execute_input":"2025-09-25T09:51:04.347014Z","iopub.status.idle":"2025-09-25T09:51:10.257001Z","shell.execute_reply.started":"2025-09-25T09:51:04.346985Z","shell.execute_reply":"2025-09-25T09:51:10.256405Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"## XGBoost","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:51:10.258201Z","iopub.execute_input":"2025-09-25T09:51:10.259136Z","iopub.status.idle":"2025-09-25T09:51:10.262224Z","shell.execute_reply.started":"2025-09-25T09:51:10.259113Z","shell.execute_reply":"2025-09-25T09:51:10.261638Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"def avg_neg_rmse(y_true, y_pred):\n        y_true = np.asarray(y_true); y_pred = np.asarray(y_pred)\n        rmse_per_h = np.sqrt(np.mean((y_true - y_pred)**2, axis=0))\n        return -float(np.mean(rmse_per_h))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:51:11.518185Z","iopub.execute_input":"2025-09-25T09:51:11.518488Z","iopub.status.idle":"2025-09-25T09:51:11.522857Z","shell.execute_reply.started":"2025-09-25T09:51:11.518467Z","shell.execute_reply":"2025-09-25T09:51:11.522137Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"def runXGBoost(df_feat):\n    # 3. train/valid/test split by time\n    label_cols   = [f'y_t+{h}' for h in range(1, HORIZON+1)]\n    feature_cols = [c for c in df_feat.columns if c not in label_cols + [TIME_COL, TARGET_COL]]\n    \n    X = df_feat[feature_cols]\n    Y = df_feat[label_cols].values\n    \n    N        = len(X)\n    train_end= int(N - 2*8*7*24)\n    valid_end= int(N - 8*7*24)\n    \n    X_train, Y_train = X.iloc[:train_end],    Y[:train_end]\n    X_valid, Y_valid = X.iloc[train_end:valid_end], Y[train_end:valid_end]\n    X_test,  Y_test  = X.iloc[valid_end:],    Y[valid_end:]\n    \n    print(X_train.shape, Y_train.shape)\n    print(X_valid.shape, Y_valid.shape)\n    print(X_test.shape, Y_test.shape)\n    # 4. preprocessor: one‑hot encode the 4 categorical columns, pass others through\n    preprocess = ColumnTransformer(\n        [\n            ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), CAT_COLS),\n            ('num', 'passthrough', [c for c in feature_cols if c not in CAT_COLS])\n        ],\n        remainder='drop'\n    )\n    # Scorer: average RMSE across 24 horizons (negated for maximization)\n    # =========================\n    scorer = make_scorer(avg_neg_rmse, greater_is_better=True)\n    import joblib\n    xgb_tuner = joblib.load('/kaggle/input/xgboostlightgbm/scikitlearn/default/1/xgb_tuner_1.pkl')\n    print(xgb_tuner.best_params_)\n    # =========================\n    # XGBoost + known-ahead Z\n    # Train on TRAIN only; predict on VALID and TEST\n    # =========================\n    DTYPE = np.float32\n    \n    # 1) Build Z for EXACT rows in df_feat, then split into train/val/test\n    def build_future_covariates(times, H, df, TIME_COL):\n        df_key = df[[TIME_COL, 'is_holiday', 'season']].copy()\n        df_key[TIME_COL] = pd.to_datetime(df_key[TIME_COL])\n        df_key = df_key.set_index(TIME_COL).sort_index()\n    \n        Z_list = []\n        for ts in times:\n            row = []\n            for h in range(1, H+1):\n                t_h = pd.Timestamp(ts) + pd.Timedelta(hours=h)\n                hour = t_h.hour; dow = t_h.dayofweek; doy = t_h.dayofyear\n                v = [\n                    np.sin(2*np.pi*hour/24), np.cos(2*np.pi*hour/24),\n                    np.sin(2*np.pi*dow/7),  np.cos(2*np.pi*dow/7),\n                    np.sin(2*np.pi*doy/365.25), np.cos(2*np.pi*doy/365.25),\n                    1 if dow>=5 else 0,  # is_weekend\n                ]\n                if t_h in df_key.index:\n                    hol = str(df_key.loc[t_h, 'is_holiday'])\n                    sea = str(df_key.loc[t_h, 'season'])\n                else:\n                    hol, sea = \"False\", None\n                hol_vec = [int(hol==\"False\"), int(hol==\"tet\"), int(hol==\"national\")]\n                sea_vec = [int(sea==s) for s in [\"winter\",\"spring\",\"summer\",\"autumn\"]] if sea is not None else [0,0,0,0]\n                v += hol_vec + sea_vec\n                row.append(v)\n            Z_list.append(row)\n        return np.array(Z_list, dtype=DTYPE)  # (N, H, Ff)\n    \n    # -- Build & split Z to align with X/Y splits defined above --\n    end_times_all = df_feat[TIME_COL].values\n    Z_all = build_future_covariates(end_times_all, HORIZON, df=df, TIME_COL=TIME_COL)\n    \n    Z_train = Z_all[:train_end]\n    Z_val   = Z_all[train_end:valid_end]\n    Z_test  = Z_all[valid_end:]\n    \n    # 2) Fit preprocessor on TRAIN only; transform VALID & TEST\n    X_train_enc = preprocess.fit_transform(X_train)\n    X_val_enc   = preprocess.transform(X_valid)\n    X_test_enc  = preprocess.transform(X_test)\n    \n    # 3) Map tuned params to xgb.train\n    bp = xgb_tuner.best_params_  # already loaded from your joblib\n    def g(k, default=None): return bp.get(f\"model__estimator__{k}\", default)\n    \n    xgb_params = {\n        \"max_depth\":         g(\"max_depth\"),\n        \"eta\":               g(\"learning_rate\"),\n        \"subsample\":         g(\"subsample\"),\n        \"colsample_bytree\":  g(\"colsample_bytree\"),\n        \"min_child_weight\":  g(\"min_child_weight\"),\n        \"gamma\":             g(\"gamma\"),\n        \"lambda\":            g(\"reg_lambda\"),\n        \"alpha\":             g(\"reg_alpha\"),\n        \"objective\":         \"reg:squarederror\",\n        \"eval_metric\":       \"rmse\",\n        \"verbosity\":         0,\n        \"tree_method\":       \"gpu_hist\",   # switch to 'hist' if no GPU\n    }\n    num_boost_round = g(\"n_estimators\", 1500)\n    early_stopping_rounds = 100\n    \n    # 4) Train per-horizon on TRAIN, early-stop on VALID; predict both VALID and TEST\n    models_xgb = []\n    y_hat_val_list  = []\n    y_hat_test_list = []\n    \n    for h in range(HORIZON):  # 0..23 => t+1..t+24\n        Xtr_h = np.hstack([X_train_enc, Z_train[:, h, :]]).astype(DTYPE)\n        Xva_h = np.hstack([X_val_enc,   Z_val[:,   h, :]]).astype(DTYPE)\n        Xte_h = np.hstack([X_test_enc,  Z_test[:,  h, :]]).astype(DTYPE)\n    \n        dtrain = xgb.DMatrix(Xtr_h, label=Y_train[:, h])\n        dvalid = xgb.DMatrix(Xva_h, label=Y_valid[:, h])\n    \n        booster = xgb.train(\n            params=xgb_params,\n            dtrain=dtrain,\n            num_boost_round=num_boost_round,\n            evals=[(dvalid, \"valid\")],\n            early_stopping_rounds=early_stopping_rounds,\n            verbose_eval=False,\n        )\n        models_xgb.append(booster)\n    \n        # predictions for VAL & TEST under the SAME horizon-specific features\n        y_hat_val_list.append(\n            booster.predict(xgb.DMatrix(Xva_h), iteration_range=(0, booster.best_iteration + 1))\n        )\n        y_hat_test_list.append(\n            booster.predict(xgb.DMatrix(Xte_h), iteration_range=(0, booster.best_iteration + 1))\n        )\n    \n    Y_hat_val  = np.column_stack(y_hat_val_list)   # shape (N_val,  H)\n    Y_hat_test = np.column_stack(y_hat_test_list)  # shape (N_test, H)\n    \n    # 5) Optional: metrics for quick inspection\n    def safe_mape(y_true, y_pred, eps=1e-6):\n        y_true = np.asarray(y_true)\n        return np.mean(np.abs((y_true - y_pred) / np.clip(np.abs(y_true), eps, None))) * 100.0\n    \n    rmse_val = [math.sqrt(mean_squared_error(Y_valid[:, h], Y_hat_val[:, h])) for h in range(HORIZON)]\n    rmse_tst = [math.sqrt(mean_squared_error(Y_test[:,  h], Y_hat_test[:, h])) for h in range(HORIZON)]\n    print(\"VAL  Avg RMSE:\", round(np.mean(rmse_val), 3), \"  TEST Avg RMSE:\", round(np.mean(rmse_tst), 3))\n\n    return Y_hat_val, Y_hat_test\n\nP_xgb_val, P_xgb_test = runXGBoost(df_feat)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:51:18.174925Z","iopub.execute_input":"2025-09-25T09:51:18.175556Z","iopub.status.idle":"2025-09-25T09:53:52.570913Z","shell.execute_reply.started":"2025-09-25T09:51:18.175529Z","shell.execute_reply":"2025-09-25T09:53:52.570216Z"}},"outputs":[{"name":"stdout","text":"(44689, 131) (44689, 24)\n(1344, 131) (1344, 24)\n(1344, 131) (1344, 24)\n{'model__estimator__subsample': 0.7, 'model__estimator__reg_lambda': 0.5, 'model__estimator__reg_alpha': 0.0, 'model__estimator__n_estimators': 1000, 'model__estimator__min_child_weight': 3, 'model__estimator__max_depth': 6, 'model__estimator__learning_rate': 0.03, 'model__estimator__gamma': 0.1, 'model__estimator__colsample_bytree': 0.7}\nVAL  Avg RMSE: 213.258   TEST Avg RMSE: 285.286\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"models_xgb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T11:04:21.421080Z","iopub.execute_input":"2025-09-25T11:04:21.421921Z","iopub.status.idle":"2025-09-25T11:04:21.435829Z","shell.execute_reply.started":"2025-09-25T11:04:21.421897Z","shell.execute_reply":"2025-09-25T11:04:21.434948Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_84/4137685484.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodels_xgb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'models_xgb' is not defined"],"ename":"NameError","evalue":"name 'models_xgb' is not defined","output_type":"error"}],"execution_count":61},{"cell_type":"code","source":"print(P_xgb_val.shape, P_xgb_test.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:59:08.428228Z","iopub.execute_input":"2025-09-25T09:59:08.428519Z","iopub.status.idle":"2025-09-25T09:59:08.432920Z","shell.execute_reply.started":"2025-09-25T09:59:08.428499Z","shell.execute_reply":"2025-09-25T09:59:08.432072Z"}},"outputs":[{"name":"stdout","text":"(1344, 24) (1344, 24)\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"import numpy as np\n\n# ---- 1) Simple average\nP_avg_val_tfxgb  = 0.5*P_trf_val + 0.5*P_xgb_val\nP_avg_test_tfxgb = 0.5*P_trf_test + 0.5*P_xgb_test\n\n# ---- 2) Per-horizon optimal weight α_h via closed-form least squares (clip to [0,1])\ndef optimal_alphas_per_horizon(Y, P1, P2):\n    # Y, P1, P2: (n, H)\n    H = Y.shape[1]\n    alpha = np.zeros(H)\n    D = P1 - P2                       # (n, H)\n    num = ((Y - P2) * D).sum(axis=0)  # covariance-like term\n    den = (D * D).sum(axis=0) + 1e-12 # variance-like term (avoid 0-division)\n    alpha = num / den\n    return np.clip(alpha, 0.0, 1.0)   # enforce convex combo\n\nalph_tfxgb = optimal_alphas_per_horizon(Y_val, P_trf_val, P_xgb_val)  # (24,)\n# Broadcast to (n,H)\nP_wavg_val_tfxgb  = alph_tfxgb * P_trf_val  + (1 - alph_tfxgb) * P_xgb_val\nP_wavg_test_tfxgb = alph_tfxgb * P_trf_test + (1 - alph_tfxgb) * P_xgb_test\nprint(\"Per-horizon alphas:\", np.round(alph_tfxgb, 3))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T10:13:07.360532Z","iopub.execute_input":"2025-09-25T10:13:07.361077Z","iopub.status.idle":"2025-09-25T10:13:07.368778Z","shell.execute_reply.started":"2025-09-25T10:13:07.361053Z","shell.execute_reply":"2025-09-25T10:13:07.368186Z"}},"outputs":[{"name":"stdout","text":"Per-horizon alphas: [0.656 0.678 0.626 0.629 0.597 0.62  0.596 0.576 0.6   0.628 0.643 0.651\n 0.598 0.672 0.609 0.616 0.609 0.573 0.643 0.681 0.634 0.653 0.712 0.696]\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\ndef rmse(y, yhat): return np.sqrt(mean_squared_error(y, yhat))\ndef nrmse(y, yhat): return rmse(y, yhat) / (np.mean(y) + 1e-12)\ndef mape(y, yhat):  return np.mean(np.abs((y - yhat) / (y + 1e-12)))\ndef mae(y, yhat): return np.mean(np.abs(y - yhat))\ndef report(name, Y, P):\n    print(f\"{name:12s} | MAE:{mae(Y,P):8.2f}  RMSE:{rmse(Y,P):8.2f}  nRMSE:{100*nrmse(Y,P):6.2f}%  MAPE:{100*mape(Y,P):6.2f}%\")\n\n\nprint(\"On validation:\")\nreport(\"XGBoost\",     Y_val, P_xgb_val)\nreport(\"Transformer\", Y_val, P_trf_val)\nreport(\"Avg(50/50)\",  Y_val, P_avg_val_tfxgb)\nreport(\"Weighted\",    Y_val, P_wavg_val_tfxgb)\n\nprint(\"On test (final results):\")\nreport(\"XGBoost\",     Y_test, P_xgb_test)\nreport(\"Transformer\", Y_test, P_trf_test)\nreport(\"Avg(50/50)\",  Y_test, P_avg_test_tfxgb)\nreport(\"Weighted\",    Y_test, P_wavg_test_tfxgb)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T10:13:13.812278Z","iopub.execute_input":"2025-09-25T10:13:13.812528Z","iopub.status.idle":"2025-09-25T10:13:13.834193Z","shell.execute_reply.started":"2025-09-25T10:13:13.812511Z","shell.execute_reply":"2025-09-25T10:13:13.833477Z"}},"outputs":[{"name":"stdout","text":"On validation:\nXGBoost      | MAE:  149.58  RMSE:  213.38  nRMSE: 10.88%  MAPE:  7.45%\nTransformer  | MAE:  134.50  RMSE:  193.76  nRMSE:  9.88%  MAPE:  6.66%\nAvg(50/50)   | MAE:  124.69  RMSE:  184.98  nRMSE:  9.44%  MAPE:  6.06%\nWeighted     | MAE:  123.87  RMSE:  183.40  nRMSE:  9.36%  MAPE:  6.02%\nOn test (final results):\nXGBoost      | MAE:  203.20  RMSE:  285.64  nRMSE: 11.54%  MAPE:  8.13%\nTransformer  | MAE:  177.82  RMSE:  247.16  nRMSE:  9.99%  MAPE:  7.17%\nAvg(50/50)   | MAE:  177.35  RMSE:  250.49  nRMSE: 10.12%  MAPE:  7.06%\nWeighted     | MAE:  174.71  RMSE:  245.96  nRMSE:  9.94%  MAPE:  6.97%\n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"np.savetxt(\"ensemble_weights_xgb_trf.csv\", alph, delimiter=\",\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T17:23:00.012237Z","iopub.execute_input":"2025-09-24T17:23:00.012547Z","iopub.status.idle":"2025-09-24T17:23:00.017047Z","shell.execute_reply.started":"2025-09-24T17:23:00.012521Z","shell.execute_reply":"2025-09-24T17:23:00.016531Z"}},"outputs":[],"execution_count":47},{"cell_type":"markdown","source":"## LightGBM","metadata":{}},{"cell_type":"code","source":"import os, json, joblib, numpy as np, gc\nfrom sklearn.model_selection import ParameterSampler\nfrom sklearn.base import clone\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.multioutput import MultiOutputRegressor\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:59:29.497538Z","iopub.execute_input":"2025-09-25T09:59:29.498084Z","iopub.status.idle":"2025-09-25T09:59:29.501788Z","shell.execute_reply.started":"2025-09-25T09:59:29.498059Z","shell.execute_reply":"2025-09-25T09:59:29.501115Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"lgb_tuner = joblib.load('/kaggle/input/lightgbm/scikitlearn/default/1/lgb_tuner_1.pkl')\nwith open('/kaggle/input/lightgbm/scikitlearn/default/1/lgbm_best_params.json', \"r\") as f:\n    best_params_lgb = json.load(f)\n\n\nprint(lgb_tuner)\nprint(best_params_lgb)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:59:35.548536Z","iopub.execute_input":"2025-09-25T09:59:35.548815Z","iopub.status.idle":"2025-09-25T09:59:35.579157Z","shell.execute_reply.started":"2025-09-25T09:59:35.548797Z","shell.execute_reply":"2025-09-25T09:59:35.578580Z"}},"outputs":[{"name":"stdout","text":"Pipeline(steps=[('prep',\n                 ColumnTransformer(transformers=[('cat',\n                                                  OneHotEncoder(handle_unknown='ignore',\n                                                                sparse_output=False),\n                                                  ['Weather', 'Wind Direction',\n                                                   'season', 'is_holiday']),\n                                                 ('num', 'passthrough',\n                                                  ['Temperature',\n                                                   'Precipitation',\n                                                   'Chance of snow', 'Humidity',\n                                                   'Wind', 'Wind Gust',\n                                                   'Wind Degree', 'Cloud Cover',\n                                                   'Visibility', 'is_weekend',\n                                                   'hour', 'dow', 'dom',\n                                                   'month', 'd...\n                                                   'doy_cos', 'y_lag_1',\n                                                   'y_lag_2', 'y_lag_3',\n                                                   'y_lag_4', 'y_lag_5',\n                                                   'y_lag_6', 'y_lag_7',\n                                                   'y_lag_8', 'y_lag_9', ...])])),\n                ('model',\n                 MultiOutputRegressor(estimator=LGBMRegressor(colsample_bytree=0.7,\n                                                              learning_rate=0.05,\n                                                              max_bin=127,\n                                                              max_depth=8,\n                                                              n_estimators=300,\n                                                              n_jobs=-1,\n                                                              num_leaves=63,\n                                                              objective='regression',\n                                                              random_state=42,\n                                                              reg_lambda=0.1,\n                                                              subsample=0.9,\n                                                              verbose=-1),\n                                      n_jobs=1))])\n{'model__estimator__subsample': 0.9, 'model__estimator__reg_lambda': 0.1, 'model__estimator__reg_alpha': 0.0, 'model__estimator__num_leaves': 63, 'model__estimator__n_estimators': 300, 'model__estimator__min_child_samples': 20, 'model__estimator__max_depth': 8, 'model__estimator__learning_rate': 0.05, 'model__estimator__colsample_bytree': 0.7}\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"def avg_neg_rmse(y_true, y_pred):\n        y_true = np.asarray(y_true); y_pred = np.asarray(y_pred)\n        rmse_per_h = np.sqrt(np.mean((y_true - y_pred)**2, axis=0))\n        return -float(np.mean(rmse_per_h))\n    \ndef runLightGBM(df_feat):\n    # 3. train/valid/test split by time\n    label_cols   = [f'y_t+{h}' for h in range(1, HORIZON+1)]\n    feature_cols = [c for c in df_feat.columns if c not in label_cols + [TIME_COL, TARGET_COL]]\n    \n    X = df_feat[feature_cols]\n    Y = df_feat[label_cols].values\n    \n    N        = len(X)\n    train_end= int(N - 2*8*7*24)\n    valid_end= int(N - 8*7*24)\n    \n    X_train, Y_train = X.iloc[:train_end],    Y[:train_end]\n    X_valid, Y_valid = X.iloc[train_end:valid_end], Y[train_end:valid_end]\n    X_test,  Y_test  = X.iloc[valid_end:],    Y[valid_end:]\n    \n    print(X_train.shape, Y_train.shape)\n    print(X_valid.shape, Y_valid.shape)\n    print(X_test.shape, Y_test.shape)\n    # 4. preprocessor: one‑hot encode the 4 categorical columns, pass others through\n    preprocess = ColumnTransformer(\n        [\n            ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), CAT_COLS),\n            ('num', 'passthrough', [c for c in feature_cols if c not in CAT_COLS])\n        ],\n        remainder='drop'\n    )\n    # Scorer: average RMSE across 24 horizons (negated for maximization)\n    # =========================\n    scorer = make_scorer(avg_neg_rmse, greater_is_better=True)\n\n    # =========================\n    # LightGBM + known-ahead Z\n    # Train on TRAIN only; predict on VALID and TEST\n    # =========================\n    DTYPE = np.float32\n\n    # 1) Build Z for EXACT rows in df_feat, then split into train/val/test\n    def build_future_covariates(times, H, df, TIME_COL):\n        df_key = df[[TIME_COL, 'is_holiday', 'season']].copy()\n        df_key[TIME_COL] = pd.to_datetime(df_key[TIME_COL])\n        df_key = df_key.set_index(TIME_COL).sort_index()\n    \n        Z_list = []\n        for ts in times:\n            row = []\n            for h in range(1, H+1):\n                t_h = pd.Timestamp(ts) + pd.Timedelta(hours=h)\n                hour = t_h.hour; dow = t_h.dayofweek; doy = t_h.dayofyear\n                v = [\n                    np.sin(2*np.pi*hour/24), np.cos(2*np.pi*hour/24),\n                    np.sin(2*np.pi*dow/7),  np.cos(2*np.pi*dow/7),\n                    np.sin(2*np.pi*doy/365.25), np.cos(2*np.pi*doy/365.25),\n                    1 if dow>=5 else 0,  # is_weekend\n                ]\n                if t_h in df_key.index:\n                    hol = str(df_key.loc[t_h, 'is_holiday'])\n                    sea = str(df_key.loc[t_h, 'season'])\n                else:\n                    hol, sea = \"False\", None\n                hol_vec = [int(hol==\"False\"), int(hol==\"tet\"), int(hol==\"national\")]\n                sea_vec = [int(sea==s) for s in [\"winter\",\"spring\",\"summer\",\"autumn\"]] if sea is not None else [0,0,0,0]\n                v += hol_vec + sea_vec\n                row.append(v)\n            Z_list.append(row)\n        return np.array(Z_list, dtype=DTYPE)  # (N, H, Ff)\n    \n    # -- Build & split Z to align with X/Y splits defined above --\n    end_times_all = df_feat[TIME_COL].values\n    Z_all = build_future_covariates(end_times_all, HORIZON, df=df, TIME_COL=TIME_COL)\n    \n    Z_train = Z_all[:train_end]\n    Z_val   = Z_all[train_end:valid_end]\n    Z_test  = Z_all[valid_end:]\n    \n    # Fit preprocessor on TRAIN only; transform VALID & TEST\n    X_train_enc = preprocess.fit_transform(X_train)\n    X_val_enc   = preprocess.transform(X_valid)\n    X_test_enc  = preprocess.transform(X_test)\n    \n    print(X_train_enc.shape)\n    print(X_val_enc.shape)\n    print(X_test_enc.shape)\n    \n    # 1) map best params from tuner to lgb.train\n    bp = best_params_lgb\n    g  = lambda k, d=None: bp.get(f\"model__estimator__{k}\", d)\n    \n    lgb_params = {\n        \"seed\": 42,                       # master seed (alias: random_state)\n        \"bagging_seed\": 42,\n        \"feature_fraction_seed\": 42,\n        \"data_random_seed\": 42,\n        \"objective\": \"regression\",\n        \"metric\": \"rmse\",\n        \"learning_rate\": g(\"learning_rate\", 0.05),\n        \"num_leaves\": g(\"num_leaves\", 127),\n        \"max_depth\": g(\"max_depth\", -1),\n        \"feature_fraction\": g(\"colsample_bytree\", 0.8),   # alias\n        \"bagging_fraction\": g(\"subsample\", 0.8),          # alias\n        \"bagging_freq\": bp.get(\"model__estimator__subsample_freq\", 0),\n        \"min_data_in_leaf\": g(\"min_child_samples\", 20),\n        \"lambda_l1\": g(\"reg_alpha\", 0.0),\n        \"lambda_l2\": g(\"reg_lambda\", 0.1),\n        \"verbosity\": -1,\n        \"num_threads\": -1,\n        # Optional GPU:\n        \"device_type\": \"gpu\",\n        # Optional speed knob during refit:\n        \"max_bin\": 255,\n    }\n    num_boost_round = g(\"n_estimators\", 4000)\n    early_stopping_rounds = 100\n    \n    # 4) Train per-horizon on TRAIN, early-stop on VALID; predict both VALID and TEST\n    models_lgb = []\n    y_hat_val_list  = []\n    y_hat_test_list = []\n\n    for h in range(HORIZON):  # 0..23 => t+1..t+24\n        Xtr_h = np.hstack([X_train_enc, Z_train[:, h, :]]).astype(DTYPE)\n        Xva_h = np.hstack([X_val_enc,   Z_val[:,   h, :]]).astype(DTYPE)\n        Xte_h = np.hstack([X_test_enc,  Z_test[:,  h, :]]).astype(DTYPE)\n    \n        dtrain = lgb.Dataset(Xtr_h, label=Y_train[:, h])\n        dvalid = lgb.Dataset(Xva_h, label=Y_valid[:, h], reference=dtrain)\n    \n        booster = lgb.train(\n            params=lgb_params,\n            train_set=dtrain,\n            num_boost_round=num_boost_round,\n            valid_sets=[dvalid],\n            callbacks=[lgb.early_stopping(early_stopping_rounds), lgb.log_evaluation(-1)],\n        )\n        models_lgb.append((booster, Xte_h))\n        \n        # predictions for VAL & TEST under the SAME horizon-specific features\n        y_hat_val_list.append(\n            booster.predict(Xva_h, iteration_range=(0, booster.best_iteration + 1))\n        )\n        y_hat_test_list.append(\n            booster.predict(Xte_h, iteration_range=(0, booster.best_iteration + 1))\n        )\n    \n    Y_hat_val  = np.column_stack(y_hat_val_list)   # shape (N_val,  H)\n    Y_hat_test = np.column_stack(y_hat_test_list)  # shape (N_test, H)\n    \n    # Optional: metrics for quick inspection\n    def safe_mape(y_true, y_pred, eps=1e-6):\n        y_true = np.asarray(y_true)\n        return np.mean(np.abs((y_true - y_pred) / np.clip(np.abs(y_true), eps, None))) * 100.0\n\n    # Val\n    rmse_val_lgb = [math.sqrt(mean_squared_error(Y_valid[:, h], Y_hat_val[:, h])) for h in range(HORIZON)]\n    nrmse_val_lgb = [rm / (np.mean(Y_valid[:, h]) + 1e-6) * 100 for h, rm in enumerate(rmse_val_lgb)]\n    mape_val_lgb  = [safe_mape(Y_valid[:, h], Y_hat_val[:, h]) for h in range(HORIZON)]\n    # Test\n    rmse_test_lgb = [math.sqrt(mean_squared_error(Y_test[:, h], Y_hat_test[:, h])) for h in range(HORIZON)]\n    nrmse_test_lgb = [rm / (np.mean(Y_test[:, h]) + 1e-6) * 100 for h, rm in enumerate(rmse_test_lgb)]\n    mape_test_lgb  = [safe_mape(Y_test[:, h], Y_hat_test[:, h]) for h in range(HORIZON)]\n    print(\"LightGBM val:\")\n    print(\"Per-horizon RMSE:\", [round(x,2) for x in rmse_val_lgb])\n    print(\"Avg RMSE:  {:.3f}\".format(np.mean(rmse_val_lgb)))\n    print(\"Avg nRMSE: {:.2f}%\".format(np.mean(nrmse_val_lgb)))\n    print(\"Avg MAPE:  {:.2f}%\".format(np.mean(mape_val_lgb)))\n\n    print(\"LightGBM test:\")\n    print(\"Per-horizon RMSE:\", [round(x,2) for x in rmse_test_lgb])\n    print(\"Avg RMSE:  {:.3f}\".format(np.mean(rmse_test_lgb)))\n    print(\"Avg nRMSE: {:.2f}%\".format(np.mean(nrmse_test_lgb)))\n    print(\"Avg MAPE:  {:.2f}%\".format(np.mean(mape_test_lgb)))\n    return Y_hat_val, Y_hat_test\n\nP_lgb_val, P_lgb_test = runLightGBM(df_feat)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:59:39.236227Z","iopub.execute_input":"2025-09-25T09:59:39.236797Z","iopub.status.idle":"2025-09-25T10:02:18.644995Z","shell.execute_reply.started":"2025-09-25T09:59:39.236777Z","shell.execute_reply":"2025-09-25T10:02:18.644316Z"}},"outputs":[{"name":"stdout","text":"(44689, 131) (44689, 24)\n(1344, 131) (1344, 24)\n(1344, 131) (1344, 24)\n(44689, 155)\n(1344, 155)\n(1344, 155)\n","output_type":"stream"},{"name":"stderr","text":"1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[300]\tvalid_0's rmse: 221.383\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[236]\tvalid_0's rmse: 214.613\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[300]\tvalid_0's rmse: 205.398\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[300]\tvalid_0's rmse: 209.231\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[299]\tvalid_0's rmse: 204.038\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[298]\tvalid_0's rmse: 207.496\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[300]\tvalid_0's rmse: 202.338\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[294]\tvalid_0's rmse: 198.047\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[299]\tvalid_0's rmse: 203.008\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[296]\tvalid_0's rmse: 207.077\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[296]\tvalid_0's rmse: 211.826\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[300]\tvalid_0's rmse: 207.841\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[297]\tvalid_0's rmse: 208.157\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[286]\tvalid_0's rmse: 211.663\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[296]\tvalid_0's rmse: 212.207\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[298]\tvalid_0's rmse: 210.015\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[299]\tvalid_0's rmse: 215.218\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[300]\tvalid_0's rmse: 207.299\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[300]\tvalid_0's rmse: 207.953\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[300]\tvalid_0's rmse: 225.419\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[300]\tvalid_0's rmse: 217.493\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[299]\tvalid_0's rmse: 215.47\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[300]\tvalid_0's rmse: 223.817\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[299]\tvalid_0's rmse: 230.119\nLightGBM val:\nPer-horizon RMSE: [221.38, 214.61, 205.4, 209.23, 204.04, 207.5, 202.34, 198.05, 203.01, 207.08, 211.83, 207.84, 208.16, 211.66, 212.21, 210.01, 215.22, 207.3, 207.95, 225.42, 217.49, 215.47, 223.82, 230.12]\nAvg RMSE:  211.547\nAvg nRMSE: 10.79%\nAvg MAPE:  7.30%\nLightGBM test:\nPer-horizon RMSE: [290.39, 291.5, 263.45, 270.8, 268.93, 267.51, 268.24, 259.52, 267.25, 276.53, 278.14, 277.37, 271.84, 294.74, 288.04, 292.73, 295.77, 273.68, 278.08, 309.11, 294.89, 291.08, 319.38, 309.62]\nAvg RMSE:  283.275\nAvg nRMSE: 11.45%\nAvg MAPE:  7.79%\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"print(P_lgb_val.shape, P_lgb_test.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T10:02:58.487517Z","iopub.execute_input":"2025-09-25T10:02:58.487820Z","iopub.status.idle":"2025-09-25T10:02:58.491882Z","shell.execute_reply.started":"2025-09-25T10:02:58.487800Z","shell.execute_reply":"2025-09-25T10:02:58.491307Z"}},"outputs":[{"name":"stdout","text":"(1344, 24) (1344, 24)\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"# ---- 1) Simple average\nP_avg_val_tflgb  = 0.5*P_trf_val + 0.5*P_lgb_val\nP_avg_test_tflgb = 0.5*P_trf_test + 0.5*P_lgb_test\n\n# ---- 2) Per-horizon optimal weight α_h via closed-form least squares (clip to [0,1])\ndef optimal_alphas_per_horizon(Y, P1, P2):\n    # Y, P1, P2: (n, H)\n    H = Y.shape[1]\n    alpha = np.zeros(H)\n    D = P1 - P2                       # (n, H)\n    num = ((Y - P2) * D).sum(axis=0)  # covariance-like term\n    den = (D * D).sum(axis=0) + 1e-12 # variance-like term (avoid 0-division)\n    alpha = num / den\n    return np.clip(alpha, 0.0, 1.0)   # enforce convex combo\n\nalph_tflgb = optimal_alphas_per_horizon(Y_val, P_trf_val, P_lgb_val)  # (24,)\n# Broadcast to (n,H)\nP_wavg_val_tflgb  = alph_tflgb * P_trf_val  + (1 - alph_tflgb) * P_lgb_val\nP_wavg_test_tflgb = alph_tflgb * P_trf_test + (1 - alph_tflgb) * P_lgb_test\nprint(\"Per-horizon alphas:\", np.round(alph_tflgb, 3))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T10:12:18.749568Z","iopub.execute_input":"2025-09-25T10:12:18.750240Z","iopub.status.idle":"2025-09-25T10:12:18.757468Z","shell.execute_reply.started":"2025-09-25T10:12:18.750212Z","shell.execute_reply":"2025-09-25T10:12:18.756926Z"}},"outputs":[{"name":"stdout","text":"Per-horizon alphas: [0.704 0.657 0.606 0.63  0.598 0.626 0.584 0.548 0.584 0.607 0.623 0.593\n 0.607 0.622 0.628 0.602 0.633 0.591 0.597 0.691 0.655 0.657 0.696 0.744]\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\ndef rmse(y, yhat): return np.sqrt(mean_squared_error(y, yhat))\ndef nrmse(y, yhat): return rmse(y, yhat) / (np.mean(y) + 1e-12)\ndef mape(y, yhat):  return np.mean(np.abs((y - yhat) / (y + 1e-12)))\ndef mae(y, yhat): return np.mean(np.abs(y - yhat))\n\ndef report(name, Y, P):\n    print(f\"{name:12s} | MAE:{mae(Y,P):8.2f}  RMSE:{rmse(Y,P):8.2f}  nRMSE:{100*nrmse(Y,P):6.2f}%  MAPE:{100*mape(Y,P):6.2f}%\")\n\nprint(\"On validation:\")\nreport(\"LightGBM\",     Y_val, P_lgb_val)\nreport(\"Transformer\", Y_val, P_trf_val)\nreport(\"Avg(50/50)\",  Y_val, P_avg_val_tflgb)\nreport(\"Weighted\",    Y_val, P_wavg_val_tflgb)\n\nprint(\"On test (final results):\")\nreport(\"LightGBM\",     Y_test, P_lgb_test)\nreport(\"Transformer\", Y_test, P_trf_test)\nreport(\"Avg(50/50)\",  Y_test, P_avg_test_tflgb)\nreport(\"Weighted\",    Y_test, P_wavg_test_tflgb)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T10:12:44.048151Z","iopub.execute_input":"2025-09-25T10:12:44.048399Z","iopub.status.idle":"2025-09-25T10:12:44.070761Z","shell.execute_reply.started":"2025-09-25T10:12:44.048382Z","shell.execute_reply":"2025-09-25T10:12:44.070010Z"}},"outputs":[{"name":"stdout","text":"On validation:\nLightGBM     | MAE:  146.83  RMSE:  211.68  nRMSE: 10.80%  MAPE:  7.30%\nTransformer  | MAE:  134.50  RMSE:  193.76  nRMSE:  9.88%  MAPE:  6.66%\nAvg(50/50)   | MAE:  123.86  RMSE:  185.15  nRMSE:  9.44%  MAPE:  6.00%\nWeighted     | MAE:  123.56  RMSE:  183.70  nRMSE:  9.37%  MAPE:  6.00%\nOn test (final results):\nLightGBM     | MAE:  197.19  RMSE:  283.70  nRMSE: 11.47%  MAPE:  7.79%\nTransformer  | MAE:  177.82  RMSE:  247.16  nRMSE:  9.99%  MAPE:  7.17%\nAvg(50/50)   | MAE:  174.45  RMSE:  249.52  nRMSE: 10.08%  MAPE:  6.89%\nWeighted     | MAE:  172.56  RMSE:  245.27  nRMSE:  9.91%  MAPE:  6.85%\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"np.savetxt(\"ensemble_weights_lgb_trf.csv\", alph, delimiter=\",\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T06:21:35.715073Z","iopub.execute_input":"2025-09-25T06:21:35.715309Z","iopub.status.idle":"2025-09-25T06:21:35.719284Z","shell.execute_reply.started":"2025-09-25T06:21:35.715294Z","shell.execute_reply":"2025-09-25T06:21:35.718755Z"}},"outputs":[],"execution_count":61},{"cell_type":"markdown","source":"# Results\nNote: The models training cells above should be executed already.","metadata":{}},{"cell_type":"markdown","source":"## LSTM + Attention","metadata":{}},{"cell_type":"code","source":"metrics_lstmattention = evaluate_on_test(model_lstmattention, X_test, Y_test_s, bundle.y_scaler, Z_test=Z_test)\nprint(f\"\\nTest MAE (MW):  {metrics_lstmattention['MAE']:.2f}\")\nprint(f\"Test RMSE (MW): {metrics_lstmattention['RMSE']:.2f}\")\nprint(f\"Test MAPE (MW): {metrics_lstmattention['MAPE']:.2f}\")\nprint(\"Horizon-wise MAE (MW):\", np.round(metrics_lstmattention[\"MAE_by_h\"], 2))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T05:55:20.683743Z","iopub.execute_input":"2025-09-25T05:55:20.683998Z","iopub.status.idle":"2025-09-25T05:55:20.752698Z","shell.execute_reply.started":"2025-09-25T05:55:20.683982Z","shell.execute_reply":"2025-09-25T05:55:20.752031Z"}},"outputs":[{"name":"stdout","text":"\nTest MAE (MW):  183.55\nTest RMSE (MW): 256.13\nTest MAPE (MW): 7.60\nHorizon-wise MAE (MW): [182.14 181.66 181.04 180.34 181.51 182.5  183.43 184.05 184.49 185.49\n 186.56 187.31 187.18 186.12 185.39 185.43 185.65 184.05 182.69 181.02\n 180.85 181.4  182.23 182.64]\n","output_type":"stream"}],"execution_count":40},{"cell_type":"markdown","source":"## Transformer","metadata":{}},{"cell_type":"code","source":"print(f\"\\nTest MAE (MW):  {metrics_transformer['MAE']:.2f}\")\nprint(f\"Test RMSE (MW): {metrics_transformer['RMSE']:.2f}\")\nprint(f\"Test MAPE (MW): {metrics_transformer['MAPE']:.2f}\")\nprint(\"Horizon-wise MAE (MW):\", np.round(metrics_transformer[\"MAE_by_h\"], 2))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T06:04:58.157818Z","iopub.execute_input":"2025-09-25T06:04:58.158113Z","iopub.status.idle":"2025-09-25T06:04:58.162824Z","shell.execute_reply.started":"2025-09-25T06:04:58.158092Z","shell.execute_reply":"2025-09-25T06:04:58.162064Z"}},"outputs":[{"name":"stdout","text":"\nTest MAE (MW):  177.82\nTest RMSE (MW): 247.16\nTest MAPE (MW): 7.17\nHorizon-wise MAE (MW): [177.03 176.56 176.4  176.69 176.97 177.27 176.72 176.67 176.7  177.\n 177.32 177.62 178.27 178.11 178.08 178.69 179.04 179.25 178.54 178.82\n 178.77 178.53 178.96 179.72]\n","output_type":"stream"}],"execution_count":43},{"cell_type":"markdown","source":"## Transformer + XGBoost","metadata":{}},{"cell_type":"code","source":"print(\"On validation:\")\nreport(\"XGBoost\",     Y_val, P_xgb_val)\nreport(\"Transformer\", Y_val, P_trf_val)\nreport(\"Avg(50/50)\",  Y_val, P_avg_val_tfxgb)\nreport(\"Weighted\",    Y_val, P_wavg_val_tfxgb)\n\nprint(\"On test (final results):\")\nreport(\"XGBoost\",     Y_test, P_xgb_test)\nreport(\"Transformer\", Y_test, P_trf_test)\nreport(\"Avg(50/50)\",  Y_test, P_avg_test_tfxgb)\nreport(\"Weighted\",    Y_test, P_wavg_test_tfxgb)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T10:14:24.161158Z","iopub.execute_input":"2025-09-25T10:14:24.161840Z","iopub.status.idle":"2025-09-25T10:14:24.178493Z","shell.execute_reply.started":"2025-09-25T10:14:24.161815Z","shell.execute_reply":"2025-09-25T10:14:24.177730Z"}},"outputs":[{"name":"stdout","text":"On validation:\nXGBoost      | MAE:  149.58  RMSE:  213.38  nRMSE: 10.88%  MAPE:  7.45%\nTransformer  | MAE:  134.50  RMSE:  193.76  nRMSE:  9.88%  MAPE:  6.66%\nAvg(50/50)   | MAE:  124.69  RMSE:  184.98  nRMSE:  9.44%  MAPE:  6.06%\nWeighted     | MAE:  123.87  RMSE:  183.40  nRMSE:  9.36%  MAPE:  6.02%\nOn test (final results):\nXGBoost      | MAE:  203.20  RMSE:  285.64  nRMSE: 11.54%  MAPE:  8.13%\nTransformer  | MAE:  177.82  RMSE:  247.16  nRMSE:  9.99%  MAPE:  7.17%\nAvg(50/50)   | MAE:  177.35  RMSE:  250.49  nRMSE: 10.12%  MAPE:  7.06%\nWeighted     | MAE:  174.71  RMSE:  245.96  nRMSE:  9.94%  MAPE:  6.97%\n","output_type":"stream"}],"execution_count":55},{"cell_type":"markdown","source":"## Transformer + LightGBM","metadata":{}},{"cell_type":"code","source":"print(\"On validation:\")\nreport(\"LightGBM\",     Y_val, P_lgb_val)\nreport(\"Transformer\", Y_val, P_trf_val)\nreport(\"Avg(50/50)\",  Y_val, P_avg_val_tflgb)\nreport(\"Weighted\",    Y_val, P_wavg_val_tflgb)\n\nprint(\"On test (final results):\")\nreport(\"LightGBM\",     Y_test, P_lgb_test)\nreport(\"Transformer\", Y_test, P_trf_test)\nreport(\"Avg(50/50)\",  Y_test, P_avg_test_tflgb)\nreport(\"Weighted\",    Y_test, P_wavg_test_tflgb)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T10:15:07.279643Z","iopub.execute_input":"2025-09-25T10:15:07.280359Z","iopub.status.idle":"2025-09-25T10:15:07.299569Z","shell.execute_reply.started":"2025-09-25T10:15:07.280337Z","shell.execute_reply":"2025-09-25T10:15:07.298809Z"}},"outputs":[{"name":"stdout","text":"On validation:\nLightGBM     | MAE:  146.83  RMSE:  211.68  nRMSE: 10.80%  MAPE:  7.30%\nTransformer  | MAE:  134.50  RMSE:  193.76  nRMSE:  9.88%  MAPE:  6.66%\nAvg(50/50)   | MAE:  123.86  RMSE:  185.15  nRMSE:  9.44%  MAPE:  6.00%\nWeighted     | MAE:  123.56  RMSE:  183.70  nRMSE:  9.37%  MAPE:  6.00%\nOn test (final results):\nLightGBM     | MAE:  197.19  RMSE:  283.70  nRMSE: 11.47%  MAPE:  7.79%\nTransformer  | MAE:  177.82  RMSE:  247.16  nRMSE:  9.99%  MAPE:  7.17%\nAvg(50/50)   | MAE:  174.45  RMSE:  249.52  nRMSE: 10.08%  MAPE:  6.89%\nWeighted     | MAE:  172.56  RMSE:  245.27  nRMSE:  9.91%  MAPE:  6.85%\n","output_type":"stream"}],"execution_count":57},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"import joblib\n\n# load back the fitted model\nloaded_model = joblib.load(\"/kaggle/input/prophet/other/default/1/prophet_model.pkl\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T10:42:30.796396Z","iopub.execute_input":"2025-09-25T10:42:30.797014Z","iopub.status.idle":"2025-09-25T10:42:32.473441Z","shell.execute_reply.started":"2025-09-25T10:42:30.796972Z","shell.execute_reply":"2025-09-25T10:42:32.472661Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"cut_valid_ts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T11:35:52.156847Z","iopub.execute_input":"2025-09-25T11:35:52.157113Z","iopub.status.idle":"2025-09-25T11:35:52.162253Z","shell.execute_reply.started":"2025-09-25T11:35:52.157094Z","shell.execute_reply":"2025-09-25T11:35:52.161493Z"}},"outputs":[{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"Timestamp('2025-04-04 00:00:00')"},"metadata":{}}],"execution_count":65},{"cell_type":"code","source":"end_times[end_times > np.datetime64(cut_valid_ts)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T11:34:40.164255Z","iopub.execute_input":"2025-09-25T11:34:40.164531Z","iopub.status.idle":"2025-09-25T11:34:40.169113Z","shell.execute_reply.started":"2025-09-25T11:34:40.164511Z","shell.execute_reply":"2025-09-25T11:34:40.168403Z"}},"outputs":[{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"numpy.datetime64('2025-05-30T00:00:00.000000000')"},"metadata":{}}],"execution_count":64},{"cell_type":"code","source":"# You already have:\n# X_raw, Y_raw, end_times = make_windows(F_all, y_all, t_all, L, HORIZON)\n\n# Determine the test mask as you did previously:\ntest_mask = (end_times > np.datetime64(cut_valid_ts))\n\n# 1. Extract the test sequences (shape: (n_test_sequences, L, D))\nX_seq_test = X_raw[test_mask]\n\n# 2. Get the corresponding timestamps at the end of each window\ntimes_test = end_times[test_mask]  # these are the same timestamps you used for splitting\n\n# 3. Build the future-known covariates for these timestamps\n#    Only pass the two expected arguments: times and horizon length\nZ_seq_test = build_future_covariates(times_test, HORIZON)\n\n# 4. Now you can select the final sequence (i.e., the one for the last day)\nlast_x_seq = X_seq_test[-1]        # shape (L, D)\nlast_z_seq = Z_seq_test[-1]        # shape (HORIZON, Fz)\n\nprint(last_x_seq.shape)  # (history_window, num_features)\nprint(last_z_seq.shape)  # (24, num_future_features)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T10:58:37.876914Z","iopub.execute_input":"2025-09-25T10:58:37.877207Z","iopub.status.idle":"2025-09-25T10:58:39.415866Z","shell.execute_reply.started":"2025-09-25T10:58:37.877186Z","shell.execute_reply":"2025-09-25T10:58:39.415215Z"}},"outputs":[{"name":"stdout","text":"(48, 48)\n(24, 14)\n","output_type":"stream"}],"execution_count":60},{"cell_type":"code","source":"# Use the first column of Y_all for sliding windows (it's y_t+1)\ny_single = Y_all[:, 0:1]\nX_raw, Y_raw, end_times = make_windows(F_all, y_single, df['timestamp'].to_numpy(), L_win, HORIZON)\n\n# Align sequence masks using end_times\ntrain_mask_seq = (end_times <= df_feat.iloc[train_end-1]['timestamp'])\nvalid_mask_seq = (end_times >  df_feat.iloc[train_end-1]['timestamp']) & (end_times <= df_feat.iloc[valid_end-1]['timestamp'])\ntest_mask_seq  = (end_times >  df_feat.iloc[valid_end-1]['timestamp'])\n\nX_seq_train, X_seq_test  = X_raw[train_mask_seq], X_raw[test_mask_seq]\nZ_seq_train, Z_seq_test  = Z_train, Z_test   # Already built above","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nwith torch.no_grad():\n    X_test_t = torch.tensor(X_seq_test, dtype=torch.float32).to(device)\n    Z_test_t = torch.tensor(Z_seq_test, dtype=torch.float32).to(device)\n    lstmattention_pred       = model_lstmattention(X_test_t, Z_test_t).cpu().numpy()[0]\n    transformer_pred         = model_transformer(X_test_t, Z_test_t).cpu().numpy()[0]\n\nprint('LSTM+Attention predictions:', lstmattention_pred)\nprint('Transformer predictions:', transformer_pred)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T11:45:39.000664Z","iopub.execute_input":"2025-09-25T11:45:39.001460Z","iopub.status.idle":"2025-09-25T11:45:39.020710Z","shell.execute_reply.started":"2025-09-25T11:45:39.001436Z","shell.execute_reply":"2025-09-25T11:45:39.019650Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_84/51947767.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mX_test_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_seq_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mZ_test_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ_seq_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlstmattention_pred\u001b[0m       \u001b[0;34m=\u001b[0m \u001b[0mmodel_lstmattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ_test_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtransformer_pred\u001b[0m         \u001b[0;34m=\u001b[0m \u001b[0mmodel_transformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ_test_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"],"ename":"NameError","evalue":"name 'device' is not defined","output_type":"error"}],"execution_count":66},{"cell_type":"code","source":"\n\n# -------------------------------------------------------------------------\n# 7. (Optional) Evaluate against ground truth of the last day\n# -------------------------------------------------------------------------\nactual = df_feat[[f'y_t+{h}' for h in range(1,HORIZON+1)]].iloc[-1].values.astype(float)\ndef mape(y_true, y_pred):\n    return np.mean(np.abs((y_true - y_pred) / np.clip(np.abs(y_true), 1e-6, None))) * 100\n\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfor name, pred in {'LSTM': lstm_pred, 'Transformer': transformer_pred}.items():\n    mae  = mean_absolute_error(actual, pred)\n    rmse = math.sqrt(mean_squared_error(actual, pred))\n    mape_val = mape(actual, pred)\n    print(f'{name}: MAE={mae:.2f}, RMSE={rmse:.2f}, MAPE={mape_val:.2f}%')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preprocess = ColumnTransformer(\n    [\n        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), CAT_COLS),\n        ('num', 'passthrough', [c for c in feature_cols if c not in CAT_COLS])\n    ],\n    remainder='drop'\n)\n# Scorer: average RMSE across 24 horizons (negated for maximization)\n# =========================\nscorer = make_scorer(avg_neg_rmse, greater_is_better=True)\nimport joblib\nxgb_tuner = joblib.load('/kaggle/input/xgboostlightgbm/scikitlearn/default/1/xgb_tuner_1.pkl')\nprint(xgb_tuner.best_params_)\nprint(xgb_tuner)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T11:54:46.148858Z","iopub.execute_input":"2025-09-25T11:54:46.149389Z","iopub.status.idle":"2025-09-25T11:54:46.630142Z","shell.execute_reply.started":"2025-09-25T11:54:46.149366Z","shell.execute_reply":"2025-09-25T11:54:46.629435Z"}},"outputs":[{"name":"stdout","text":"{'model__estimator__subsample': 0.7, 'model__estimator__reg_lambda': 0.5, 'model__estimator__reg_alpha': 0.0, 'model__estimator__n_estimators': 1000, 'model__estimator__min_child_weight': 3, 'model__estimator__max_depth': 6, 'model__estimator__learning_rate': 0.03, 'model__estimator__gamma': 0.1, 'model__estimator__colsample_bytree': 0.7}\nRandomizedSearchCV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=3, test_size=None),\n                   estimator=Pipeline(steps=[('prep',\n                                              ColumnTransformer(transformers=[('cat',\n                                                                               OneHotEncoder(handle_unknown='ignore',\n                                                                                             sparse_output=False),\n                                                                               ['Weather',\n                                                                                'Wind '\n                                                                                'Direction',\n                                                                                'season',\n                                                                                'is_holiday']),\n                                                                              ('num',\n                                                                               'passthrough',\n                                                                               ['Temperature',\n                                                                                'Precipitation',\n                                                                                'Chance '\n                                                                                'of '\n                                                                                'snow',\n                                                                                'Hu...\n                                        'model__estimator__learning_rate': [0.03,\n                                                                            0.05,\n                                                                            0.08],\n                                        'model__estimator__max_depth': [6, 8],\n                                        'model__estimator__min_child_weight': [1,\n                                                                               3],\n                                        'model__estimator__n_estimators': [600,\n                                                                           800,\n                                                                           1000],\n                                        'model__estimator__reg_alpha': [0.0,\n                                                                        0.1],\n                                        'model__estimator__reg_lambda': [0.5,\n                                                                         1.0],\n                                        'model__estimator__subsample': [0.7,\n                                                                        0.9]},\n                   random_state=42,\n                   scoring=make_scorer(avg_neg_rmse, response_method='predict'),\n                   verbose=1)\n","output_type":"stream"}],"execution_count":67},{"cell_type":"code","source":"lgb_tuner = joblib.load('/kaggle/input/lightgbm/scikitlearn/default/1/lgb_tuner_1.pkl')\nwith open('/kaggle/input/lightgbm/scikitlearn/default/1/lgbm_best_params.json', \"r\") as f:\n    best_params_lgb = json.load(f)\n\n\nprint(lgb_tuner)\nprint(best_params_lgb)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T11:55:59.465975Z","iopub.execute_input":"2025-09-25T11:55:59.466260Z","iopub.status.idle":"2025-09-25T11:55:59.486002Z","shell.execute_reply.started":"2025-09-25T11:55:59.466240Z","shell.execute_reply":"2025-09-25T11:55:59.485450Z"}},"outputs":[{"name":"stdout","text":"Pipeline(steps=[('prep',\n                 ColumnTransformer(transformers=[('cat',\n                                                  OneHotEncoder(handle_unknown='ignore',\n                                                                sparse_output=False),\n                                                  ['Weather', 'Wind Direction',\n                                                   'season', 'is_holiday']),\n                                                 ('num', 'passthrough',\n                                                  ['Temperature',\n                                                   'Precipitation',\n                                                   'Chance of snow', 'Humidity',\n                                                   'Wind', 'Wind Gust',\n                                                   'Wind Degree', 'Cloud Cover',\n                                                   'Visibility', 'is_weekend',\n                                                   'hour', 'dow', 'dom',\n                                                   'month', 'd...\n                                                   'doy_cos', 'y_lag_1',\n                                                   'y_lag_2', 'y_lag_3',\n                                                   'y_lag_4', 'y_lag_5',\n                                                   'y_lag_6', 'y_lag_7',\n                                                   'y_lag_8', 'y_lag_9', ...])])),\n                ('model',\n                 MultiOutputRegressor(estimator=LGBMRegressor(colsample_bytree=0.7,\n                                                              learning_rate=0.05,\n                                                              max_bin=127,\n                                                              max_depth=8,\n                                                              n_estimators=300,\n                                                              n_jobs=-1,\n                                                              num_leaves=63,\n                                                              objective='regression',\n                                                              random_state=42,\n                                                              reg_lambda=0.1,\n                                                              subsample=0.9,\n                                                              verbose=-1),\n                                      n_jobs=1))])\n{'model__estimator__subsample': 0.9, 'model__estimator__reg_lambda': 0.1, 'model__estimator__reg_alpha': 0.0, 'model__estimator__num_leaves': 63, 'model__estimator__n_estimators': 300, 'model__estimator__min_child_samples': 20, 'model__estimator__max_depth': 8, 'model__estimator__learning_rate': 0.05, 'model__estimator__colsample_bytree': 0.7}\n","output_type":"stream"}],"execution_count":68},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom datetime import timedelta\n\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# -------------------------------------------------------------------------\n# 1. Load data and rebuild all features\n# -------------------------------------------------------------------------\nDATA_PATH = '/kaggle/input/weather-hanoi-2020-2025-normed/merge_weather_energy_hanoi_20202025_norm.csv'\ndf = pd.read_csv(DATA_PATH)\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\ndf = df.sort_values('timestamp').reset_index(drop=True)\n\n# Convert categoricals to strings\nfor col in ['Weather','Wind Direction','season','is_holiday']:\n    df[col] = df[col].astype(str)\n\n# Add calendar signals\ndf['hour'] = df['timestamp'].dt.hour\ndf['dow']  = df['timestamp'].dt.dayofweek\ndf['doy']  = df['timestamp'].dt.dayofyear\ndf['hour_sin'] = np.sin(2*np.pi*df['hour']/24)\ndf['hour_cos'] = np.cos(2*np.pi*df['hour']/24)\ndf['dow_sin']  = np.sin(2*np.pi*df['dow']/7)\ndf['dow_cos']  = np.cos(2*np.pi*df['dow']/7)\ndf['doy_sin']  = np.sin(2*np.pi*df['doy']/365.25)\ndf['doy_cos']  = np.cos(2*np.pi*df['doy']/365.25)\ndf['is_weekend'] = (df['dow'] >= 5).astype(int)\n\n# Define hyperparameters\nTARGET_COL = 'total_consumption_mw'\nHORIZON = 24\nTARGET_LAGS = range(1,49)\nROLL_WINDOWS = [3,6,12,24,48]\nEXOG_NUM = ['Temperature','Precipitation','Humidity','Wind',\n            'Wind Gust','Wind Degree','Cloud Cover','Visibility']\n\n# Lagged target and rolling stats\nfor L in TARGET_LAGS:\n    df[f'y_lag_{L}'] = df[TARGET_COL].shift(L)\nfor W in ROLL_WINDOWS:\n    df[f'y_rollmean_{W}'] = df[TARGET_COL].shift(1).rolling(W, min_periods=1).mean()\n    df[f'y_rollstd_{W}']  = df[TARGET_COL].shift(1).rolling(W, min_periods=1).std()\n\n# Lagged exogenous vars\nfor col in EXOG_NUM:\n    for L in [0,1,3,6,12,24]:\n        df[f'{col}_lag{L}'] = df[col].shift(L)\n\n# Multi-horizon labels\nfor h in range(1, HORIZON+1):\n    df[f'y_t+{h}'] = df[TARGET_COL].shift(-h)\n\n# Drop rows with NaNs\ndf_feat = df.dropna().reset_index(drop=True)\n\n# -------------------------------------------------------------------------\n# 2. Prepare feature matrix and future-known Z covariates\n# -------------------------------------------------------------------------\ndef build_future_covariates(times, H, df, TIME_COL='timestamp'):\n    df_key = df[[TIME_COL, 'is_holiday','season']].copy()\n    df_key[TIME_COL] = pd.to_datetime(df_key[TIME_COL])\n    df_key = df_key.set_index(TIME_COL).sort_index()\n    Z_list = []\n    for ts in times:\n        row = []\n        for h in range(1, H+1):\n            future = pd.Timestamp(ts) + timedelta(hours=h)\n            hour  = future.hour; dow = future.dayofweek; doy = future.dayofyear\n            v = [\n                np.sin(2*np.pi*hour/24), np.cos(2*np.pi*hour/24),\n                np.sin(2*np.pi*dow/7),  np.cos(2*np.pi*dow/7),\n                np.sin(2*np.pi*doy/365.25), np.cos(2*np.pi*doy/365.25),\n                1.0 if dow>=5 else 0.0\n            ]\n            # holiday & season one-hots\n            if future in df_key.index:\n                hol = df_key.at[future,'is_holiday']\n                sea = df_key.at[future,'season']\n            else:\n                hol = 'False'\n                sea = None\n            hol_vec = [1-int(hol=='True'), int(hol=='True')]\n            # adjust this if you have more holiday types (e.g. tet vs national)\n            season_list = ['winter','spring','summer','autumn']\n            sea_vec  = [1 if sea==s else 0 for s in season_list]\n            row.append(v + hol_vec + sea_vec)\n        Z_list.append(row)\n    return np.array(Z_list, dtype=np.float32)\n\n# Build base feature matrix\nlabel_cols   = [f'y_t+{h}' for h in range(1,HORIZON+1)]\nfeature_cols = [c for c in df_feat.columns if c not in label_cols + ['timestamp', TARGET_COL]]\nX_all = df_feat[feature_cols].values\nY_all = df_feat[label_cols].values\ntimes = df_feat['timestamp'].to_numpy()\n\n# Split indices\nN = len(df_feat)\ntrain_end = int(N - 2*8*7*24)     # last 8 weeks test, previous 8 weeks val\nvalid_end = int(N - 8*7*24)\n\n# Use end timestamps for splitting\ntrain_mask = (times[:len(X_all)] <= df_feat.iloc[train_end-1]['timestamp'])\nvalid_mask = (times[:len(X_all)] >  df_feat.iloc[train_end-1]['timestamp']) & (times[:len(X_all)] <= df_feat.iloc[valid_end-1]['timestamp'])\ntest_mask  = (times[:len(X_all)] >  df_feat.iloc[valid_end-1]['timestamp'])\n\nX_train, Y_train = X_all[train_mask], Y_all[train_mask]\nX_valid, Y_valid = X_all[valid_mask], Y_all[valid_mask]\nX_test,  Y_test  = X_all[test_mask],  Y_all[test_mask]\ntimes_train, times_valid, times_test = times[train_mask], times[valid_mask], times[test_mask]\n\n# Build Z covariates per split\nZ_train = build_future_covariates(times_train, HORIZON, df_feat)\nZ_valid = build_future_covariates(times_valid, HORIZON, df_feat)\nZ_test  = build_future_covariates(times_test,  HORIZON, df_feat)\n\n# -------------------------------------------------------------------------\n# 3. Train/Load XGBoost and LightGBM multi-output models and predict\n# -------------------------------------------------------------------------\n# (You can load tuned hyperparameters if you have them; otherwise use defaults)\n# Flatten Z covariates (HORIZON * Fz) and append to X for XGB/LGB\nX_train_flat = np.hstack([X_train, Z_train.reshape(Z_train.shape[0], -1)])\nX_test_flat  = np.hstack([X_test,  Z_test.reshape(Z_test.shape[0],  -1)])\nxgb_model.fit(X_train_flat, Y_train)\nY_hat_test_xgb = xgb_model.predict(X_test_flat)\n\nlgb_model = MultiOutputRegressor(LGBMRegressor(\n    n_estimators=600, max_depth=4, learning_rate=0.05,\n    subsample=0.9, colsample_bytree=0.9, random_state=42\n))\nlgb_model.fit(X_train_flat, Y_train)\nY_hat_test_lgb = lgb_model.predict(X_test_flat)\n\n# -------------------------------------------------------------------------\n# 4. Build sequence windows and load your trained LSTM and Transformer models\n# -------------------------------------------------------------------------\n# Sequence features for LSTM/Transformer (use the same columns you used for training)\nseq_cols = [TARGET_COL] + EXOG_NUM + [\n    'hour_sin','hour_cos','dow_sin','dow_cos','doy_sin','doy_cos','is_weekend','is_holiday']\nF_all = df[seq_cols].astype(np.float32).values\n\n# Build sliding windows: [t-L+1 .. t] -> [t+1 .. t+H]\nL_win = 48\ndef make_windows(F_all, y_all, times, L, H):\n    T, D = F_all.shape\n    N = T - L - H + 1\n    X = np.empty((N, L, D), dtype=np.float32)\n    Y = np.empty((N, H), dtype=np.float32)\n    end_ts = np.empty(N, dtype='datetime64[ns]')\n    for i in range(N):\n        X[i] = F_all[i:i+L]\n        Y[i] = y_all[i+L:i+L+H,0]\n        end_ts[i] = times[i+L-1]\n    return X, Y, end_ts\n\n# Use the first column of Y_all for sliding windows (it's y_t+1)\ny_single = Y_all[:, 0:1]\nX_raw, Y_raw, end_times = make_windows(F_all, y_single, df['timestamp'].to_numpy(), L_win, HORIZON)\n\n# Align sequence masks using end_times\ntrain_mask_seq = (end_times <= df_feat.iloc[train_end-1]['timestamp'])\nvalid_mask_seq = (end_times >  df_feat.iloc[train_end-1]['timestamp']) & (end_times <= df_feat.iloc[valid_end-1]['timestamp'])\ntest_mask_seq  = (end_times >  df_feat.iloc[valid_end-1]['timestamp'])\n\nX_seq_train, X_seq_test  = X_raw[train_mask_seq], X_raw[test_mask_seq]\nZ_seq_train, Z_seq_test  = Z_train, Z_test   # Already built above\n\n# Load your trained LSTM_Attention model\nclass LSTMAttn(nn.Module):\n    def __init__(self, input_dim, hidden=128, horizon=24, future_dim=0):\n        super().__init__()\n        self.lstm = nn.LSTM(input_dim, hidden, batch_first=True)\n        self.future_proj = nn.Linear(future_dim, hidden) if future_dim>0 else None\n        self.fc = nn.Linear(hidden*2 + (hidden if future_dim>0 else 0), 1)\n        self.horizon = horizon\n    def forward(self, x, z_future=None):\n        B = x.size(0)\n        seq_out, (h_n, _) = self.lstm(x)\n        q = h_n[-1]\n        attn_scores = torch.softmax(torch.sum(seq_out * q.unsqueeze(1), dim=-1), dim=1)\n        ctx = torch.sum(seq_out * attn_scores.unsqueeze(-1), dim=1)\n        base = torch.cat([ctx, q], dim=-1).unsqueeze(1).expand(-1, self.horizon, -1)\n        if z_future is not None:\n            z_proj = self.future_proj(z_future)\n            fused = torch.cat([base, z_proj], dim=-1)\n        else:\n            fused = base\n        out = self.fc(fused).squeeze(-1)\n        return out\n\n# Load your Transformer class (same as training)\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=1000):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1).float()\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div_term)\n        pe[:, 1::2] = torch.cos(pos * div_term)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n    def forward(self, x):\n        return x + self.pe[:, :x.size(1)]\n\nclass SimpleTransformer(nn.Module):\n    def __init__(self, input_dim, d_model=128, nhead=4, num_layers=2, d_ff=256, horizon=24, future_dim=0):\n        super().__init__()\n        self.input_proj = nn.Linear(input_dim, d_model)\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model, nhead=nhead, dim_feedforward=d_ff, batch_first=True\n        )\n        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        self.future_proj = nn.Linear(future_dim, d_model) if future_dim > 0 else None\n        self.fc = nn.Linear(d_model*2, horizon)\n        self.horizon = horizon\n    def forward(self, x, z_future=None):\n        x = self.input_proj(x)\n        enc_out = self.encoder(x)\n        pooled = enc_out.mean(dim=1)\n        if z_future is not None:\n            z_proj = self.future_proj(z_future).mean(dim=1)\n            fused = torch.cat([pooled, z_proj], dim=-1)\n        else:\n            fused = torch.cat([pooled, pooled], dim=-1)\n        return self.fc(fused)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Load saved models (replace with your actual paths)\nlstm_model = LSTMAttn(input_dim=X_seq_train.shape[2], hidden=128, horizon=HORIZON, future_dim=Z_seq_train.shape[2])\nlstm_model.load_state_dict(torch.load('/kaggle/input/your-models/lstm_attention_model.pth', map_location=device))\nlstm_model.to(device).eval()\n\ntransformer_model = SimpleTransformer(input_dim=X_seq_train.shape[2], d_model=128, nhead=4, num_layers=2,\n                                      d_ff=256, horizon=HORIZON, future_dim=Z_seq_train.shape[2])\ntransformer_model.load_state_dict(torch.load('/kaggle/input/your-models/transformer_model.pth', map_location=device))\ntransformer_model.to(device).eval()\n\n# Predict sequences for the test set\nwith torch.no_grad():\n    X_test_t  = torch.tensor(X_seq_test, dtype=torch.float32).to(device)\n    Z_test_t  = torch.tensor(Z_seq_test, dtype=torch.float32).to(device)\n    Y_hat_test_lstm = lstm_model(X_test_t, Z_test_t).cpu().numpy()\n    Y_hat_test_trf  = transformer_model(X_test_t, Z_test_t).cpu().numpy()\n\n# -------------------------------------------------------------------------\n# 5. Hybrid models: average Transformer with XGB/LGB\n# -------------------------------------------------------------------------\n# Align shapes: Y_hat_test_trf and Y_hat_test_xgb must have same shape (n_test_samples, HORIZON)\n# If needed, trim or pad sequences accordingly\nmin_rows = min(len(Y_hat_test_trf), len(Y_hat_test_xgb))\nY_hat_test_trf  = Y_hat_test_trf[-min_rows:]\nY_hat_test_xgb  = Y_hat_test_xgb[-min_rows:]\nY_hat_test_lgb  = Y_hat_test_lgb[-min_rows:]\n\n# Simple average hybrids (equal weight)\nY_hat_test_hybrid_xgb = 0.5 * Y_hat_test_trf + 0.5 * Y_hat_test_xgb\nY_hat_test_hybrid_lgb = 0.5 * Y_hat_test_trf + 0.5 * Y_hat_test_lgb\n\n# -------------------------------------------------------------------------\n# 6. Evaluate and print metrics for the test set\n# -------------------------------------------------------------------------\n# Use Y_test[-min_rows:] to align with predictions\nY_test_trim = Y_test[-min_rows:]\ndef mape(y_true, y_pred):\n    return np.mean(np.abs((y_true - y_pred) / np.maximum(np.abs(y_true), 1e-6))) * 100\n\nmodels_preds = {\n    'XGBoost': Y_hat_test_xgb,\n    'LightGBM': Y_hat_test_lgb,\n    'LSTM+Attn': Y_hat_test_lstm,\n    'Transformer': Y_hat_test_trf,\n    'Hybrid_XGB': Y_hat_test_hybrid_xgb,\n    'Hybrid_LGB': Y_hat_test_hybrid_lgb,\n}\nfor name, pred in models_preds.items():\n    mae  = mean_absolute_error(Y_test_trim.ravel(), pred.ravel())\n    rmse = np.sqrt(mean_squared_error(Y_test_trim.ravel(), pred.ravel()))\n    mp   = mape(Y_test_trim, pred)\n    print(f\"{name}: MAE={mae:.2f}, RMSE={rmse:.2f}, MAPE={mp:.2f}%\")\n\n# -------------------------------------------------------------------------\n# The last row of each prediction array gives the 24-hour forecast for the final day:\nfinal_day_pred = {\n    model: pred[-1] for model, pred in models_preds.items()\n}\nprint(\"24-hour forecasts for the last day:\")\nfor model, pred in final_day_pred.items():\n    print(model, pred)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}